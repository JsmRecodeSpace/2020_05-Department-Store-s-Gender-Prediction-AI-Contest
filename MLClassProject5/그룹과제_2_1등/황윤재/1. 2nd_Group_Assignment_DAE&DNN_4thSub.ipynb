{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot + DAE DNN 단일 모델\n",
    "- **One-Hot + DAE with DropOut**\n",
    "\n",
    "    - Dropout rate값들 조정해보며 비교\n",
    "    \n",
    "    - Encoder, Decoder를 두번 진행해보는 시도를 해봄 -> 성능이 많이 떨어짐\n",
    "    - Enocder layer와 Decoder layer 사이에 Noise 추가 -> 성능이 떨어짐\n",
    "    \n",
    "    - Encoder Layer에도 Noise를 삽입하는 시도를 해봄\n",
    "        -> 성능이 개선되어 총 2개의 Noise를 추가\n",
    "\n",
    "- **One-Hot + DAE with GaussianNoise**\n",
    "\n",
    "    - GaussianNoise의 stddev값 조정(0.01, 0.2, 0.2, 0.3) 및 비교\n",
    "\n",
    "    - Encoder, Decoder Layer 두 번 반복 -> 성능이 많이 떨어짐\n",
    "\n",
    "    - 가장 성적이 좋은 stddev=0.2를 기준으로 Dropout과 결합한 Modeling 시도(But. 오히려 성적이 떨어짐) \n",
    "\n",
    "- 이외에도 GaussianDropout, AlphaDropout을 사용한 DAE 시도\n",
    "\n",
    "    - **One-Hot + DAE with GaussianDropout**\n",
    "\n",
    "    - **One-Hot + DAE with AlphaDropout**\n",
    "    \n",
    "< Feature 추가 >\n",
    "- 'goods_nm'과 'tran_date'를 Feature로 추가\n",
    "    -> 결과적으로 성능이 개선되는 'goods_nm'만 추가하기로 결정\n",
    "    \n",
    "- 위의 과정 그대로 진행\n",
    "1. Dropout, GaussianNoise의 rate값, stddev값 조정하면서 비교\n",
    "2. 성적이 좋은 rate값, stddev값에 한해 Encoder Layer에 Dropout Noise 삽입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='tomato'><font color=\"#CC3D3D\"><p>\n",
    "**최종적으로 가장 성적이 우수했던 'Feature 추가 + GaussianNoise stddev값(0.01) 고정 + Encoder layer Dropout 1개 -> mean=0.77234, std=0.017' 채택**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run import_modules.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import featuretools as ft\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import *\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import Input\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.constraints import max_norm\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import warnings; warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>tran_date</th>\n",
       "      <th>store_nm</th>\n",
       "      <th>goods_id</th>\n",
       "      <th>gds_grp_nm</th>\n",
       "      <th>gds_grp_mclas_nm</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2007-01-19 00:00:00</td>\n",
       "      <td>강남점</td>\n",
       "      <td>127105</td>\n",
       "      <td>기초 화장품</td>\n",
       "      <td>화장품</td>\n",
       "      <td>850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2007-03-30 00:00:00</td>\n",
       "      <td>강남점</td>\n",
       "      <td>342220</td>\n",
       "      <td>니  트</td>\n",
       "      <td>시티웨어</td>\n",
       "      <td>480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2007-03-30 00:00:00</td>\n",
       "      <td>강남점</td>\n",
       "      <td>127105</td>\n",
       "      <td>기초 화장품</td>\n",
       "      <td>화장품</td>\n",
       "      <td>3000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2007-03-30 00:00:00</td>\n",
       "      <td>강남점</td>\n",
       "      <td>342205</td>\n",
       "      <td>니  트</td>\n",
       "      <td>시티웨어</td>\n",
       "      <td>840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2007-03-30 00:00:00</td>\n",
       "      <td>강남점</td>\n",
       "      <td>342220</td>\n",
       "      <td>상품군미지정</td>\n",
       "      <td>기타</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cust_id            tran_date store_nm  goods_id gds_grp_nm  \\\n",
       "0        0  2007-01-19 00:00:00      강남점    127105     기초 화장품   \n",
       "1        0  2007-03-30 00:00:00      강남점    342220       니  트   \n",
       "2        0  2007-03-30 00:00:00      강남점    127105     기초 화장품   \n",
       "3        0  2007-03-30 00:00:00      강남점    342205       니  트   \n",
       "4        0  2007-03-30 00:00:00      강남점    342220     상품군미지정   \n",
       "\n",
       "  gds_grp_mclas_nm   amount  \n",
       "0              화장품   850000  \n",
       "1             시티웨어   480000  \n",
       "2              화장품  3000000  \n",
       "3             시티웨어   840000  \n",
       "4               기타    20000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'><p>\n",
    "# Transform data + more features_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>tran_date</th>\n",
       "      <th>store_nm</th>\n",
       "      <th>goods_id</th>\n",
       "      <th>gds_grp_nm</th>\n",
       "      <th>gds_grp_mclas_nm</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2007-01-19 00:00:00</td>\n",
       "      <td>강남점</td>\n",
       "      <td>127105</td>\n",
       "      <td>기초 화장품</td>\n",
       "      <td>화장품</td>\n",
       "      <td>850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2007-03-30 00:00:00</td>\n",
       "      <td>강남점</td>\n",
       "      <td>342220</td>\n",
       "      <td>니  트</td>\n",
       "      <td>시티웨어</td>\n",
       "      <td>480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2007-03-30 00:00:00</td>\n",
       "      <td>강남점</td>\n",
       "      <td>127105</td>\n",
       "      <td>기초 화장품</td>\n",
       "      <td>화장품</td>\n",
       "      <td>3000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2007-03-30 00:00:00</td>\n",
       "      <td>강남점</td>\n",
       "      <td>342205</td>\n",
       "      <td>니  트</td>\n",
       "      <td>시티웨어</td>\n",
       "      <td>840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2007-03-30 00:00:00</td>\n",
       "      <td>강남점</td>\n",
       "      <td>342220</td>\n",
       "      <td>상품군미지정</td>\n",
       "      <td>기타</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cust_id            tran_date store_nm  goods_id gds_grp_nm  \\\n",
       "0        0  2007-01-19 00:00:00      강남점    127105     기초 화장품   \n",
       "1        0  2007-03-30 00:00:00      강남점    342220       니  트   \n",
       "2        0  2007-03-30 00:00:00      강남점    127105     기초 화장품   \n",
       "3        0  2007-03-30 00:00:00      강남점    342205       니  트   \n",
       "4        0  2007-03-30 00:00:00      강남점    342220     상품군미지정   \n",
       "\n",
       "  gds_grp_mclas_nm   amount  \n",
       "0              화장품   850000  \n",
       "1             시티웨어   480000  \n",
       "2              화장품  3000000  \n",
       "3             시티웨어   840000  \n",
       "4               기타    20000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 4203), (2482, 4203))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# soter_nm 추가\n",
    "\n",
    "level = 'gds_grp_nm'\n",
    "\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "train1 = pd.pivot_table(df_all, index='cust_id', columns=level, values='amount',\n",
    "                         aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0). \\\n",
    "                         reset_index(). \\\n",
    "                         query('cust_id not in @IDtest'). \\\n",
    "                         drop(columns=['cust_id']).values\n",
    "test1 = pd.pivot_table(df_all, index='cust_id', columns=level, values='amount',\n",
    "                         aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0). \\\n",
    "                         reset_index(). \\\n",
    "                         query('cust_id in @IDtest'). \\\n",
    "                         drop(columns=['cust_id']).values\n",
    "\n",
    "level = 'gds_grp_mclas_nm'\n",
    "\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "train2 = pd.pivot_table(df_all, index='cust_id', columns=level, values='amount',\n",
    "                         aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0). \\\n",
    "                         reset_index(). \\\n",
    "                         query('cust_id not in @IDtest'). \\\n",
    "                         drop(columns=['cust_id']).values\n",
    "test2 = pd.pivot_table(df_all, index='cust_id', columns=level, values='amount',\n",
    "                         aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0). \\\n",
    "                         reset_index(). \\\n",
    "                         query('cust_id in @IDtest'). \\\n",
    "                         drop(columns=['cust_id']).values\n",
    "\n",
    "level = 'goods_id'\n",
    "\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "train3 = pd.pivot_table(df_all, index='cust_id', columns=level, values='amount',\n",
    "                         aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0). \\\n",
    "                         reset_index(). \\\n",
    "                         query('cust_id not in @IDtest'). \\\n",
    "                         drop(columns=['cust_id']).values\n",
    "test3 = pd.pivot_table(df_all, index='cust_id', columns=level, values='amount',\n",
    "                         aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0). \\\n",
    "                         reset_index(). \\\n",
    "                         query('cust_id in @IDtest'). \\\n",
    "                         drop(columns=['cust_id']).values\n",
    "\n",
    "level = 'store_nm'\n",
    "\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "train4 = pd.pivot_table(df_all, index='cust_id', columns=level, values='amount',\n",
    "                         aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0). \\\n",
    "                         reset_index(). \\\n",
    "                         query('cust_id not in @IDtest'). \\\n",
    "                         drop(columns=['cust_id']).values\n",
    "test4 = pd.pivot_table(df_all, index='cust_id', columns=level, values='amount',\n",
    "                         aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0). \\\n",
    "                         reset_index(). \\\n",
    "                         query('cust_id in @IDtest'). \\\n",
    "                         drop(columns=['cust_id']).values\n",
    "\n",
    "train_add = np.hstack([train1, train2, train3, train4])\n",
    "test_add = np.hstack([test1, test2, test3, test4])\n",
    "\n",
    "train_add.shape, test_add.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder layer에 noise 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='tomato'><font color=\"#CC3D3D\"><p>\n",
    "# GaussianNoise stddev값(0.01) 고정 + Encoder layer Dropout 1개\n",
    "    -> mean=0.77234, std=0.017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-1280aafcaa20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m##### STEP 1: Randomize Seed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mSEED\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;34m'2'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "### GaussianNoise, stddev=0.01\n",
    "\n",
    "# Set hyper-parameters for power mean ensemble \n",
    "N = 10\n",
    "p = 3.5\n",
    "GN_preds_a_4 = []\n",
    "GN_aucs_a_4 = []\n",
    "noise_level = 0.01\n",
    "\n",
    "for i in tqdm(range(N)):    \n",
    "    X_train, X_test = train_add, test_add\n",
    "\n",
    "    ##### STEP 1: Randomize Seed\n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "\n",
    "    ##### STEP 2: Build DAE #####\n",
    "\n",
    "    # Define the encoder dimension\n",
    "    encoding_dim = 128\n",
    "\n",
    "    # Input Layer\n",
    "    input_dim = Input(shape = (X_train.shape[1], ))\n",
    "\n",
    "    # Encoder Layers\n",
    "    noise1 = GaussianNoise(noise_level)(input_dim)\n",
    "    encoded1 = Dense(512, activation = 'relu')(noise1)\n",
    "    noise2 = Dropout(0.1)(encoded1)\n",
    "    \n",
    "    encoded2 = Dense(256, activation = 'relu')(noise2)\n",
    "    encoded3 = Dense(128, activation = 'relu')(encoded2)\n",
    "    encoded4 = Dense(encoding_dim, activation = 'relu')(encoded3)\n",
    "\n",
    "    # Decoder Layers\n",
    "    decoded1 = Dense(128, activation = 'relu')(encoded4)\n",
    "    decoded2 = Dense(256, activation = 'relu')(decoded1)\n",
    "    decoded3 = Dense(512, activation = 'relu')(decoded2)\n",
    "    decoded4 = Dense(X_train.shape[1], activation = 'linear')(decoded3)\n",
    "\n",
    "    # Combine Encoder and Deocder layers\n",
    "    autoencoder = Model(inputs = input_dim, outputs = decoded4)\n",
    "    autoencoder.summary()\n",
    "\n",
    "    # Compile the model\n",
    "    autoencoder.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "    # Train the model\n",
    "    history = autoencoder.fit(X_train, X_train, epochs=20, batch_size=64,\n",
    "                              shuffle=True, validation_data=(X_test,X_test), verbose=0)\n",
    "\n",
    "    print(f'DAE learning curve {i+1}/{N}')\n",
    "    plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    ##### STEP 3: Reduce Dimension #####\n",
    "\n",
    "    # Use a middle Bottleneck Layer to Reduce Dimension\n",
    "    GN_model_a_4 = Model(inputs=input_dim, outputs=encoded4)\n",
    "    X_train = GN_model_a_4.predict(X_train)\n",
    "    X_test = GN_model_a_4.predict(X_test)\n",
    "\n",
    "    ##### STEP 4: Build a DNN Model\n",
    "\n",
    "    # Define the Model architecture\n",
    "    GN_model_a_4 = Sequential()\n",
    "    GN_model_a_4.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],),\n",
    "                           kernel_regularizer=l2(0.01), kernel_initializer='he_normal'))\n",
    "    GN_model_a_4.add(Dropout(0.3))\n",
    "    GN_model_a_4.add(Dense(16, activation='relu'))\n",
    "    GN_model_a_4.add(Dropout(0.3))\n",
    "    GN_model_a_4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Train the Model\n",
    "    GN_model_a_4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc',tf.keras.metrics.AUC()])\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X_train, y_train, test_size=0.2)\n",
    "    history = GN_model_a_4.fit(train_x, train_y, epochs=100, batch_size=64,\n",
    "                               validation_data=(valid_x,valid_y),\n",
    "                               callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',patience=5),\n",
    "                                            keras.callbacks.ModelCheckpoint(filepath='best_model.h5',\n",
    "                                                                            monitor='val_loss',save_best_only=True)],\n",
    "                               verbose=0)\n",
    "\n",
    "    print(f'DNN learning curve {i+1}/{N}')\n",
    "    plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    # Make Prediction\n",
    "    auc = roc_auc_score(valid_y, GN_model_a_4.predict(valid_x).flatten())\n",
    "    GN_aucs_a_4.append(auc)\n",
    "    print('AUC', auc)\n",
    "    GN_preds_a_4.append(GN_model_a_4.predict(X_test).flatten())   \n",
    "\n",
    "### Validate the Models\n",
    "print('\\nValidation Summary:')\n",
    "GN_aucs_a_4 = pd.Series(GN_aucs_a_4)\n",
    "print(GN_aucs_a_4.sort_values(ascending=False))\n",
    "print('mean={:.5f}, std={:.3f}'.format(GN_aucs_a_4.mean(), GN_aucs_a_4.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power mean ensemble\n",
    "THRESHOLD = 0.77  # Use only models whose AUC exceeds this value\n",
    "\n",
    "pred = 0\n",
    "n = 0\n",
    "for i in range(N):\n",
    "    if GN_aucs_a_4.iloc[i] > THRESHOLD:\n",
    "        pred = pred + GN_preds_a_4[i]**p \n",
    "        n += 1\n",
    "pred = pred / n    \n",
    "pred = pred**(1/p)\n",
    "\n",
    "# Make a submission file\n",
    "#t = pd.Timestamp.now()\n",
    "#fname = f\"dae_p{p}n{n}_submit_{t.month:02}{t.day:02}{t.hour:02}{t.minute:02}.csv\"\n",
    "submissions = pd.concat([pd.Series(IDtest, name=\"cust_id\"), pd.Series(pred, name=\"gender\")] ,axis=1)\n",
    "submissions.to_csv('dae_GN_a_4.csv', index=False)\n",
    "#print(f\"'{fname}' is ready to submit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
