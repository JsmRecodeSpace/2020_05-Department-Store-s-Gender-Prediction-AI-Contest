{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#CC3D3D\"><p>\n",
    "# gensim - reproducible training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Library, Read Data\n",
    "2. Feature Making:\n",
    "       - 0. oversampling2: \n",
    "                           nunique 사용X -> 고객들이 산 물품 그대로 모두 뽑아냄, \n",
    "                           비복원추출 -> 복원추출\n",
    "                           많이 산 고객들이 오버샘플링이 더 많이 됨\n",
    "       - 1. EmbeddingVectozier_W2V: (중분류 np.std, np.var 대분류 np.max, np.mean) -> 1200개\n",
    "       - 2. Cosine_Similarity_between_gender_&_Products_W2V: (대, 중, 소 물품들 사이에 gender를 넣어서 학습,\n",
    "                                                   고객별 물품들 gender와의 코사인 유사도를 계산.\n",
    "                                                   이후 모든 코사인 유사도를 각 고객별로 평균을 내어 피쳐로 채택. -> 6개\n",
    "3. Feature Preprocessing: SelectPercentile\n",
    "4. Modeling: Tuning with BayesianOptimization\n",
    "5. Ensemble: Voting, Stacking: Stacking submission 채택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmbeddingVectorzier -W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 장성민1.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile 장성민1.py\n",
    "\n",
    "# 1. Library, Read Data\n",
    "# 2. Feature Making:\n",
    "#        - 0. oversampling2: \n",
    "#                            nunique 사용X -> 고객들이 산 물품 그대로 모두 뽑아냄, \n",
    "#                            비복원추출 -> 복원추출\n",
    "#                           많이 산 고객들이 오버샘플링이 더 많이 됨\n",
    "#        - 1. EmbeddingVectozier_W2V: (중분류 np.std, np.var 대분류 np.max, np.mean) -> 1200개\n",
    "#        - 2. Cosine_Similarity_between_gender_&_Products_W2V: (대, 중, 소 물품들 사이에 gender를 넣어서 학습,\n",
    "#                                                    고객별 물품들 gender와의 코사인 유사도를 계산.\n",
    "#                                                    이후 모든 코사인 유사도를 각 고객별로 평균을 내어 피쳐로 채택. -> 6개\n",
    "# 3. Feature Preprocessing: SelectPercentile\n",
    "# 4. Modeling: Tuning with BayesianOptimization\n",
    "# 5. Ensemble: Voting, Stacking: Stacking submission 채택\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# basic \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50, 'display.max_rows', 200)\n",
    "import os\n",
    "#os.environ[\"PYTHONHASHSEED\"] = \"123\"\n",
    "\n",
    "\n",
    "# plot\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "%matplotlib inline\n",
    "\n",
    "import platform\n",
    "your_os = platform.system()\n",
    "if your_os == 'Linux':\n",
    "    rc('font', family='NanumGothic')\n",
    "elif your_os == 'Windows':\n",
    "    ttf = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "    font_name = font_manager.FontProperties(fname=ttf).get_name()\n",
    "    rc('font', family=font_name)\n",
    "elif your_os == 'Darwin':\n",
    "    rc('font', family='AppleGothic')\n",
    "rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "# models\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "import shap\n",
    "from scipy.stats.mstats import gmean\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from vecstack import StackingTransformer\n",
    "from vecstack import stacking\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from sklearn.pipeline import Pipeline\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "### Read data\n",
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "\n",
    "### Make corpus: corpus는 말뭉치라는 뜻임\n",
    "\n",
    "# oversample2: unique사용 X, 비복원추출 -> 복원추출\n",
    "# data는 X_train 또는 X_test가 될 것\n",
    "# p_level은 대, 중, 소분류중에 하나 넣기('gds_grp_mclas_nm', 'gds_grp_nm', 'goods_id')\n",
    "# n은 몇번 오버샘플링 할 것인지 -> 1일때 기본 + 오버샘플링, 2일때 기본 + 오버샘플링2\n",
    "#                               -> 고객리스트가 10개인데 n값이 1이면 고객리스트를 20개(기본(10) + 오버샘플링(10))로 반환\n",
    "#                               -> 고객리스트가 10개인데 n값이 2이면 고객리스트를 30개(기본(10) + 오버샘플링(20))로 반환\n",
    "\n",
    "def oversample2(data, p_level, n=1, seed=0):    \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    cust_ids = data['cust_id'].unique().tolist() \n",
    "    \n",
    "    customerProducts = []\n",
    "    \n",
    "    for cust_id in cust_ids:\n",
    "        \n",
    "        productLst = data.query(f'cust_id=={cust_id}')[p_level].tolist()\n",
    "        \n",
    "        for j in range(n):\n",
    "   \n",
    "            productLst = list(np.append(productLst, np.random.choice(productLst, len(productLst) * n, replace=True)))\n",
    "        \n",
    "        customerProducts.append(productLst)\n",
    "    \n",
    "    return customerProducts\n",
    "\n",
    "# 중분류 구매목록\n",
    "print('중분류 구매목록 뽑는중')\n",
    "X_train_corpus_nm = oversample2(df_train,'gds_grp_nm', 2)\n",
    "X_test_corpus_nm = oversample2(df_test,'gds_grp_nm', 2)\n",
    "\n",
    "# 중분류 구매목록\n",
    "print('중분류 구매목록 뽑는중')\n",
    "X_train_corpus_mclas_nm = oversample2(df_train,'gds_grp_mclas_nm', 2)\n",
    "X_test_corpus_mclas_nm = oversample2(df_test,'gds_grp_mclas_nm', 2)\n",
    "\n",
    "### Training the Word2Vec model\n",
    "num_features = 300 # 단어 벡터 차원 수\n",
    "min_word_count = 1 # 최소 단어 수\n",
    "context = 10 # 학습 윈도우(인접한 단어 리스트) 크기\n",
    "\n",
    "# 중분류\n",
    "print('중분류 학습중')\n",
    "w2v_nm = word2vec.Word2Vec(X_train_corpus_nm,                  # 학습시킬 단어 리스트\n",
    "                        size=num_features,        # 단어 벡터의 차원 수\n",
    "                        window=context,           # 주변 단어(window)는 안뒤로 몇개까지 볼 것인지\n",
    "                        min_count=min_word_count, # 단어 리스트에서 출현 빈도가 몇번 미만인 단어는 분석에서 제외해라\n",
    "                        workers=4,                # cpu는 쿼드코어를 써라 n_jobs=-1과 같음\n",
    "                        sg = 1,                   # CBOW와 skip-gram중 후자를 선택\n",
    "                        iter=7,\n",
    "                        seed=0)\n",
    "\n",
    "# 필요없는 메모리 unload\n",
    "w2v_nm.init_sims(replace=True)\n",
    "\n",
    "\n",
    "\n",
    "# 대분류\n",
    "print('대분류 학습중')\n",
    "w2v_mclas_nm = word2vec.Word2Vec(X_train_corpus_mclas_nm,                  # 학습시킬 단어 리스트\n",
    "                        size=num_features,        # 단어 벡터의 차원 수\n",
    "                        window=context,           # 주변 단어(window)는 안뒤로 몇개까지 볼 것인지\n",
    "                        min_count=min_word_count, # 단어 리스트에서 출현 빈도가 몇번 미만인 단어는 분석에서 제외해라\n",
    "                        workers=4,                # cpu는 쿼드코어를 써라 n_jobs=-1과 같음\n",
    "                        sg = 1,                   # CBOW와 skip-gram중 후자를 선택\n",
    "                        iter=7,\n",
    "                        seed=0)\n",
    "\n",
    "# 필요없는 메모리 unload\n",
    "w2v_mclas_nm.init_sims(replace=True)\n",
    "\n",
    "### Make features\n",
    "# 구매상품에 해당하는 벡터의 평균/최소/최대 벡터를 feature로 만드는 전처리기(pipeline에서 사용 가능)\n",
    "class EmbeddingVectorizer_nm(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.var([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),          \n",
    "            ]) \n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "### Make features\n",
    "# 구매상품에 해당하는 벡터의 평균/최소/최대 벡터를 feature로 만드는 전처리기(pipeline에서 사용 가능)\n",
    "class EmbeddingVectorizer_mclas(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),               \n",
    "            ]) \n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "# EmbeddingVectorizer 클래스를 통해서 피쳐뽑기(std, max, var -> 600개 피쳐나옴)\n",
    "\n",
    "# 중분류\n",
    "Vectorizer = EmbeddingVectorizer_nm(w2v_nm.wv)\n",
    "Vectorizer.fit(X_train_corpus_nm, y_train)\n",
    "\n",
    "X_train_nm = pd.DataFrame(Vectorizer.transform(X_train_corpus_nm))\n",
    "X_test_nm = pd.DataFrame(Vectorizer.transform(X_test_corpus_nm))\n",
    "\n",
    "display(X_train_nm)\n",
    "\n",
    "\n",
    "# EmbeddingVectorizer 클래스를 통해서 피쳐뽑기(std, max, var -> 600개 피쳐나옴)\n",
    "\n",
    "# 중분류\n",
    "Vectorizer = EmbeddingVectorizer_mclas(w2v_mclas_nm.wv)\n",
    "Vectorizer.fit(X_train_corpus_mclas_nm, y_train)\n",
    "\n",
    "X_train_mclas = pd.DataFrame(Vectorizer.transform(X_train_corpus_mclas_nm))\n",
    "X_test_mclas = pd.DataFrame(Vectorizer.transform(X_test_corpus_mclas_nm))\n",
    "\n",
    "display(X_train_mclas)\n",
    "\n",
    "X_train_mix =  pd.concat([X_train_nm, X_train_mclas], axis=1)\n",
    "X_test_mix = pd.concat([X_test_nm, X_test_mclas], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "### Read data\n",
    "# y_train에서 .gender 지워준거 말고 다 같음\n",
    "\n",
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "y_train.gender = y_train.gender.astype(str)\n",
    "\n",
    "tr = pd.merge(df_train, y_train, on='cust_id')\n",
    "tr2 = pd.concat([df_train, df_test])\n",
    "y_train = y_train.gender\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4 , shuffle=True, random_state=0)\n",
    "\n",
    "# 말뭉치(corpus) 제작전 뽑기\n",
    "\n",
    "before_corpus_goods = []\n",
    "before_corpus_nm = []\n",
    "before_corpus_mclas = []\n",
    "\n",
    "for i in range(len(tr)):\n",
    "    \n",
    "    goods = tr.loc[i, 'goods_id']\n",
    "    nm = tr.loc[i, 'gds_grp_nm']\n",
    "    mclas = tr.loc[i, 'gds_grp_mclas_nm']\n",
    "    gender = tr.loc[i, 'gender']\n",
    "    \n",
    "    before_corpus_goods.append([goods, gender])\n",
    "    before_corpus_nm.append([nm, gender])\n",
    "    before_corpus_mclas.append([mclas, gender])\n",
    "    \n",
    "# 말뭉치 만들 칼럼 넣기\n",
    "tr['before_corpus_goods'] = before_corpus_goods\n",
    "tr['before_corpus_nm'] = before_corpus_nm\n",
    "tr['before_corpus_mclas'] = before_corpus_mclas\n",
    "\n",
    "# 말뭉치 칼럼 제작, 말뭉치 단어 몇개들어가 있는지\n",
    "\n",
    "corpus_df = pd.DataFrame(tr.groupby('cust_id')['before_corpus_goods'].agg(lambda x: [j for i in x for j in i]))\n",
    "corpus_df['before_corpus_nm'] = tr.groupby('cust_id')['before_corpus_nm'].agg(lambda x: [j for i in x for j in i])\n",
    "corpus_df['before_corpus_mclas'] = tr.groupby('cust_id')['before_corpus_mclas'].agg(lambda x: [j for i in x for j in i])\n",
    "\n",
    "display(corpus_df)\n",
    "\n",
    "corpus1 = corpus_df['before_corpus_goods']\n",
    "corpus2 = corpus_df['before_corpus_nm']\n",
    "corpus3 = corpus_df['before_corpus_mclas']\n",
    "\n",
    "\n",
    "def oversample3(data, n=1, seed=0):    \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    customerProducts = []\n",
    "    \n",
    "    for cor in data:\n",
    "            \n",
    "        cor = list(np.append(cor, np.random.choice(cor, len(cor) * n, replace=True)))\n",
    "        \n",
    "        customerProducts.append(cor)\n",
    "    \n",
    "    return customerProducts\n",
    "\n",
    "corpus1_oversample = oversample3(corpus1, n=20, seed=0)\n",
    "corpus2_oversample = oversample3(corpus2, n=20, seed=0)\n",
    "corpus3_oversample = oversample3(corpus3, n=20, seed=0)\n",
    "\n",
    "# W2V 학습\n",
    "\n",
    "num_features = 20 # 문자 벡터 차원 수\n",
    "min_word_count = 0 # 최소 문자 수\n",
    "num_workers = 4 # 병렬 처리 스레드 수\n",
    "context = 3 # 문자열 창 크기\n",
    "\n",
    "print('첫번째 w2v모델 학습 진행')\n",
    "wv_model1 = word2vec.Word2Vec(corpus1_oversample, \n",
    "                          workers=num_workers, \n",
    "                          size=num_features, \n",
    "                          min_count=min_word_count,\n",
    "                          window=context)\n",
    "print('두번째 w2v모델 학습 진행')\n",
    "wv_model2 = word2vec.Word2Vec(corpus2_oversample, \n",
    "                          workers=num_workers, \n",
    "                          size=num_features, \n",
    "                          min_count=min_word_count,\n",
    "                          window=context)\n",
    "print('세번째 w2v모델 학습 진행')\n",
    "wv_model3 = word2vec.Word2Vec(corpus3_oversample, \n",
    "                          workers=num_workers, \n",
    "                          size=num_features, \n",
    "                          min_count=min_word_count,\n",
    "                          window=context)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## goods_id\n",
    "\n",
    "def get_0_similarity_goods(product):\n",
    "    try:\n",
    "        sim = wv_model1.similarity('0', f'{product}')\n",
    "    except:\n",
    "        sim = np.nan\n",
    "    return sim\n",
    "\n",
    "def get_1_similarity_goods(product):\n",
    "    try:\n",
    "        sim = wv_model1.similarity('1', f'{product}')\n",
    "    except:\n",
    "        sim = np.nan\n",
    "    return sim\n",
    "\n",
    "## gds_grp_nm\n",
    "\n",
    "def get_0_similarity_nm(product):\n",
    "    try:\n",
    "        sim = wv_model2.similarity('0', f'{product}')\n",
    "    except:\n",
    "        sim = np.nan\n",
    "    return sim\n",
    "\n",
    "def get_1_similarity_nm(product):\n",
    "    try:\n",
    "        sim = wv_model2.similarity('1', f'{product}')\n",
    "    except:\n",
    "        sim = np.nan\n",
    "    return sim\n",
    "\n",
    "## gds_grp_mclas_nm\n",
    "\n",
    "def get_0_similarity_mclas(product):\n",
    "    try:\n",
    "        sim = wv_model3.similarity('0', f'{product}')\n",
    "    except:\n",
    "        sim = np.nan\n",
    "    return sim\n",
    "\n",
    "def get_1_similarity_mclas(product):\n",
    "    try:\n",
    "        sim = wv_model3.similarity('1', f'{product}')\n",
    "    except:\n",
    "        sim = np.nan\n",
    "    return sim\n",
    "\n",
    "\n",
    "\n",
    "tr2['goods_0_similarity'] = tr2['goods_id'].apply(get_0_similarity_goods)\n",
    "tr2['goods_1_similarity'] = tr2['goods_id'].apply(get_1_similarity_goods)\n",
    "tr2['nm_0_similarity'] = tr2['gds_grp_nm'].apply(get_0_similarity_nm)\n",
    "tr2['nm_1_similarity'] = tr2['gds_grp_nm'].apply(get_1_similarity_nm)\n",
    "tr2['mclas_0_similarity'] = tr2['gds_grp_mclas_nm'].apply(get_0_similarity_mclas)\n",
    "tr2['mclas_1_similarity'] = tr2['gds_grp_mclas_nm'].apply(get_1_similarity_mclas)\n",
    "\n",
    "\n",
    "train_test = pd.DataFrame({'cust_id' : range(5982)}).set_index('cust_id')\n",
    "\n",
    "train_test['goods_0_similarity'] = tr2.groupby('cust_id')['goods_0_similarity'].apply(lambda x: np.nanmean(x))\n",
    "train_test['goods_1_similarity'] = tr2.groupby('cust_id')['goods_1_similarity'].apply(lambda x: np.nanmean(x))\n",
    "train_test['nm_0_similarity'] = tr2.groupby('cust_id')['nm_0_similarity'].apply(lambda x: np.nanmean(x))\n",
    "train_test['nm_1_similarity'] = tr2.groupby('cust_id')['nm_1_similarity'].apply(lambda x: np.nanmean(x))\n",
    "train_test['mclas_0_similarity'] = tr2.groupby('cust_id')['mclas_0_similarity'].apply(lambda x: np.nanmean(x))\n",
    "train_test['mclas_1_similarity'] = tr2.groupby('cust_id')['mclas_1_similarity'].apply(lambda x: np.nanmean(x))\n",
    "\n",
    "\n",
    "X_train_w2v = train_test[:3500]\n",
    "X_test_w2v = train_test[3500:]\n",
    "\n",
    "\n",
    "X_train_experiment = pd.concat([mix, X_train_w2v], axis=1)\n",
    "X_train_experiment\n",
    "\n",
    "X_test_experiment = pd.concat([X_test_mix, X_test_w2v], axis=1)\n",
    "X_test_experiment = X_test_experiment.drop('cust_id', axis=1)\n",
    "X_test_experiment\n",
    "\n",
    "logreg = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "lgbm = LGBMClassifier(n_jobs=-1, random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "\n",
    "models = [logreg, lgbm, rf]\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    cv_scores = cross_val_score(model, X_train_experiment, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "    print(model.__class__.__name__,cv_scores)\n",
    "    print('최대성능 :', max(cv_scores))\n",
    "    print('평균성능 :', np.mean(cv_scores))\n",
    "\n",
    "X_train_experiment.to_csv('X_train_corpus_nm&mclas.csv', index=False, encoding='cp949')\n",
    "X_test_experiment.to_csv('X_test_corpus_nm&mclas.csv', index=False, encoding='cp949')\n",
    "\n",
    "\n",
    "##  Modeling\n",
    "\n",
    "X_train = pd.read_csv('X_train_corpus_nm&mclas.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv').gender\n",
    "X_test = pd.read_csv('X_test_corpus_nm&mclas.csv', encoding='cp949')\n",
    "\n",
    "logreg = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "gbm = GradientBoostingClassifier(random_state=0)\n",
    "lgbm = LGBMClassifier(random_state=0 ,n_jobs=-1)\n",
    "\n",
    "models = [logreg, rf, gbm, lgbm]\n",
    "\n",
    "models = [logreg]\n",
    "\n",
    "\n",
    "# 6개의 모델을 이용해서 가장 잘나온 p를 뽑을 것임\n",
    "for model in models:\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    # 퍼센타일을 5~100프로 모두 살피기 <- 처음에만 100프로 찍고 이후 조절하기\n",
    "    for percentile in tqdm(range(5,100)):\n",
    "    \n",
    "        X_new = SelectPercentile(percentile = percentile).fit_transform(X_train,y_train)\n",
    "       \n",
    "        # cross_val_score 4번의 평균값 (정수시 skf로 자동으로 들어간다)\n",
    "        cv_score = cross_val_score(model, X_new, y_train, scoring='roc_auc', cv=skf).mean()\n",
    "        \n",
    "        cv_scores.append((percentile, cv_score))\n",
    "        \n",
    "    # 베스트 percentile과 점수 출력\n",
    "    best_score = cv_scores[np.argmax([score for _, score in cv_scores])]\n",
    "    print(model.__class__.__name__, best_score)\n",
    "    \n",
    "    # 모델별 percentile에 따른 성능 그림\n",
    "    plt.plot([p for p,_ in cv_scores], [score for _, score in cv_scores])\n",
    "    plt.xlabel('Percent of features')\n",
    "    plt.legend(loc=0)\n",
    "    plt.grid()\n",
    "\n",
    "X_test = X_test.fillna({'goods_0_similarity':np.mean(X_test['goods_0_similarity']), 'goods_1_similarity':np.mean(X_test['goods_1_similarity'])})\n",
    "\n",
    "select_p = SelectPercentile(percentile= 22).fit(X_train, y_train)\n",
    "X_train = select_p.transform(X_train)\n",
    "X_test = select_p.transform(X_test)\n",
    "\n",
    "pd.DataFrame(X_train).to_csv('X_train_after_percentile_nm&mclas.csv',index=False,encoding='cp949')\n",
    "pd.DataFrame(X_test).to_csv('X_test_after_percentile_nm&mclas.csv',index=False,encoding='cp949')\n",
    "\n",
    "X_train = pd.read_csv('X_train_after_percentile_nm&mclas.csv', encoding='cp949')\n",
    "X_test = pd.read_csv('X_test_after_percentile_nm&mclas.csv', encoding='cp949')\n",
    "\n",
    "logreg = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "gbm = GradientBoostingClassifier(random_state=0)\n",
    "lgbm = LGBMClassifier(random_state=0, n_jobs=-1)\n",
    "\n",
    "models = [logreg, rf, gbm, lgbm]\n",
    "\n",
    "# 모델별로 총 8개의 모델로 기본성능을 보고 평균적인 성능을 예측한다.\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    lucky_seed = [2016, 2533]\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    for rs in lucky_seed:\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=4 , shuffle=True, random_state=rs)\n",
    "        \n",
    "        scores = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv = skf)\n",
    "        \n",
    "        cv_scores.append(scores)\n",
    "        \n",
    "    print(f'{model.__class__.__name__}, \\ncv성능들: {cv_scores}\\n최고성능: {max([score for scoreArr in cv_scores for score in scoreArr])}\\n평균성능: {np.mean(cv_scores)}\\n')\n",
    "\n",
    "##  Ensemble\n",
    "\n",
    "# 튜닝 진행하며 미리 init_points=20, n_iter=10 으로 때려보고 파라미터 조절해가며 최고성능 더 끌어내기\n",
    "# 파라미터 조절이후 지속적으로 안정적인 성능이 나오면 50,50 안때려도 됨. 50,50은 파라미터 튜닝 귀찮을때\n",
    "\n",
    "BO_tuned_clfs = []\n",
    "\n",
    "### Basian LR\n",
    "\n",
    "# 하이퍼 파라미터 범위\n",
    "\n",
    "pbounds = { 'C': (.05,.7),}\n",
    "\n",
    "\n",
    "def logreg_opt(C):\n",
    "    \n",
    "    params = {\n",
    "        'C' : C\n",
    "    }\n",
    "\n",
    "    logreg = LogisticRegression(**params, n_jobs=-1, random_state=50)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=4 , shuffle=False, random_state=50)\n",
    "    \n",
    "    score = cross_val_score(logreg, X_train, y_train, scoring='roc_auc', cv=skf, n_jobs=-1)\n",
    "    \n",
    "    return np.mean(score)\n",
    "\n",
    "\n",
    "BO_logreg = BayesianOptimization(f = logreg_opt, pbounds = pbounds, random_state=0)\n",
    "\n",
    "BO_logreg.maximize(init_points=40, n_iter=10)\n",
    "\n",
    "# BO_rf.res  # 모든 성능 들어가있음\n",
    "BO_logreg.max\n",
    "\n",
    "max_params = BO_logreg.max['params']\n",
    "\n",
    "max_params\n",
    "\n",
    "\n",
    "logreg_clf = LogisticRegression(**max_params,  n_jobs=-1, random_state=50)\n",
    "\n",
    "scores = cross_val_score(logreg_clf, X_train, y_train, scoring='roc_auc', cv=skf, n_jobs=-1)\n",
    "\n",
    "print(scores)\n",
    "print(f'최대성능: {max(scores)}\\n평균성능: {np.mean(scores)}')\n",
    "\n",
    "BO_tuned_clfs.append((logreg_clf.__class__.__name__, logreg_clf, max(scores)))\n",
    "\n",
    "### Basian RF\n",
    "\n",
    "# 하이퍼 파라미터 범위\n",
    "\n",
    "pbounds = { 'n_estimators': (200,260),\n",
    "            'max_depth': (5,15), \n",
    "            'max_features': (0.8,0.95),\n",
    "            'min_samples_leaf': (1, 5)}\n",
    "\n",
    "def rf_opt(n_estimators, max_depth, max_features, min_samples_leaf):\n",
    "    \n",
    "    params = {\n",
    "        'n_estimators' : int(round(n_estimators)),\n",
    "        'max_depth' : int(round(max_depth)),\n",
    "        'min_samples_leaf' : int(round(min_samples_leaf))\n",
    "    }\n",
    "\n",
    "    rf = RandomForestClassifier(**params, n_jobs=-1, random_state=50)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=4 , shuffle=False, random_state=50)\n",
    "    \n",
    "    score = cross_val_score(rf, X_train, y_train, scoring='roc_auc', cv=skf, n_jobs=-1)\n",
    "    \n",
    "    return np.mean(score)\n",
    "\n",
    "\n",
    "BO_rf = BayesianOptimization(f = rf_opt, pbounds = pbounds, random_state=0)\n",
    "\n",
    "BO_rf.maximize(init_points=20, n_iter=10)\n",
    "\n",
    "# BO_rf.res  # 모든 성능 들어가있음\n",
    "BO_rf.max\n",
    "\n",
    "max_params = BO_rf.max['params']\n",
    "\n",
    "max_params['n_estimators'] = int(round(max_params['n_estimators']))\n",
    "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
    "max_params['min_samples_leaf'] = int(round(max_params['min_samples_leaf']))\n",
    "\n",
    "max_params\n",
    "\n",
    "rf_clf = RandomForestClassifier(**max_params,  n_jobs=-1, random_state=50)\n",
    "\n",
    "scores = cross_val_score(rf_clf, X_train, y_train, scoring='roc_auc', cv=skf, n_jobs=-1)\n",
    "\n",
    "print(scores)\n",
    "print(f'최대성능: {max(scores)}\\n평균성능: {np.mean(scores)}')\n",
    "\n",
    "BO_tuned_clfs.append((rf_clf.__class__.__name__, rf_clf, max(scores)))\n",
    "\n",
    "### Basian LGBM\n",
    "\n",
    "pbounds = { 'learning_rate': (0.05, 1.5),\n",
    "            'n_estimators': (90, 150),\n",
    "            'max_depth': (3,10),   \n",
    "            'subsample': (0.8,0.95), \n",
    "            'colsample_bytree': (0.75,0.9),   \n",
    "            'num_leaves': (2,10),\n",
    "            'min_child_weight': (1, 7)}\n",
    "\n",
    "\n",
    "def lgbm_opt(learning_rate, n_estimators, max_depth, subsample, colsample_bytree, num_leaves, min_child_weight):\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_estimators' : int(round(n_estimators)),\n",
    "        'max_depth' : int(round(max_depth)),\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree' : colsample_bytree,\n",
    "        'num_leaves' : int(round(num_leaves)),\n",
    "        'min_child_weight' : min_child_weight,\n",
    "        'n_jobs' : -1\n",
    "    }\n",
    "    \n",
    "    lgbm = LGBMClassifier(**params)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=4 , shuffle=False, random_state=50)\n",
    "\n",
    "    score = cross_val_score(lgbm, X_train, y_train, scoring='roc_auc', cv=skf, n_jobs=-1)\n",
    "    \n",
    "    return np.mean(score)\n",
    "\n",
    "BO_lgbm = BayesianOptimization(f = lgbm_opt, pbounds = pbounds, random_state=1)    \n",
    "\n",
    "\n",
    "BO_lgbm.maximize(init_points=20, n_iter=10)\n",
    "\n",
    "# BO_rf.res  # 모든 성능 들어가있음\n",
    "BO_lgbm.max\n",
    "\n",
    "max_params = BO_lgbm.max['params']\n",
    "\n",
    "max_params['n_estimators'] = int(round(max_params['n_estimators']))\n",
    "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
    "max_params['num_leaves'] = int(round(max_params['num_leaves']))\n",
    "\n",
    "max_params\n",
    "\n",
    "lgbm_clf = LGBMClassifier(**max_params)\n",
    "\n",
    "scores = cross_val_score(lgbm_clf, X_train, y_train, scoring='roc_auc', cv=skf, n_jobs=-1)\n",
    "\n",
    "print(scores)\n",
    "print(f'최대성능: {max(scores)}\\n평균성능: {np.mean(scores)}')\n",
    "\n",
    "BO_tuned_clfs.append((lgbm_clf.__class__.__name__, lgbm_clf, max(scores)))\n",
    "\n",
    "### Basian GB\n",
    "\n",
    "pbounds = { 'learning_rate': (0.05, 1.5),\n",
    "            'n_estimators': (50, 250),\n",
    "            'max_depth': (3,10),   \n",
    "            'subsample': (0.8,0.95), \n",
    "            'min_samples_split': (2,5),   \n",
    "            'min_samples_leaf': (1,5)}\n",
    "\n",
    "\n",
    "def gb_opt(learning_rate, n_estimators, max_depth, subsample, min_samples_split, min_samples_leaf):\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_estimators' : int(round(n_estimators)),\n",
    "        'max_depth' : int(round(max_depth)),\n",
    "        'subsample': subsample,\n",
    "        'min_samples_split' : int(round(min_samples_split)),\n",
    "        'min_samples_leaf' : int(round(min_samples_leaf))\n",
    "    }\n",
    "    \n",
    "    gb = GradientBoostingClassifier(**params)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=4 , shuffle=False, random_state=50)\n",
    "\n",
    "    score = cross_val_score(gb, X_train, y_train, scoring='roc_auc', cv=skf, n_jobs=-1)\n",
    "    \n",
    "    return np.mean(score)\n",
    "\n",
    "BO_gb = BayesianOptimization(f = gb_opt, pbounds = pbounds, random_state=0)    \n",
    "\n",
    "\n",
    "BO_gb.maximize(init_points=20, n_iter=10)\n",
    "\n",
    "# BO_rf.res  # 모든 성능 들어가있음\n",
    "BO_gb.max\n",
    "\n",
    "max_params = BO_gb.max['params']\n",
    "\n",
    "max_params['n_estimators'] = int(round(max_params['n_estimators']))\n",
    "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
    "max_params['min_samples_leaf'] = int(round(max_params['min_samples_leaf']))\n",
    "max_params['min_samples_split'] = int(round(max_params['min_samples_split']))\n",
    "\n",
    "max_params\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(**max_params)\n",
    "\n",
    "scores = cross_val_score(gb_clf, X_train, y_train, scoring='roc_auc', cv=4, n_jobs=-1)\n",
    "\n",
    "print(scores)\n",
    "print(f'최대성능: {max(scores)}\\n평균성능: {np.mean(scores)}')\n",
    "\n",
    "BO_tuned_clfs.append((gb_clf.__class__.__name__, gb_clf, max(scores)))\n",
    "\n",
    "gb_clf = gbm = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "scores = cross_val_score(gb_clf, X_train, y_train, scoring='roc_auc', cv=skf, n_jobs=-1)\n",
    "\n",
    "print(scores)\n",
    "print(f'최대성능: {max(scores)}\\n평균성능: {np.mean(scores)}')\n",
    "\n",
    "BO_tuned_clfs.append((gb_clf.__class__.__name__, gb_clf, max(scores)))\n",
    "\n",
    "BO_tuned_clfs\n",
    "\n",
    "len(BO_tuned_clfs)\n",
    "\n",
    "\n",
    "### *Ensemble Stacking*\n",
    "\n",
    "stack_estimators = estimators\n",
    "\n",
    "len(stack_estimators)\n",
    "\n",
    "#- S_train, S_test transform\n",
    "\n",
    "S_train, S_test = stacking(stack_estimators,\n",
    "                           X_train, y_train, X_test,\n",
    "                           regression=False, needs_proba=True, metric=None, n_folds=5, stratified=True, shuffle=True,\n",
    "                           random_state=0, verbose=0)\n",
    "\n",
    "# Stacking - Meta_Model: LogReg\n",
    "\n",
    "# -> 피쳐별로 파라미터 범위 간단한 튜닝 필요할 수 있음\n",
    "\n",
    "pbounds = { 'C': (0.05,0.8),}\n",
    "\n",
    "\n",
    "def logreg_meta(C):\n",
    "    \n",
    "    params = {\n",
    "        'C' : C\n",
    "    }\n",
    "\n",
    "    logreg = LogisticRegression(**params, n_jobs=-1, random_state=50)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=4 , shuffle=False, random_state=50)\n",
    "    \n",
    "    score = cross_val_score(logreg, S_train, y_train, scoring='roc_auc', cv=skf, n_jobs=-1)\n",
    "    \n",
    "    return np.mean(score)\n",
    "\n",
    "\n",
    "BO_logreg = BayesianOptimization(f = logreg_meta, pbounds = pbounds, random_state=0)\n",
    "\n",
    "BO_logreg.maximize(init_points=40, n_iter=20)\n",
    "\n",
    "\n",
    "# BO_rf.res  # 모든 성능 들어가있음\n",
    "BO_logreg.max\n",
    "\n",
    "max_params = BO_logreg.max['params']\n",
    "\n",
    "max_params\n",
    "\n",
    "\n",
    "logreg_meta = LogisticRegression(**max_params,  n_jobs=-1, random_state=50)\n",
    "\n",
    "scores = cross_val_score(logreg_meta, S_train, y_train, scoring='roc_auc', cv=skf, n_jobs=-1)\n",
    "\n",
    "print(scores)\n",
    "print(f'최대성능: {max(scores)}\\n평균성능: {np.mean(scores)}')\n",
    "\n",
    "S_train = pd.DataFrame(S_train)\n",
    "\n",
    "model = logreg_meta\n",
    "  \n",
    "for train_index, test_index in skf.split(S_train, y_train):\n",
    "    \n",
    "    x_tra, x_val = S_train.iloc[train_index], S_train.iloc[test_index]\n",
    "    y_tra, y_val = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    pred = model.fit(x_tra, y_tra).predict_proba(x_val)[:,1]\n",
    "    \n",
    "    print(roc_auc_score(y_val, pred))\n",
    "        \n",
    "print(model.__class__.__name__)\n",
    "    \n",
    "stack_prediction = model.predict_proba(S_test)[:,1]\n",
    "\n",
    "\n",
    "# 서브미션 파일 이름 조정\n",
    "\n",
    "# stacking emsemble result\n",
    "\n",
    "pd.DataFrame({'cust_id':np.arange(3500,5982), 'gender':stack_prediction}).set_index('cust_id').to_csv('submission_nm&mclas_stack.csv', index=True, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '장성민.py'], returncode=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(['python', '장성민.py'], env={**os.environ, 'PYTHONHASHSEED': '123'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#CC3D3D\"><p>\n",
    "# End"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
