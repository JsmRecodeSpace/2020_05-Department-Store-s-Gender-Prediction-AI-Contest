{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50, 'display.max_rows', 200)\n",
    "from itertools import combinations\n",
    "\n",
    "# plot\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "import platform\n",
    "your_os = platform.system()\n",
    "if your_os == 'Linux':\n",
    "    rc('font', family='NanumGothic')\n",
    "elif your_os == 'Windows':\n",
    "    ttf = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "    font_name = font_manager.FontProperties(fname=ttf).get_name()\n",
    "    rc('font', family=font_name)\n",
    "elif your_os == 'Darwin':\n",
    "    rc('font', family='AppleGothic')\n",
    "rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "# models\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from scipy.stats.mstats import gmean\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from vecstack import StackingTransformer\n",
    "from vecstack import stacking\n",
    "\n",
    "%run import_modules.py  \n",
    "%matplotlib inline\n",
    "\n",
    "# For DNN modeling\n",
    "import tensorflow as tf\n",
    "\n",
    "# Tensorflow warning off\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "#  tf.logging.set_verbosity(tf.logging.ERROR) <- logging이 버전업그레이드에서 사용 X라함\n",
    "\n",
    "# from tensorflow import set_random_seed \n",
    "# 위에 코드 안됨에 대한 답변: In Tensoflow2 there is no need to perform\n",
    "# from tensorflow import set_random_seed\n",
    "# in order to run\n",
    "# set_random_seed(x)\n",
    "# (as it was in older version)\n",
    "# Only have to run\n",
    "# import tensorflow\n",
    "# tensorflow.random.set_seed(x)\n",
    "\n",
    "tf.random.set_seed(2020)\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import * #Input, Dense\n",
    "from keras.models import * #Model\n",
    "from keras.optimizers import *\n",
    "from keras.initializers import *\n",
    "from keras.regularizers import *\n",
    "from keras.utils.np_utils import *\n",
    "from keras.utils.vis_utils import * #model_to_dot\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dd7e3527fd56e5b5855c6846ca226f5f860f725b"
   },
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "794123064b09797f51add356a676fb6a7aed015d"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv').gender\n",
    "IDtest = df_test.cust_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8fdb273f73bbee0a8f11b33be46446b6825e9b6a"
   },
   "source": [
    "### Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.goods_id.nunique(), df_train.gds_grp_nm.nunique(), df_train.gds_grp_mclas_nm.nunique()\n",
    "#df_train.store_nm.astype('category').cat.codes\n",
    "#pd.to_datetime(df_train.tran_date).dt.weekday\n",
    "\n",
    "max_features = 100000\n",
    "max_len = 100\n",
    "emb_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### low level: goods_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 100), (2482, 100))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts a \"goods_id\" to a sequence of indexes in a fixed-size hashing space\n",
    "df_train.goods_id = df_train.goods_id.apply(lambda x: str(x))\n",
    "df_test.goods_id = df_test.goods_id.apply(lambda x: str(x))\n",
    "X_train = df_train.groupby('cust_id')['goods_id'].apply(lambda x: [one_hot(i, max_features)[0] for i in x]).values\n",
    "X_test = df_test.groupby('cust_id')['goods_id'].apply(lambda x: [one_hot(i, max_features)[0] for i in x]).values\n",
    "\n",
    "# Pads sequences to the same length\n",
    "X_train_low = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test_low = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "X_train_low.shape, X_test_low.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### middle level: gds_grp_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "1d8108fcba83b0743ebefafd65d491e79fd1c4bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 100), (2482, 100))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts a \"gds_grp_nm\" to a sequence of indexes in a fixed-size hashing space\n",
    "X_train = df_train.groupby('cust_id')['gds_grp_nm'].apply(lambda x: [one_hot(i, max_features//10)[0] for i in x]).values\n",
    "X_test = df_test.groupby('cust_id')['gds_grp_nm'].apply(lambda x: [one_hot(i, max_features//10)[0] for i in x]).values\n",
    "\n",
    "# Pads sequences to the same length\n",
    "X_train_mid = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test_mid = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "X_train_mid.shape, X_test_mid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### high level: gds_grp_mclas_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 100), (2482, 100))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts a \"gds_grp_mclas_nm\" to a sequence of indexes in a fixed-size hashing space\n",
    "X_train = df_train.groupby('cust_id')['gds_grp_mclas_nm'].apply(lambda x: [one_hot(i, max_features//100)[0] for i in x]).values\n",
    "X_test = df_test.groupby('cust_id')['gds_grp_mclas_nm'].apply(lambda x: [one_hot(i, max_features//100)[0] for i in x]).values\n",
    "\n",
    "# Pads sequences to the same length\n",
    "X_train_high = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test_high = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "X_train_high.shape, X_test_high.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 100), (2482, 100))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts a \"store_nm\" to a sequence of indexes in a fixed-size hashing space\n",
    "X_train = df_train.groupby('cust_id')['store_nm'].apply(lambda x: [one_hot(i, max_features//100)[0] for i in x]).values\n",
    "X_test = df_test.groupby('cust_id')['store_nm'].apply(lambda x: [one_hot(i, max_features//100)[0] for i in x]).values\n",
    "\n",
    "# Pads sequences to the same length\n",
    "X_train_str = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test_str = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "X_train_str.shape, X_test_str.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 100), (2482, 100))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts a \"tran_date\" to a sequence of indexes in a fixed-size hashing space\n",
    "df_train.tran_date = pd.to_datetime(df_train.tran_date).dt.weekday\n",
    "df_test.tran_date = pd.to_datetime(df_test.tran_date).dt.weekday\n",
    "X_train = df_train.groupby('cust_id')['tran_date'].apply(lambda x: list(x)).values\n",
    "X_test = df_test.groupby('cust_id')['tran_date'].apply(lambda x: list(x)).values\n",
    "\n",
    "# Pads sequences to the same length\n",
    "X_train_day = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test_day = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "X_train_day.shape, X_test_day.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "756511749ed6b1dcd34e791c15f041d4e47f9df4"
   },
   "source": [
    "summation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 500), (2482, 500))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.DataFrame(np.hstack([X_train_low, X_train_mid,X_train_high, X_train_str,X_train_day]))\n",
    "X_test = pd.DataFrame(np.hstack([X_test_low, X_test_mid,X_test_high, X_test_str,X_test_day]))\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.goods_id.nunique(), df_train.gds_grp_nm.nunique(), df_train.gds_grp_mclas_nm.nunique()\n",
    "#df_train.store_nm.astype('category').cat.codes\n",
    "#pd.to_datetime(df_train.tran_date).dt.weekday\n",
    "\n",
    "max_features = 100000\n",
    "max_len = 100\n",
    "emb_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>...</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68926</td>\n",
       "      <td>59486</td>\n",
       "      <td>30530</td>\n",
       "      <td>30534</td>\n",
       "      <td>76084</td>\n",
       "      <td>76084</td>\n",
       "      <td>43439</td>\n",
       "      <td>98720</td>\n",
       "      <td>60292</td>\n",
       "      <td>47225</td>\n",
       "      <td>56032</td>\n",
       "      <td>58156</td>\n",
       "      <td>56032</td>\n",
       "      <td>45564</td>\n",
       "      <td>72232</td>\n",
       "      <td>30530</td>\n",
       "      <td>99569</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3500 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7      8      9      10     11   \\\n",
       "0       0    0    0    0    0    0    0    0      0      0      0      0   \n",
       "1       0    0    0    0    0    0    0    0      0      0      0      0   \n",
       "2       0    0    0    0    0    0    0    0      0      0      0      0   \n",
       "3       0    0    0    0    0    0    0    0      0      0      0      0   \n",
       "4       0    0    0    0    0    0    0    0      0      0      0      0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...    ...    ...    ...    ...   \n",
       "3495    0    0    0    0    0    0    0    0      0      0      0      0   \n",
       "3496    0    0    0    0    0    0    0    0      0      0      0      0   \n",
       "3497    0    0    0    0    0    0    0    0      0      0      0      0   \n",
       "3498    0    0    0    0    0    0    0    0      0      0      0      0   \n",
       "3499    0    0    0    0    0    0    0    0  68926  59486  30530  30534   \n",
       "\n",
       "        12     13     14     15     16     17     18     19     20     21   \\\n",
       "0         0      0      0      0      0      0      0      0      0      0   \n",
       "1         0      0      0      0      0      0      0      0      0      0   \n",
       "2         0      0      0      0      0      0      0      0      0      0   \n",
       "3         0      0      0      0      0      0      0      0      0      0   \n",
       "4         0      0      0      0      0      0      0      0      0      0   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "3495      0      0      0      0      0      0      0      0      0      0   \n",
       "3496      0      0      0      0      0      0      0      0      0      0   \n",
       "3497      0      0      0      0      0      0      0      0      0      0   \n",
       "3498      0      0      0      0      0      0      0      0      0      0   \n",
       "3499  76084  76084  43439  98720  60292  47225  56032  58156  56032  45564   \n",
       "\n",
       "        22     23     24   ...  475  476  477  478  479  480  481  482  483  \\\n",
       "0         0      0      0  ...    3    1    3    3    3    6    6    6    6   \n",
       "1         0      0      0  ...    0    0    0    0    0    0    0    0    0   \n",
       "2         0      0      0  ...    0    0    0    0    0    0    0    0    0   \n",
       "3         0      0      0  ...    4    4    1    1    1    3    3    1    1   \n",
       "4         0      0      0  ...    0    0    0    0    0    0    0    0    0   \n",
       "...     ...    ...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "3495      0      0      0  ...    0    0    0    0    0    0    0    0    0   \n",
       "3496      0      0      0  ...    0    0    0    0    0    0    0    0    0   \n",
       "3497      0      0      0  ...    0    0    0    0    0    0    0    0    0   \n",
       "3498      0      0      0  ...    0    0    0    0    0    0    0    0    0   \n",
       "3499  72232  30530  99569  ...    4    4    4    4    4    4    5    5    5   \n",
       "\n",
       "      484  485  486  487  488  489  490  491  492  493  494  495  496  497  \\\n",
       "0       6    6    6    0    0    0    0    5    5    5    5    5    5    5   \n",
       "1       0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "2       0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "3       1    6    6    6    6    0    0    6    1    0    0    0    5    3   \n",
       "4       0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "3495    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "3496    0    0    0    4    0    5    1    1    1    5    5    6    5    5   \n",
       "3497    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "3498    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "3499    2    2    2    2    2    6    6    6    6    6    0    0    6    6   \n",
       "\n",
       "      498  499  \n",
       "0       0    0  \n",
       "1       3    3  \n",
       "2       2    2  \n",
       "3       3    4  \n",
       "4       2    2  \n",
       "...   ...  ...  \n",
       "3495    6    6  \n",
       "3496    3    3  \n",
       "3497    0    1  \n",
       "3498    1    3  \n",
       "3499    3    3  \n",
       "\n",
       "[3500 rows x 500 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test DNN -> 0.56 ~ 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "all (InputLayer)             (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 500, 128)          12800000  \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 498, 32)           12320     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 249, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 249, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 247, 32)           3104      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 12,815,457\n",
      "Trainable params: 12,815,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2800 samples, validate on 700 samples\n",
      "Epoch 1/50\n",
      "2800/2800 [==============================] - 9s 3ms/step - loss: 0.6448 - acc: 0.6393 - val_loss: 0.6600 - val_acc: 0.6057\n",
      "Epoch 2/50\n",
      "2800/2800 [==============================] - 9s 3ms/step - loss: 0.5992 - acc: 0.6561 - val_loss: 0.6415 - val_acc: 0.6200\n",
      "Epoch 3/50\n",
      "2800/2800 [==============================] - 9s 3ms/step - loss: 0.5773 - acc: 0.6918 - val_loss: 0.6280 - val_acc: 0.6414\n",
      "Epoch 4/50\n",
      "2800/2800 [==============================] - 9s 3ms/step - loss: 0.5613 - acc: 0.7175 - val_loss: 0.6172 - val_acc: 0.6629\n",
      "Epoch 5/50\n",
      "2800/2800 [==============================] - 9s 3ms/step - loss: 0.5357 - acc: 0.7468 - val_loss: 0.6048 - val_acc: 0.6614\n",
      "Epoch 6/50\n",
      "2800/2800 [==============================] - 10s 4ms/step - loss: 0.4999 - acc: 0.7664 - val_loss: 0.6201 - val_acc: 0.6657\n",
      "Epoch 7/50\n",
      "2800/2800 [==============================] - 10s 3ms/step - loss: 0.4645 - acc: 0.7993 - val_loss: 0.6359 - val_acc: 0.6586\n",
      "0.7037682827735645\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the Model & its Architecture\n",
    "in_all = Input(shape=(max_len,), dtype='int32', name='all')\n",
    "x = Embedding(max_features, emb_dim)(in_all)\n",
    "x = Conv1D(32, 3, activation='elu')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(2)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Conv1D(32, 3, activation='elu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "out_all = Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(in_all, out)\n",
    "model.summary()\n",
    "\n",
    "# Choose the Optimizer and the Cost function\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X_train, y_train, test_size=0.2)\n",
    "# Train the Model\n",
    "history = model.fit(train_x, train_y, epochs=50, batch_size=64, \n",
    "                    validation_data=(valid_x, valid_y), callbacks=[EarlyStopping(patience=2)])\n",
    "\n",
    "print(roc_auc_score(valid_y, model.predict(valid_x)))\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test CNN again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "low (InputLayer)                (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mid (InputLayer)                (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "high (InputLayer)               (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "store (InputLayer)              (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "day (InputLayer)                (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 100, 128)     12800000    low[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, 100, 128)     1280000     mid[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)        (None, 100, 128)     128000      high[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_21 (Embedding)        (None, 100, 128)     128000      store[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_22 (Embedding)        (None, 100, 128)     896         day[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 96, 32)       20512       embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 98, 32)       12320       embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 100, 32)      4128        embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 100, 32)      4128        embedding_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 100, 32)      4128        embedding_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 19, 32)       0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 32, 32)       0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 100, 32)      0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 100, 32)      0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 100, 32)      0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 15, 32)       5152        max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 30, 32)       3104        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 100, 32)      1056        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 100, 32)      1056        max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 100, 32)      1056        max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 32)           0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 32)           0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 32)           0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 32)           0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 32)           0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 32)           0           global_max_pooling1d_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 32)           0           global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 32)           0           global_max_pooling1d_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 32)           0           global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 32)           0           global_max_pooling1d_16[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "my_layer (Add)                  (None, 32)           0           dropout_21[0][0]                 \n",
      "                                                                 dropout_22[0][0]                 \n",
      "                                                                 dropout_23[0][0]                 \n",
      "                                                                 dropout_24[0][0]                 \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1)            33          my_layer[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 14,393,569\n",
      "Trainable params: 14,393,569\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2800 samples, validate on 700 samples\n",
      "Epoch 1/6\n",
      "2800/2800 [==============================] - 9s 3ms/step - loss: 0.6460 - acc: 0.6275 - val_loss: 0.6413 - val_acc: 0.6271\n",
      "Epoch 2/6\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.6361 - acc: 0.6246 - val_loss: 0.6352 - val_acc: 0.6371\n",
      "Epoch 3/6\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.6259 - acc: 0.6436 - val_loss: 0.6299 - val_acc: 0.6371\n",
      "Epoch 4/6\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.6227 - acc: 0.6375 - val_loss: 0.6257 - val_acc: 0.6471\n",
      "Epoch 5/6\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.6136 - acc: 0.6482 - val_loss: 0.6207 - val_acc: 0.6557\n",
      "Epoch 6/6\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.6055 - acc: 0.6575 - val_loss: 0.6161 - val_acc: 0.6586\n",
      "0.7206018557177372\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the Model & its Architecture\n",
    "in_low = Input(shape=(max_len,), dtype='int32', name='low')\n",
    "x = Embedding(max_features, emb_dim)(in_low)\n",
    "x = Conv1D(32, 5, activation='elu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(32, 5, activation='elu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "out_low = Dropout(0.5)(x)\n",
    "\n",
    "in_mid = Input(shape=(max_len,), dtype='int32', name='mid')\n",
    "x = Embedding(max_features//10, emb_dim)(in_mid)\n",
    "x = Conv1D(32, 3, activation='elu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(32, 3, activation='elu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "out_mid = Dropout(0.5)(x)\n",
    "\n",
    "in_high = Input(shape=(max_len,), dtype='int32', name='high')\n",
    "x = Embedding(max_features//100, emb_dim)(in_high)\n",
    "x = Conv1D(32, 1, activation='elu')(x)\n",
    "x = MaxPooling1D(1)(x)\n",
    "x = Conv1D(32, 1, activation='elu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "out_high = Dropout(0.5)(x)\n",
    "\n",
    "in_str = Input(shape=(max_len,), dtype='int32', name='store')\n",
    "x = Embedding(max_features//100, emb_dim)(in_str)\n",
    "x = Conv1D(32, 1, activation='elu')(x)\n",
    "x = MaxPooling1D(1)(x)\n",
    "x = Conv1D(32, 1, activation='elu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "out_str = Dropout(0.5)(x)\n",
    "\n",
    "in_day = Input(shape=(max_len,), dtype='int32', name='day')\n",
    "x = Embedding(7, emb_dim)(in_day)\n",
    "x = Conv1D(32, 1, activation='elu')(x)\n",
    "x = MaxPooling1D(1)(x)\n",
    "x = Conv1D(32, 1, activation='elu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "out_day = Dropout(0.5)(x)\n",
    "\n",
    "x = add([out_low, out_mid, out_high, out_str, out_day], name='my_layer')\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model([in_low, in_mid, in_high, in_str, in_day], out)\n",
    "model.summary()\n",
    "\n",
    "# Choose the Optimizer and the Cost function\n",
    "model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit([X_train_low, X_train_mid, X_train_high, X_train_str, X_train_day], y_train, epochs=6, batch_size=64, \n",
    "                    validation_split=0.2, callbacks=[EarlyStopping(patience=7)])\n",
    "\n",
    "print(roc_auc_score(y_train, model.predict([X_train_low, X_train_mid, X_train_high, X_train_str, X_train_day])))\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8fdb273f73bbee0a8f11b33be46446b6825e9b6a"
   },
   "source": [
    "### Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "079f898dfc7067c71be246a08f80e7d9a411006b"
   },
   "outputs": [],
   "source": [
    "max_features = 100000\n",
    "max_len = 100\n",
    "emb_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3fe366b881e6a45b945f3849d86420ee2b579d63"
   },
   "source": [
    "##### low level: goods_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "056f88f94ce7de7b7c5718ff303860c005720930"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 100), (2482, 100))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts a \"goods_id\" to a sequence of indexes in a fixed-size hashing space\n",
    "df_train.goods_id = df_train.goods_id.apply(lambda x: str(x))\n",
    "df_test.goods_id = df_test.goods_id.apply(lambda x: str(x))\n",
    "X_train = df_train.groupby('cust_id')['goods_id'].apply(lambda x: [one_hot(i, max_features)[0] for i in x]).values\n",
    "X_test = df_test.groupby('cust_id')['goods_id'].apply(lambda x: [one_hot(i, max_features)[0] for i in x]).values\n",
    "\n",
    "# Pads sequences to the same length\n",
    "X_train_low = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test_low = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "X_train_low.shape, X_test_low.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4c057c1c61700fbd2a8480c39c25e69afeaabf4"
   },
   "source": [
    "##### middle level: gds_grp_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "1d8108fcba83b0743ebefafd65d491e79fd1c4bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 100), (2482, 100))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts a \"gds_grp_nm\" to a sequence of indexes in a fixed-size hashing space\n",
    "X_train = df_train.groupby('cust_id')['gds_grp_nm'].apply(lambda x: [one_hot(i, max_features//10)[0] for i in x]).values\n",
    "X_test = df_test.groupby('cust_id')['gds_grp_nm'].apply(lambda x: [one_hot(i, max_features//10)[0] for i in x]).values\n",
    "\n",
    "# Pads sequences to the same length\n",
    "X_train_mid = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test_mid = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "X_train_mid.shape, X_test_mid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e87e1bc9f894afdd432af0e814e48d889576b447"
   },
   "source": [
    "##### high level: gds_grp_mclas_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "9c4e8aee701ca55165fff051a4d732d039a36ed5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 100), (2482, 100))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts a \"gds_grp_mclas_nm\" to a sequence of indexes in a fixed-size hashing space\n",
    "X_train = df_train.groupby('cust_id')['gds_grp_mclas_nm'].apply(lambda x: [one_hot(i, max_features//100)[0] for i in x]).values\n",
    "X_test = df_test.groupby('cust_id')['gds_grp_mclas_nm'].apply(lambda x: [one_hot(i, max_features//100)[0] for i in x]).values\n",
    "\n",
    "# Pads sequences to the same length\n",
    "X_train_high = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test_high = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "X_train_high.shape, X_test_high.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "756511749ed6b1dcd34e791c15f041d4e47f9df4"
   },
   "source": [
    "### Build Bidirectional LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "5e470379b59e72b66cc024f62ba5ac514f7cf15b",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "low (InputLayer)                (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mid (InputLayer)                (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "high (InputLayer)               (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 100, 128)     12800000    low[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 100, 128)     1280000     mid[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 100, 128)     128000      high[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 32)           18560       embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 32)           18560       embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 32)           18560       embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "my_layer (Concatenate)          (None, 96)           0           bidirectional_4[0][0]            \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "                                                                 bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1)            97          my_layer[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 14,263,777\n",
      "Trainable params: 14,263,777\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 2800 samples, validate on 700 samples\n",
      "Epoch 1/5\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 0.6805 - acc: 0.6196 - val_loss: 0.6711 - val_acc: 0.6229\n",
      "Epoch 2/5\n",
      "2800/2800 [==============================] - 10s 4ms/step - loss: 0.6642 - acc: 0.6243 - val_loss: 0.6592 - val_acc: 0.6229\n",
      "Epoch 3/5\n",
      "2800/2800 [==============================] - 10s 4ms/step - loss: 0.6522 - acc: 0.6243 - val_loss: 0.6490 - val_acc: 0.6229\n",
      "Epoch 4/5\n",
      "2800/2800 [==============================] - 10s 4ms/step - loss: 0.6395 - acc: 0.6243 - val_loss: 0.6382 - val_acc: 0.6229\n",
      "Epoch 5/5\n",
      "2800/2800 [==============================] - 10s 4ms/step - loss: 0.6267 - acc: 0.6289 - val_loss: 0.6285 - val_acc: 0.6300\n",
      "0.7011337984457285\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the Model & its Architecture\n",
    "in_low = Input(shape=(max_len,), dtype='int32', name='low')\n",
    "x = Embedding(max_features, emb_dim)(in_low)\n",
    "out_low = Bidirectional(LSTM(16))(x)\n",
    "\n",
    "in_mid = Input(shape=(max_len,), dtype='int32', name='mid')\n",
    "x = Embedding(max_features//10, emb_dim)(in_mid)\n",
    "out_mid = Bidirectional(LSTM(16))(x)\n",
    "\n",
    "in_high = Input(shape=(max_len,), dtype='int32', name='high')\n",
    "x = Embedding(max_features//100, emb_dim)(in_high)\n",
    "out_high = Bidirectional(LSTM(16))(x)\n",
    "\n",
    "x = concatenate([out_low, out_mid, out_high], name='my_layer')\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model([in_low, in_mid, in_high], out)\n",
    "model.summary()\n",
    "\n",
    "# Choose the Optimizer and the Cost function\n",
    "model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit([X_train_low, X_train_mid, X_train_high], y_train, epochs=5, batch_size=64, \n",
    "                    validation_split=0.2, callbacks=[EarlyStopping(patience=5)])\n",
    "\n",
    "print(roc_auc_score(y_train, model.predict([X_train_low, X_train_mid, X_train_high])))\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_test_after_percentile_nm&mclas.csv',encoding='cp949')\n",
    "X_test = pd.read_csv('X_test_after_percentile_nm&mclas.csv',encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>...</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.720856e-02</td>\n",
       "      <td>0.042145</td>\n",
       "      <td>3.624360e-02</td>\n",
       "      <td>0.043737</td>\n",
       "      <td>0.049043</td>\n",
       "      <td>3.638836e-02</td>\n",
       "      <td>2.225512e-02</td>\n",
       "      <td>0.043864</td>\n",
       "      <td>0.046939</td>\n",
       "      <td>0.047247</td>\n",
       "      <td>3.672420e-02</td>\n",
       "      <td>0.052970</td>\n",
       "      <td>0.032652</td>\n",
       "      <td>0.042611</td>\n",
       "      <td>0.029854</td>\n",
       "      <td>0.038975</td>\n",
       "      <td>0.041701</td>\n",
       "      <td>0.027953</td>\n",
       "      <td>6.054522e-02</td>\n",
       "      <td>0.036375</td>\n",
       "      <td>2.744886e-02</td>\n",
       "      <td>0.027893</td>\n",
       "      <td>0.040317</td>\n",
       "      <td>3.533241e-02</td>\n",
       "      <td>3.900481e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078783</td>\n",
       "      <td>-0.015705</td>\n",
       "      <td>-0.070130</td>\n",
       "      <td>0.104986</td>\n",
       "      <td>-0.087078</td>\n",
       "      <td>-0.026219</td>\n",
       "      <td>-0.091528</td>\n",
       "      <td>-0.016858</td>\n",
       "      <td>0.028914</td>\n",
       "      <td>-0.066299</td>\n",
       "      <td>-0.014294</td>\n",
       "      <td>0.034225</td>\n",
       "      <td>-0.051002</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>-0.008437</td>\n",
       "      <td>0.017275</td>\n",
       "      <td>0.012446</td>\n",
       "      <td>-0.052215</td>\n",
       "      <td>-0.019227</td>\n",
       "      <td>-0.114905</td>\n",
       "      <td>0.121020</td>\n",
       "      <td>-0.017553</td>\n",
       "      <td>0.161887</td>\n",
       "      <td>-0.071814</td>\n",
       "      <td>-0.004024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.353416e-02</td>\n",
       "      <td>0.074409</td>\n",
       "      <td>4.255105e-02</td>\n",
       "      <td>0.038985</td>\n",
       "      <td>0.073937</td>\n",
       "      <td>4.231464e-02</td>\n",
       "      <td>5.450580e-02</td>\n",
       "      <td>0.036693</td>\n",
       "      <td>0.034345</td>\n",
       "      <td>0.034598</td>\n",
       "      <td>3.797293e-02</td>\n",
       "      <td>0.050380</td>\n",
       "      <td>0.025835</td>\n",
       "      <td>0.048371</td>\n",
       "      <td>0.039852</td>\n",
       "      <td>0.052264</td>\n",
       "      <td>0.039762</td>\n",
       "      <td>0.036755</td>\n",
       "      <td>3.911033e-02</td>\n",
       "      <td>0.031253</td>\n",
       "      <td>4.213731e-02</td>\n",
       "      <td>0.030002</td>\n",
       "      <td>0.056388</td>\n",
       "      <td>3.503751e-02</td>\n",
       "      <td>3.628049e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096042</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>-0.063846</td>\n",
       "      <td>0.084214</td>\n",
       "      <td>-0.095974</td>\n",
       "      <td>-0.070154</td>\n",
       "      <td>-0.041530</td>\n",
       "      <td>-0.042055</td>\n",
       "      <td>-0.117423</td>\n",
       "      <td>-0.092343</td>\n",
       "      <td>-0.037317</td>\n",
       "      <td>0.009548</td>\n",
       "      <td>-0.045925</td>\n",
       "      <td>-0.025983</td>\n",
       "      <td>0.020410</td>\n",
       "      <td>0.018712</td>\n",
       "      <td>0.011218</td>\n",
       "      <td>-0.041737</td>\n",
       "      <td>-0.112703</td>\n",
       "      <td>0.226192</td>\n",
       "      <td>0.087743</td>\n",
       "      <td>0.326348</td>\n",
       "      <td>0.025749</td>\n",
       "      <td>0.131011</td>\n",
       "      <td>0.034408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.311731e-02</td>\n",
       "      <td>0.041534</td>\n",
       "      <td>4.332975e-02</td>\n",
       "      <td>0.044375</td>\n",
       "      <td>0.039704</td>\n",
       "      <td>3.962666e-02</td>\n",
       "      <td>4.192009e-02</td>\n",
       "      <td>0.043751</td>\n",
       "      <td>0.041240</td>\n",
       "      <td>0.044299</td>\n",
       "      <td>4.419534e-02</td>\n",
       "      <td>0.044535</td>\n",
       "      <td>0.033264</td>\n",
       "      <td>0.047472</td>\n",
       "      <td>0.047831</td>\n",
       "      <td>0.044416</td>\n",
       "      <td>0.047909</td>\n",
       "      <td>0.036949</td>\n",
       "      <td>4.308294e-02</td>\n",
       "      <td>0.027224</td>\n",
       "      <td>4.872717e-02</td>\n",
       "      <td>0.033952</td>\n",
       "      <td>0.040696</td>\n",
       "      <td>3.075889e-02</td>\n",
       "      <td>3.061089e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094258</td>\n",
       "      <td>-0.000912</td>\n",
       "      <td>-0.034913</td>\n",
       "      <td>0.093483</td>\n",
       "      <td>-0.040156</td>\n",
       "      <td>-0.019092</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>-0.042454</td>\n",
       "      <td>-0.123784</td>\n",
       "      <td>-0.084773</td>\n",
       "      <td>-0.028194</td>\n",
       "      <td>0.059326</td>\n",
       "      <td>-0.050264</td>\n",
       "      <td>0.011243</td>\n",
       "      <td>0.019398</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>-0.017388</td>\n",
       "      <td>-0.028387</td>\n",
       "      <td>-0.025638</td>\n",
       "      <td>0.324469</td>\n",
       "      <td>0.232713</td>\n",
       "      <td>0.325795</td>\n",
       "      <td>0.156143</td>\n",
       "      <td>0.204108</td>\n",
       "      <td>0.139712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.113057e-02</td>\n",
       "      <td>0.064852</td>\n",
       "      <td>4.754248e-02</td>\n",
       "      <td>0.041295</td>\n",
       "      <td>0.069944</td>\n",
       "      <td>5.052356e-02</td>\n",
       "      <td>5.514720e-02</td>\n",
       "      <td>0.045604</td>\n",
       "      <td>0.064742</td>\n",
       "      <td>0.041495</td>\n",
       "      <td>4.955265e-02</td>\n",
       "      <td>0.039412</td>\n",
       "      <td>0.051891</td>\n",
       "      <td>0.049352</td>\n",
       "      <td>0.044680</td>\n",
       "      <td>0.053034</td>\n",
       "      <td>0.038611</td>\n",
       "      <td>0.061254</td>\n",
       "      <td>3.943272e-02</td>\n",
       "      <td>0.053118</td>\n",
       "      <td>3.650822e-02</td>\n",
       "      <td>0.051762</td>\n",
       "      <td>0.054078</td>\n",
       "      <td>4.298509e-02</td>\n",
       "      <td>4.325961e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088447</td>\n",
       "      <td>0.013541</td>\n",
       "      <td>-0.061657</td>\n",
       "      <td>0.062981</td>\n",
       "      <td>-0.076516</td>\n",
       "      <td>0.005572</td>\n",
       "      <td>-0.048722</td>\n",
       "      <td>-0.002061</td>\n",
       "      <td>-0.040813</td>\n",
       "      <td>-0.089504</td>\n",
       "      <td>-0.033022</td>\n",
       "      <td>0.043820</td>\n",
       "      <td>-0.011709</td>\n",
       "      <td>0.010612</td>\n",
       "      <td>-0.051576</td>\n",
       "      <td>0.037105</td>\n",
       "      <td>-0.032151</td>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.162965</td>\n",
       "      <td>-0.042897</td>\n",
       "      <td>0.217256</td>\n",
       "      <td>-0.080766</td>\n",
       "      <td>0.054245</td>\n",
       "      <td>-0.034014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.439314e-02</td>\n",
       "      <td>0.043583</td>\n",
       "      <td>4.510792e-02</td>\n",
       "      <td>0.053774</td>\n",
       "      <td>0.033458</td>\n",
       "      <td>4.519232e-02</td>\n",
       "      <td>4.023671e-02</td>\n",
       "      <td>0.047995</td>\n",
       "      <td>0.029674</td>\n",
       "      <td>0.060926</td>\n",
       "      <td>4.244117e-02</td>\n",
       "      <td>0.041576</td>\n",
       "      <td>0.022459</td>\n",
       "      <td>0.022361</td>\n",
       "      <td>0.038346</td>\n",
       "      <td>0.033120</td>\n",
       "      <td>0.020158</td>\n",
       "      <td>0.026132</td>\n",
       "      <td>3.383551e-02</td>\n",
       "      <td>0.017740</td>\n",
       "      <td>3.844339e-02</td>\n",
       "      <td>0.030508</td>\n",
       "      <td>0.030441</td>\n",
       "      <td>2.416564e-02</td>\n",
       "      <td>3.334504e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078274</td>\n",
       "      <td>-0.011854</td>\n",
       "      <td>-0.040545</td>\n",
       "      <td>0.085186</td>\n",
       "      <td>-0.067765</td>\n",
       "      <td>-0.007751</td>\n",
       "      <td>0.026868</td>\n",
       "      <td>-0.031092</td>\n",
       "      <td>-0.089362</td>\n",
       "      <td>-0.063299</td>\n",
       "      <td>-0.012518</td>\n",
       "      <td>0.032725</td>\n",
       "      <td>-0.045640</td>\n",
       "      <td>0.011930</td>\n",
       "      <td>-0.002422</td>\n",
       "      <td>0.019902</td>\n",
       "      <td>-0.015711</td>\n",
       "      <td>-0.036049</td>\n",
       "      <td>-0.037758</td>\n",
       "      <td>0.256876</td>\n",
       "      <td>0.174835</td>\n",
       "      <td>0.281329</td>\n",
       "      <td>0.115436</td>\n",
       "      <td>0.202430</td>\n",
       "      <td>0.080326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>4.637539e-02</td>\n",
       "      <td>0.030179</td>\n",
       "      <td>4.605711e-02</td>\n",
       "      <td>0.045967</td>\n",
       "      <td>0.037490</td>\n",
       "      <td>4.745286e-02</td>\n",
       "      <td>3.333072e-02</td>\n",
       "      <td>0.031836</td>\n",
       "      <td>0.023273</td>\n",
       "      <td>0.031561</td>\n",
       "      <td>4.067503e-02</td>\n",
       "      <td>0.046175</td>\n",
       "      <td>0.041691</td>\n",
       "      <td>0.037972</td>\n",
       "      <td>0.031890</td>\n",
       "      <td>0.043433</td>\n",
       "      <td>0.062462</td>\n",
       "      <td>0.026598</td>\n",
       "      <td>6.079313e-02</td>\n",
       "      <td>0.039255</td>\n",
       "      <td>3.393309e-02</td>\n",
       "      <td>0.035049</td>\n",
       "      <td>0.026044</td>\n",
       "      <td>3.334117e-02</td>\n",
       "      <td>3.914359e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058280</td>\n",
       "      <td>-0.026613</td>\n",
       "      <td>-0.066966</td>\n",
       "      <td>0.084405</td>\n",
       "      <td>-0.094837</td>\n",
       "      <td>-0.049232</td>\n",
       "      <td>-0.119595</td>\n",
       "      <td>-0.073820</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>-0.063159</td>\n",
       "      <td>-0.060510</td>\n",
       "      <td>0.039405</td>\n",
       "      <td>-0.067871</td>\n",
       "      <td>-0.036443</td>\n",
       "      <td>-0.008320</td>\n",
       "      <td>0.010896</td>\n",
       "      <td>0.037790</td>\n",
       "      <td>-0.068629</td>\n",
       "      <td>-0.042366</td>\n",
       "      <td>0.069105</td>\n",
       "      <td>-0.095875</td>\n",
       "      <td>0.105212</td>\n",
       "      <td>0.027083</td>\n",
       "      <td>-0.030293</td>\n",
       "      <td>-0.061545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>3.725290e-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.862645e-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.725290e-09</td>\n",
       "      <td>7.450581e-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.725290e-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.862645e-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.862645e-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.328306e-10</td>\n",
       "      <td>7.450581e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096247</td>\n",
       "      <td>-0.039116</td>\n",
       "      <td>-0.077144</td>\n",
       "      <td>0.095060</td>\n",
       "      <td>-0.105519</td>\n",
       "      <td>0.016244</td>\n",
       "      <td>-0.060533</td>\n",
       "      <td>0.025513</td>\n",
       "      <td>-0.057389</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>0.022483</td>\n",
       "      <td>0.057059</td>\n",
       "      <td>-0.007926</td>\n",
       "      <td>0.010672</td>\n",
       "      <td>-0.052794</td>\n",
       "      <td>0.047730</td>\n",
       "      <td>-0.040079</td>\n",
       "      <td>0.039044</td>\n",
       "      <td>-0.079711</td>\n",
       "      <td>0.286647</td>\n",
       "      <td>-0.188201</td>\n",
       "      <td>0.319889</td>\n",
       "      <td>-0.318131</td>\n",
       "      <td>-0.032594</td>\n",
       "      <td>-0.157397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>4.952636e-02</td>\n",
       "      <td>0.046983</td>\n",
       "      <td>4.101980e-02</td>\n",
       "      <td>0.054742</td>\n",
       "      <td>0.044845</td>\n",
       "      <td>4.380448e-02</td>\n",
       "      <td>4.517845e-02</td>\n",
       "      <td>0.050970</td>\n",
       "      <td>0.046278</td>\n",
       "      <td>0.043911</td>\n",
       "      <td>5.833370e-02</td>\n",
       "      <td>0.058505</td>\n",
       "      <td>0.024218</td>\n",
       "      <td>0.043999</td>\n",
       "      <td>0.025585</td>\n",
       "      <td>0.039419</td>\n",
       "      <td>0.038339</td>\n",
       "      <td>0.054524</td>\n",
       "      <td>5.046827e-02</td>\n",
       "      <td>0.056957</td>\n",
       "      <td>2.976551e-02</td>\n",
       "      <td>0.040574</td>\n",
       "      <td>0.036952</td>\n",
       "      <td>4.365204e-02</td>\n",
       "      <td>3.568205e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082812</td>\n",
       "      <td>-0.017144</td>\n",
       "      <td>-0.072184</td>\n",
       "      <td>0.105484</td>\n",
       "      <td>-0.084091</td>\n",
       "      <td>-0.032068</td>\n",
       "      <td>-0.068522</td>\n",
       "      <td>-0.009878</td>\n",
       "      <td>-0.021369</td>\n",
       "      <td>-0.049510</td>\n",
       "      <td>-0.021831</td>\n",
       "      <td>0.031285</td>\n",
       "      <td>-0.035789</td>\n",
       "      <td>0.028005</td>\n",
       "      <td>-0.006907</td>\n",
       "      <td>-0.005027</td>\n",
       "      <td>0.008545</td>\n",
       "      <td>-0.043471</td>\n",
       "      <td>-0.092145</td>\n",
       "      <td>-0.074386</td>\n",
       "      <td>0.090535</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>0.138496</td>\n",
       "      <td>-0.093343</td>\n",
       "      <td>0.033897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>4.430941e-02</td>\n",
       "      <td>0.068512</td>\n",
       "      <td>4.395704e-02</td>\n",
       "      <td>0.015102</td>\n",
       "      <td>0.036514</td>\n",
       "      <td>3.854300e-02</td>\n",
       "      <td>3.046988e-02</td>\n",
       "      <td>0.035441</td>\n",
       "      <td>0.027283</td>\n",
       "      <td>0.042999</td>\n",
       "      <td>3.015056e-02</td>\n",
       "      <td>0.030841</td>\n",
       "      <td>0.059505</td>\n",
       "      <td>0.039528</td>\n",
       "      <td>0.026530</td>\n",
       "      <td>0.043533</td>\n",
       "      <td>0.058673</td>\n",
       "      <td>0.024796</td>\n",
       "      <td>4.931083e-02</td>\n",
       "      <td>0.026646</td>\n",
       "      <td>2.097439e-02</td>\n",
       "      <td>0.031888</td>\n",
       "      <td>0.058403</td>\n",
       "      <td>6.315978e-02</td>\n",
       "      <td>3.665446e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071723</td>\n",
       "      <td>-0.006557</td>\n",
       "      <td>-0.011120</td>\n",
       "      <td>0.094452</td>\n",
       "      <td>-0.032466</td>\n",
       "      <td>0.010424</td>\n",
       "      <td>0.031339</td>\n",
       "      <td>-0.057466</td>\n",
       "      <td>-0.158556</td>\n",
       "      <td>-0.105530</td>\n",
       "      <td>-0.022240</td>\n",
       "      <td>0.016358</td>\n",
       "      <td>-0.070309</td>\n",
       "      <td>0.024007</td>\n",
       "      <td>-0.065312</td>\n",
       "      <td>0.014719</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>-0.078908</td>\n",
       "      <td>-0.152654</td>\n",
       "      <td>0.222451</td>\n",
       "      <td>0.146587</td>\n",
       "      <td>0.175865</td>\n",
       "      <td>0.105830</td>\n",
       "      <td>0.073114</td>\n",
       "      <td>0.062405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>3.916294e-02</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>5.331018e-02</td>\n",
       "      <td>0.034911</td>\n",
       "      <td>0.022331</td>\n",
       "      <td>1.873562e-02</td>\n",
       "      <td>2.567027e-02</td>\n",
       "      <td>0.030341</td>\n",
       "      <td>0.032435</td>\n",
       "      <td>0.040688</td>\n",
       "      <td>3.459457e-02</td>\n",
       "      <td>0.024868</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>0.072327</td>\n",
       "      <td>0.051227</td>\n",
       "      <td>0.036231</td>\n",
       "      <td>0.051489</td>\n",
       "      <td>0.027555</td>\n",
       "      <td>3.464604e-02</td>\n",
       "      <td>0.020988</td>\n",
       "      <td>5.710298e-02</td>\n",
       "      <td>0.042658</td>\n",
       "      <td>0.031490</td>\n",
       "      <td>3.615029e-02</td>\n",
       "      <td>2.462140e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098260</td>\n",
       "      <td>-0.003398</td>\n",
       "      <td>-0.036076</td>\n",
       "      <td>0.111108</td>\n",
       "      <td>-0.043741</td>\n",
       "      <td>-0.017379</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>-0.052533</td>\n",
       "      <td>-0.105208</td>\n",
       "      <td>-0.093058</td>\n",
       "      <td>-0.034406</td>\n",
       "      <td>0.061566</td>\n",
       "      <td>-0.059850</td>\n",
       "      <td>0.014284</td>\n",
       "      <td>0.023412</td>\n",
       "      <td>0.014464</td>\n",
       "      <td>-0.015158</td>\n",
       "      <td>-0.043058</td>\n",
       "      <td>-0.014935</td>\n",
       "      <td>0.282055</td>\n",
       "      <td>0.288268</td>\n",
       "      <td>0.259324</td>\n",
       "      <td>0.172035</td>\n",
       "      <td>0.130485</td>\n",
       "      <td>0.238318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2482 rows × 266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1             2         3         4             5  \\\n",
       "0     6.720856e-02  0.042145  3.624360e-02  0.043737  0.049043  3.638836e-02   \n",
       "1     5.353416e-02  0.074409  4.255105e-02  0.038985  0.073937  4.231464e-02   \n",
       "2     5.311731e-02  0.041534  4.332975e-02  0.044375  0.039704  3.962666e-02   \n",
       "3     4.113057e-02  0.064852  4.754248e-02  0.041295  0.069944  5.052356e-02   \n",
       "4     5.439314e-02  0.043583  4.510792e-02  0.053774  0.033458  4.519232e-02   \n",
       "...            ...       ...           ...       ...       ...           ...   \n",
       "2477  4.637539e-02  0.030179  4.605711e-02  0.045967  0.037490  4.745286e-02   \n",
       "2478  3.725290e-09  0.000000  1.862645e-09  0.000000  0.000000  3.725290e-09   \n",
       "2479  4.952636e-02  0.046983  4.101980e-02  0.054742  0.044845  4.380448e-02   \n",
       "2480  4.430941e-02  0.068512  4.395704e-02  0.015102  0.036514  3.854300e-02   \n",
       "2481  3.916294e-02  0.022321  5.331018e-02  0.034911  0.022331  1.873562e-02   \n",
       "\n",
       "                 6         7         8         9            10        11  \\\n",
       "0     2.225512e-02  0.043864  0.046939  0.047247  3.672420e-02  0.052970   \n",
       "1     5.450580e-02  0.036693  0.034345  0.034598  3.797293e-02  0.050380   \n",
       "2     4.192009e-02  0.043751  0.041240  0.044299  4.419534e-02  0.044535   \n",
       "3     5.514720e-02  0.045604  0.064742  0.041495  4.955265e-02  0.039412   \n",
       "4     4.023671e-02  0.047995  0.029674  0.060926  4.244117e-02  0.041576   \n",
       "...            ...       ...       ...       ...           ...       ...   \n",
       "2477  3.333072e-02  0.031836  0.023273  0.031561  4.067503e-02  0.046175   \n",
       "2478  7.450581e-09  0.000000  0.000000  0.000000  3.725290e-09  0.000000   \n",
       "2479  4.517845e-02  0.050970  0.046278  0.043911  5.833370e-02  0.058505   \n",
       "2480  3.046988e-02  0.035441  0.027283  0.042999  3.015056e-02  0.030841   \n",
       "2481  2.567027e-02  0.030341  0.032435  0.040688  3.459457e-02  0.024868   \n",
       "\n",
       "            12        13        14        15        16        17  \\\n",
       "0     0.032652  0.042611  0.029854  0.038975  0.041701  0.027953   \n",
       "1     0.025835  0.048371  0.039852  0.052264  0.039762  0.036755   \n",
       "2     0.033264  0.047472  0.047831  0.044416  0.047909  0.036949   \n",
       "3     0.051891  0.049352  0.044680  0.053034  0.038611  0.061254   \n",
       "4     0.022459  0.022361  0.038346  0.033120  0.020158  0.026132   \n",
       "...        ...       ...       ...       ...       ...       ...   \n",
       "2477  0.041691  0.037972  0.031890  0.043433  0.062462  0.026598   \n",
       "2478  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2479  0.024218  0.043999  0.025585  0.039419  0.038339  0.054524   \n",
       "2480  0.059505  0.039528  0.026530  0.043533  0.058673  0.024796   \n",
       "2481  0.035366  0.072327  0.051227  0.036231  0.051489  0.027555   \n",
       "\n",
       "                18        19            20        21        22            23  \\\n",
       "0     6.054522e-02  0.036375  2.744886e-02  0.027893  0.040317  3.533241e-02   \n",
       "1     3.911033e-02  0.031253  4.213731e-02  0.030002  0.056388  3.503751e-02   \n",
       "2     4.308294e-02  0.027224  4.872717e-02  0.033952  0.040696  3.075889e-02   \n",
       "3     3.943272e-02  0.053118  3.650822e-02  0.051762  0.054078  4.298509e-02   \n",
       "4     3.383551e-02  0.017740  3.844339e-02  0.030508  0.030441  2.416564e-02   \n",
       "...            ...       ...           ...       ...       ...           ...   \n",
       "2477  6.079313e-02  0.039255  3.393309e-02  0.035049  0.026044  3.334117e-02   \n",
       "2478  1.862645e-09  0.000000  1.862645e-09  0.000000  0.000000  2.328306e-10   \n",
       "2479  5.046827e-02  0.056957  2.976551e-02  0.040574  0.036952  4.365204e-02   \n",
       "2480  4.931083e-02  0.026646  2.097439e-02  0.031888  0.058403  6.315978e-02   \n",
       "2481  3.464604e-02  0.020988  5.710298e-02  0.042658  0.031490  3.615029e-02   \n",
       "\n",
       "                24  ...       241       242       243       244       245  \\\n",
       "0     3.900481e-02  ... -0.078783 -0.015705 -0.070130  0.104986 -0.087078   \n",
       "1     3.628049e-02  ... -0.096042  0.000898 -0.063846  0.084214 -0.095974   \n",
       "2     3.061089e-02  ... -0.094258 -0.000912 -0.034913  0.093483 -0.040156   \n",
       "3     4.325961e-02  ... -0.088447  0.013541 -0.061657  0.062981 -0.076516   \n",
       "4     3.334504e-02  ... -0.078274 -0.011854 -0.040545  0.085186 -0.067765   \n",
       "...            ...  ...       ...       ...       ...       ...       ...   \n",
       "2477  3.914359e-02  ... -0.058280 -0.026613 -0.066966  0.084405 -0.094837   \n",
       "2478  7.450581e-09  ... -0.096247 -0.039116 -0.077144  0.095060 -0.105519   \n",
       "2479  3.568205e-02  ... -0.082812 -0.017144 -0.072184  0.105484 -0.084091   \n",
       "2480  3.665446e-02  ... -0.071723 -0.006557 -0.011120  0.094452 -0.032466   \n",
       "2481  2.462140e-02  ... -0.098260 -0.003398 -0.036076  0.111108 -0.043741   \n",
       "\n",
       "           246       247       248       249       250       251       252  \\\n",
       "0    -0.026219 -0.091528 -0.016858  0.028914 -0.066299 -0.014294  0.034225   \n",
       "1    -0.070154 -0.041530 -0.042055 -0.117423 -0.092343 -0.037317  0.009548   \n",
       "2    -0.019092  0.005699 -0.042454 -0.123784 -0.084773 -0.028194  0.059326   \n",
       "3     0.005572 -0.048722 -0.002061 -0.040813 -0.089504 -0.033022  0.043820   \n",
       "4    -0.007751  0.026868 -0.031092 -0.089362 -0.063299 -0.012518  0.032725   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2477 -0.049232 -0.119595 -0.073820  0.004163 -0.063159 -0.060510  0.039405   \n",
       "2478  0.016244 -0.060533  0.025513 -0.057389  0.005176  0.022483  0.057059   \n",
       "2479 -0.032068 -0.068522 -0.009878 -0.021369 -0.049510 -0.021831  0.031285   \n",
       "2480  0.010424  0.031339 -0.057466 -0.158556 -0.105530 -0.022240  0.016358   \n",
       "2481 -0.017379  0.003547 -0.052533 -0.105208 -0.093058 -0.034406  0.061566   \n",
       "\n",
       "           253       254       255       256       257       258       259  \\\n",
       "0    -0.051002  0.022624 -0.008437  0.017275  0.012446 -0.052215 -0.019227   \n",
       "1    -0.045925 -0.025983  0.020410  0.018712  0.011218 -0.041737 -0.112703   \n",
       "2    -0.050264  0.011243  0.019398  0.020408 -0.017388 -0.028387 -0.025638   \n",
       "3    -0.011709  0.010612 -0.051576  0.037105 -0.032151  0.031008  0.000131   \n",
       "4    -0.045640  0.011930 -0.002422  0.019902 -0.015711 -0.036049 -0.037758   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2477 -0.067871 -0.036443 -0.008320  0.010896  0.037790 -0.068629 -0.042366   \n",
       "2478 -0.007926  0.010672 -0.052794  0.047730 -0.040079  0.039044 -0.079711   \n",
       "2479 -0.035789  0.028005 -0.006907 -0.005027  0.008545 -0.043471 -0.092145   \n",
       "2480 -0.070309  0.024007 -0.065312  0.014719  0.016599 -0.078908 -0.152654   \n",
       "2481 -0.059850  0.014284  0.023412  0.014464 -0.015158 -0.043058 -0.014935   \n",
       "\n",
       "           260       261       262       263       264       265  \n",
       "0    -0.114905  0.121020 -0.017553  0.161887 -0.071814 -0.004024  \n",
       "1     0.226192  0.087743  0.326348  0.025749  0.131011  0.034408  \n",
       "2     0.324469  0.232713  0.325795  0.156143  0.204108  0.139712  \n",
       "3     0.162965 -0.042897  0.217256 -0.080766  0.054245 -0.034014  \n",
       "4     0.256876  0.174835  0.281329  0.115436  0.202430  0.080326  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "2477  0.069105 -0.095875  0.105212  0.027083 -0.030293 -0.061545  \n",
       "2478  0.286647 -0.188201  0.319889 -0.318131 -0.032594 -0.157397  \n",
       "2479 -0.074386  0.090535  0.008151  0.138496 -0.093343  0.033897  \n",
       "2480  0.222451  0.146587  0.175865  0.105830  0.073114  0.062405  \n",
       "2481  0.282055  0.288268  0.259324  0.172035  0.130485  0.238318  \n",
       "\n",
       "[2482 rows x 266 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model & its Architecture\n",
    "in_low = Input(shape=(max_len,), dtype='int32', name='low')\n",
    "x = Embedding(max_features, emb_dim)(in_low)\n",
    "out_low = Bidirectional(LSTM(16))(x)\n",
    "\n",
    "in_mid = Input(shape=(max_len,), dtype='int32', name='mid')\n",
    "x = Embedding(max_features//10, emb_dim)(in_mid)\n",
    "out_mid = Bidirectional(LSTM(16))(x)\n",
    "\n",
    "in_high = Input(shape=(max_len,), dtype='int32', name='high')\n",
    "x = Embedding(max_features//100, emb_dim)(in_high)\n",
    "out_high = Bidirectional(LSTM(16))(x)\n",
    "\n",
    "x = concatenate([out_low, out_mid, out_high], name='my_layer')\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model([in_low, in_mid, in_high], out)\n",
    "model.summary()\n",
    "\n",
    "# Choose the Optimizer and the Cost function\n",
    "model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit([X_train_low, X_train_mid, X_train_high], y_train, epochs=5, batch_size=64, \n",
    "                    validation_split=0.2, callbacks=[EarlyStopping(patience=5)])\n",
    "\n",
    "print(roc_auc_score(y_train, model.predict([X_train_low, X_train_mid, X_train_high])))\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer bidirectional_22: expected ndim=3, found ndim=4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-5e1cb4601113>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'elu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                     \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                     \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer bidirectional_22: expected ndim=3, found ndim=4"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(1,2,X_train.shape[1])))\n",
    "model.add(Dense(16, activation='elu'))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer bidirectional_14: expected ndim=3, found ndim=1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-de65502d7a58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'elu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer bidirectional_14: expected ndim=3, found ndim=1"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128, input_shape = (X_train.shape[1],))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, input_shape = (X_train.shape[1],),activation = 'elu'))\n",
    "model.add(Dense(64, activation = 'elu'))\n",
    "model.add(Dense(5, activation = 'softmax'))\n",
    "model.build(input_shape=(X_train.shape[1],))\n",
    "model.compile(optimizer = 'adam', metrics = ['acc'], loss = 'categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(20, input_shape=(10, 1), return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_26_input to have 3 dimensions, but got array with shape (2482, 266)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-99170244e094>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# fit model for one epoch on this sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_26_input to have 3 dimensions, but got array with shape (2482, 266)"
     ]
    }
   ],
   "source": [
    "# fit model for one epoch on this sequence\n",
    "model.fit(X_train, y_train, epochs=1, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    " \n",
    "# create a sequence classification instance\n",
    "def get_sequence(n_timesteps):\n",
    "    # create a sequence of random numbers in [0,1]\n",
    "    X = array([random() for _ in range(n_timesteps)])\n",
    "    # calculate cut-off value to change class values\n",
    "    limit = n_timesteps/4.0\n",
    "    # determine the class outcome for each item in cumulative sequence\n",
    "    y = array([0 if x < limit else 1 for x in cumsum(X)])\n",
    "    # reshape input and output data to be suitable for LSTMs\n",
    "    X = X.reshape(1, n_timesteps, 1)\n",
    "    y = y.reshape(1, n_timesteps, 1)\n",
    "    return X, y\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-83f048a22d77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# generate new random sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m# fit model for one epoch on this sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-82-dd77091b6e66>\u001b[0m in \u001b[0;36mget_sequence\u001b[1;34m(n_timesteps)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# create a sequence of random numbers in [0,1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;31m# calculate cut-off value to change class values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_timesteps\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m4.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-82-dd77091b6e66>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# create a sequence of random numbers in [0,1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;31m# calculate cut-off value to change class values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_timesteps\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m4.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# define problem properties\n",
    "n_timesteps = 10\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(n_timesteps, 1)))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# train LSTM\n",
    "for epoch in range(1000):\n",
    "    # generate new random sequence\n",
    "    X,y = get_sequence(n_timesteps)\n",
    "    # fit model for one epoch on this sequence\n",
    "    model.fit(X, y, epochs=1, batch_size=1, verbose=2)\n",
    "# evaluate LSTM\n",
    "X,y = get_sequence(n_timesteps)\n",
    "yhat = model.predict_classes(X, verbose=0)\n",
    "for i in range(n_timesteps):\n",
    "    print('Expected:', y[0, i], 'Predicted', yhat[0, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
