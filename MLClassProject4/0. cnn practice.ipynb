{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangling\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from vecstack import stacking\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "\n",
    "# Keras\n",
    "import tensorflow as tf\n",
    "# Tensorflow warning off\n",
    "if tf.__version__[0] < '2':\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import * #Input, Dense\n",
    "from keras.models import * #Model\n",
    "from keras.optimizers import *\n",
    "from keras.initializers import *\n",
    "from keras.regularizers import *\n",
    "from keras.constraints import *\n",
    "from keras.utils.np_utils import *\n",
    "from keras.utils.vis_utils import * #model_to_dot\n",
    "from keras.preprocessing.image import *\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import *\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras import Input\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.constraints import max_norm\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read data & set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test['cust_id'].unique()\n",
    "\n",
    "# set seed\n",
    "seed = 2020\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기, 점, 콤마, 세로바 없애기\n",
    "\n",
    "before = [\n",
    "# 띄어쓰기\n",
    "'기초 화장품', '니  트', '캐릭터 여화', '캐릭터 남화', '수      저', '4대 B/D', 'N/B 아동복', '수입 아동복', '스낵형 델리','수입청소기 외','영 캐릭터','영 트렌디','식기 단독매입','커       튼','생활잡화 단독매입',\n",
    "'가구 단독매입','커리어 행사','가 방','패션 단독매입','시티 단독매입','신생아완구 단독매입','MP3 外','인텔리젼스 행사',\n",
    "'구두 단독매입','시네마 매점','시티웨어 행사','장신 단독매입','NB 남화','남성시티 직매입','영플라자 (올리브핫스텁)',\n",
    "'색조 화장품','NB 여화','직수입 골프의류','트래디 행사','IT 게임기,S/W','TAKE OUT','주방 단독매입','셔츠 직매입(PB)', '남성정장 직매입',\n",
    "# 점\n",
    "'N.B정장', '즉석.스넥(매장)','N.B', 'L.B',\n",
    "# 콤마\n",
    "'기타(가발,포장,담배,끽연구,사진,수선)', '원목,학생,철재',  \n",
    "# 세로바\n",
    "'N/B골프의류', 'L/C 아동복', 'L/C골프의류', '4대B/D', '우/양산(특정)', 'N/B아동복', 'L/C정장', '국산A/V','수입A/V','영플라자(진/유니)',\n",
    "'IT게임기,S/W', '피아노/악기',\n",
    "] \n",
    "\n",
    "after = ['기초화장품', '니트', '캐릭터여화', '캐릭터남화', '수저', '4대B/D', 'N/B아동복', '수입아동복', '스낵형델리','수입청소기외','영캐릭터','영트렌디','식기단독매입','커튼','생활잡화단독매입','가구단독매입',\n",
    "'커리어행사','가방','패션단독매입','시티단독매입','신생아완구단독매입','MP3外','인텔리젼스행사','구두단독매입','시네마매점','시티웨어행사','장신단독매입','NB남화','남성시티직매입','영플라자(올리브핫스텁)',\n",
    "'색조화장품','NB여화','직수입골프의류','트래디행사','IT게임기,S/W','TAKEOUT','주방단독매입','셔츠직매입(PB)', '남성정장직매입',\n",
    "'NB정장', '즉석스넥(매장)','NB', 'LB', '기타(가발포장담배끽연구사진수선)', '원목학생철재',\n",
    "'NB골프의류', 'LC아동복', 'LC골프의류', '4대BD', '우양산(특정)', 'NB아동복', 'LC정장', '국산AV','수입AV','영플라자(진유니)',\n",
    "'IT게임기SW', '피아노악기',]\n",
    "\n",
    "\n",
    "\n",
    "df_train.gds_grp_nm = df_train.gds_grp_nm.replace(before, after)\n",
    "df_test.gds_grp_nm = df_test.gds_grp_nm.replace(before, after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_nm = 340 # 중분류 nunique = 330\n",
    "maxlen_nm = 450 # oversample n=3 일때 np.mean이 397이어서\n",
    "emb_dim_nm = 256 # 임베딩 벡터의 출력 차원. 결과로서 나오는 임베딩 벡터의 크기\n",
    "\n",
    "# Converts a \"gds_grp_nm\" to a sequence of indexes in a fixed-size hashing space\n",
    "X_train_nm = df_train.groupby('cust_id')['gds_grp_nm'].apply(lambda x: [one_hot(products, max_features_nm)[0] for products in x]).values\n",
    "X_test_nm = df_test.groupby('cust_id')['gds_grp_nm'].apply(lambda x: [one_hot(products, max_features_nm)[0] for products in x]).values\n",
    "\n",
    "\n",
    "# oversample2: \n",
    "    # 1. unique X(get all products with duplication)\n",
    "    # 2. replace=True(Restoration extraction)\n",
    "    # 3. more buy, more oversample\n",
    "    #       -> if you want to oversample in same ratio, just adjust n=1 to oversample=1, and erase 'for oversample in range(n)'\n",
    "\n",
    "def oversample2(data, n=1, seed=seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    customerProducts = []\n",
    "    \n",
    "    for products in data:\n",
    "        \n",
    "        for oversample in range(n):\n",
    "            \n",
    "            products = list(np.append(products, np.random.choice(products, len(products) * oversample, replace=True)))\n",
    "            \n",
    "        customerProducts.append(products)\n",
    "        \n",
    "    return customerProducts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nm = oversample2(X_train_nm, 3)\n",
    "X_test_nm = oversample2(X_test_nm, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 450), (2482, 450))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 차이를 한번 봐야함: pad_sequences에서 truncating 하는 것이 좋은지, 안하는 것이 성능이 더 좋은지\n",
    "# 1. 일단 적은 양으로 가보기 위해 truncating 실시.\n",
    "# 2. \n",
    "\n",
    "X_train_nm = pad_sequences(X_train_nm, maxlen=maxlen_nm, padding='post', truncating='post', value=0)\n",
    "X_test_nm = pad_sequences(X_test_nm, maxlen=maxlen_nm, padding='post', truncating='post', value=0)\n",
    "\n",
    "X_train_nm.shape, X_test_nm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model & its Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_f = Input(shape=(maxlen_nm, ), dtype='int32', name='forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'forward_2:0' shape=(None, 450) dtype=int32>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Embedding(max_features_nm, emb_dim_nm)(in_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Conv1D(32, 3, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Conv1D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Input(shape=(maxlen_nm, ), dtypes='int32', name='gds_grp_nm'),\n",
    "        x = layers.\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(250, 250, 3)),\n",
    "        layers.Conv2D(32, 5, strides=2, activation=\"relu\"),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sequential model with 3 layers\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(2, activation=\"relu\", name=\"layer1\"),\n",
    "        layers.Dense(3, activation=\"relu\", name=\"layer2\"),\n",
    "        layers.Dense(4, name=\"layer3\"),\n",
    "    ]\n",
    ")\n",
    "# Call model on a test input\n",
    "x = tf.ones((3, 3))\n",
    "y = model(x)\n",
    "\n",
    "\n",
    "is equivalent to this function:\n",
    "\n",
    "# Create 3 layers\n",
    "layer1 = layers.Dense(2, activation=\"relu\", name=\"layer1\")\n",
    "layer2 = layers.Dense(3, activation=\"relu\", name=\"layer2\")\n",
    "layer3 = layers.Dense(4, name=\"layer3\")\n",
    "\n",
    "# Call layers on a test input\n",
    "x = tf.ones((3, 3))\n",
    "y = layer3(layer2(layer1(x)))\n",
    "\n",
    "\n",
    "You can also create a Sequential model incrementally via the add() method:\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(2, activation=\"relu\"))\n",
    "model.add(layers.Dense(3, activation=\"relu\"))\n",
    "model.add(layers.Dense(4))\n",
    "\n",
    "\n",
    "Also note that the Sequential constructor accepts a name argument, just like any layer or model in Keras. This is useful to annotate TensorBoard graphs with semantically meaningful names.\n",
    "\n",
    "model = keras.Sequential(name=\"my_sequential\")\n",
    "model.add(layers.Dense(2, activation=\"relu\", name=\"layer1\"))\n",
    "model.add(layers.Dense(3, activation=\"relu\", name=\"layer2\"))\n",
    "model.add(layers.Dense(4, name=\"layer3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature extraction with a Sequential model\n",
    "Once a Sequential model has been built, it behaves like a Functional API model. This means that every layer has an input and output attribute. These attributes can be used to do neat things, like quickly creating a model that extracts the outputs of all intermediate layers in a Sequential model:\n",
    "\n",
    "initial_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(250, 250, 3)),\n",
    "        layers.Conv2D(32, 5, strides=2, activation=\"relu\"),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "    ]\n",
    ")\n",
    "feature_extractor = keras.Model(\n",
    "    inputs=initial_model.inputs,\n",
    "    outputs=[layer.output for layer in initial_model.layers],\n",
    ")\n",
    "\n",
    "# Call feature extractor on test input.\n",
    "x = tf.ones((1, 250, 250, 3))\n",
    "features = feature_extractor(x)\n",
    "Here's a similar example that only extract features from one layer:\n",
    "\n",
    "initial_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(250, 250, 3)),\n",
    "        layers.Conv2D(32, 5, strides=2, activation=\"relu\"),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\", name=\"my_intermediate_layer\"),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "    ]\n",
    ")\n",
    "feature_extractor = keras.Model(\n",
    "    inputs=initial_model.inputs,\n",
    "    outputs=initial_model.get_layer(name=\"my_intermediate_layer\").output,\n",
    ")\n",
    "# Call feature extractor on test input.\n",
    "x = tf.ones((1, 250, 250, 3))\n",
    "features = feature_extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data reading\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "x_train, y_train = mnist.train.images, mnist.train.labels\n",
    "x_test, y_test = mnist.test.images, mnist.test.labels\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import metrics\n",
    "\n",
    "## sequential model \n",
    "seq_model = Sequential([\n",
    "    Dense(512, input_shape=(784,), activation='relu'), \n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "print(\"#### Sequential Model\")\n",
    "seq_model.summary()\n",
    "seq_model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "train_history = seq_model.fit(x_train, y_train, epochs=5, batch_size=500, verbose=2)\n",
    "train_history = train_history.history # epoch마다 변화한 loss, metric\n",
    "\n",
    "loss_and_metric = seq_model.evaluate(x_train, y_train, batch_size=128, verbose=0)\n",
    "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
    "loss_and_metric = seq_model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test, loss and metric: {}\".format(loss_and_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
