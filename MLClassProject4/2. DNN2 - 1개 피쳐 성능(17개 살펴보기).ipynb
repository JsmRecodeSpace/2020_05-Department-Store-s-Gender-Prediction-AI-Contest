{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Data Wrangling\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from vecstack import stacking\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "\n",
    "# Keras\n",
    "import tensorflow as tf\n",
    "# Tensorflow warning off\n",
    "if tf.__version__[0] < '2':\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import * #Input, Dense\n",
    "from keras.models import * #Model\n",
    "from keras.optimizers import *\n",
    "from keras.initializers import *\n",
    "from keras.regularizers import *\n",
    "from keras.constraints import *\n",
    "from keras.utils.np_utils import *\n",
    "from keras.utils.vis_utils import * #model_to_dot\n",
    "from keras.preprocessing.image import *\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import *\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras import Input\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.constraints import max_norm\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모형 학습 시 RMSE를 계산하는 함수\n",
    "def rmse(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# 예측값을 저장할 폴더 생성\n",
    "folder = 'Ensemble'\n",
    "if not os.path.isdir(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Ensemble Models \n",
    "# 생성된 다수의 DNN 모형을 power mean하여 앙상블\n",
    "\n",
    "# nf = 0\n",
    "# for f in os.listdir(folder):\n",
    "#     ext = os.path.splitext(f)[-1]\n",
    "#     if ext == '.csv': \n",
    "#         s = pd.read_csv(folder+\"/\"+f)\n",
    "#     else: \n",
    "#         continue\n",
    "#     if len(s.columns) !=2:\n",
    "#         continue\n",
    "#     if nf == 0: \n",
    "#         slist = s\n",
    "#     else: \n",
    "#         slist = pd.merge(slist, s, on=\"item_id\")\n",
    "#     nf += 1\n",
    "\n",
    "# p = 4.5 # 이 값에 따라 성능이 달라짐 (p=1: 산술평균, p>1: 멱평균)    \n",
    "# if nf >= 2:\n",
    "#     pred = 0\n",
    "#     for j in range(nf): pred = pred + slist.iloc[:,j+1]**p \n",
    "#     pred = pred / nf    \n",
    "#     pred = pred**(1/p)\n",
    "\n",
    "#     submission = pd.DataFrame({'item_id': slist.item_id, 'item_cnt_month': pred})\n",
    "#     t = pd.Timestamp.now()\n",
    "#     fname = f\"p{p}mean_dnn_submission_{t.month:02}{t.day:02}_{t.hour:02}{t.minute:02}.csv\"\n",
    "#     submission.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 중분류 구매건수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.625380</td>\n",
       "      <td>0.034396</td>\n",
       "      <td>-0.672246</td>\n",
       "      <td>0.441779</td>\n",
       "      <td>0.159294</td>\n",
       "      <td>-0.598718</td>\n",
       "      <td>0.325886</td>\n",
       "      <td>-0.936302</td>\n",
       "      <td>0.305429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143142</td>\n",
       "      <td>0.118564</td>\n",
       "      <td>-0.002334</td>\n",
       "      <td>0.089901</td>\n",
       "      <td>-0.111054</td>\n",
       "      <td>-0.021497</td>\n",
       "      <td>0.140365</td>\n",
       "      <td>0.117912</td>\n",
       "      <td>-0.126485</td>\n",
       "      <td>0.046712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.107635</td>\n",
       "      <td>-0.008147</td>\n",
       "      <td>-0.036582</td>\n",
       "      <td>-0.227227</td>\n",
       "      <td>-0.173067</td>\n",
       "      <td>0.257546</td>\n",
       "      <td>0.036107</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>-0.073846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030561</td>\n",
       "      <td>-0.018014</td>\n",
       "      <td>0.032755</td>\n",
       "      <td>0.015114</td>\n",
       "      <td>0.011464</td>\n",
       "      <td>-0.002095</td>\n",
       "      <td>-0.073120</td>\n",
       "      <td>-0.051948</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>-0.007939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.099033</td>\n",
       "      <td>-0.109059</td>\n",
       "      <td>-0.020829</td>\n",
       "      <td>0.007957</td>\n",
       "      <td>-0.059710</td>\n",
       "      <td>0.090729</td>\n",
       "      <td>0.179059</td>\n",
       "      <td>-0.068382</td>\n",
       "      <td>0.062943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.035945</td>\n",
       "      <td>-0.017813</td>\n",
       "      <td>-0.008323</td>\n",
       "      <td>-0.036238</td>\n",
       "      <td>0.021623</td>\n",
       "      <td>0.038809</td>\n",
       "      <td>0.046284</td>\n",
       "      <td>0.007039</td>\n",
       "      <td>0.014445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.265591</td>\n",
       "      <td>-0.619277</td>\n",
       "      <td>-0.279149</td>\n",
       "      <td>-0.187955</td>\n",
       "      <td>0.023135</td>\n",
       "      <td>-0.135279</td>\n",
       "      <td>-0.067050</td>\n",
       "      <td>0.675716</td>\n",
       "      <td>-0.159730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131376</td>\n",
       "      <td>0.064030</td>\n",
       "      <td>-0.118500</td>\n",
       "      <td>0.066814</td>\n",
       "      <td>-0.149491</td>\n",
       "      <td>-0.003301</td>\n",
       "      <td>0.486243</td>\n",
       "      <td>-0.482798</td>\n",
       "      <td>-0.028555</td>\n",
       "      <td>-0.156546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.141518</td>\n",
       "      <td>-0.191579</td>\n",
       "      <td>0.005533</td>\n",
       "      <td>-0.059629</td>\n",
       "      <td>0.026487</td>\n",
       "      <td>0.013643</td>\n",
       "      <td>-0.095080</td>\n",
       "      <td>0.043151</td>\n",
       "      <td>-0.057431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008822</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>-0.005974</td>\n",
       "      <td>-0.006765</td>\n",
       "      <td>0.007707</td>\n",
       "      <td>0.013415</td>\n",
       "      <td>-0.002152</td>\n",
       "      <td>0.008844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-0.877498</td>\n",
       "      <td>0.236507</td>\n",
       "      <td>-0.127787</td>\n",
       "      <td>-0.066931</td>\n",
       "      <td>-0.135133</td>\n",
       "      <td>0.047662</td>\n",
       "      <td>0.136196</td>\n",
       "      <td>-0.051518</td>\n",
       "      <td>-0.006189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100014</td>\n",
       "      <td>-0.477439</td>\n",
       "      <td>-0.028659</td>\n",
       "      <td>0.204758</td>\n",
       "      <td>-0.015639</td>\n",
       "      <td>0.083038</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>-0.005576</td>\n",
       "      <td>-0.006840</td>\n",
       "      <td>0.081172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.162111</td>\n",
       "      <td>-0.159106</td>\n",
       "      <td>0.019576</td>\n",
       "      <td>0.078777</td>\n",
       "      <td>0.026611</td>\n",
       "      <td>-0.007421</td>\n",
       "      <td>-0.020261</td>\n",
       "      <td>0.025530</td>\n",
       "      <td>-0.021468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005455</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>0.011396</td>\n",
       "      <td>0.006313</td>\n",
       "      <td>-0.050464</td>\n",
       "      <td>-0.032066</td>\n",
       "      <td>-0.036101</td>\n",
       "      <td>-0.007855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>0.199450</td>\n",
       "      <td>1.781264</td>\n",
       "      <td>-0.540263</td>\n",
       "      <td>0.071173</td>\n",
       "      <td>0.331206</td>\n",
       "      <td>1.250888</td>\n",
       "      <td>0.351207</td>\n",
       "      <td>-0.141133</td>\n",
       "      <td>-0.117322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100453</td>\n",
       "      <td>0.028486</td>\n",
       "      <td>-0.128008</td>\n",
       "      <td>0.088439</td>\n",
       "      <td>-0.112445</td>\n",
       "      <td>-0.079405</td>\n",
       "      <td>0.109667</td>\n",
       "      <td>0.038992</td>\n",
       "      <td>-0.088774</td>\n",
       "      <td>0.054421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.635700</td>\n",
       "      <td>-0.227858</td>\n",
       "      <td>-0.237216</td>\n",
       "      <td>0.009492</td>\n",
       "      <td>-0.166560</td>\n",
       "      <td>-0.074526</td>\n",
       "      <td>0.129139</td>\n",
       "      <td>-0.274902</td>\n",
       "      <td>-0.380003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.285796</td>\n",
       "      <td>0.651523</td>\n",
       "      <td>0.113559</td>\n",
       "      <td>-0.241631</td>\n",
       "      <td>0.065565</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>-0.040256</td>\n",
       "      <td>-0.085648</td>\n",
       "      <td>-0.044685</td>\n",
       "      <td>-0.053650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.674396</td>\n",
       "      <td>-0.398007</td>\n",
       "      <td>0.607995</td>\n",
       "      <td>-0.003883</td>\n",
       "      <td>-0.024185</td>\n",
       "      <td>0.051853</td>\n",
       "      <td>0.060919</td>\n",
       "      <td>0.352332</td>\n",
       "      <td>0.488067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256775</td>\n",
       "      <td>0.148656</td>\n",
       "      <td>-0.043532</td>\n",
       "      <td>-0.055591</td>\n",
       "      <td>-0.132109</td>\n",
       "      <td>-0.018838</td>\n",
       "      <td>0.036651</td>\n",
       "      <td>0.041369</td>\n",
       "      <td>0.035976</td>\n",
       "      <td>-0.103166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  0.625380  0.034396 -0.672246  0.441779  0.159294 -0.598718   \n",
       "1           1 -1.107635 -0.008147 -0.036582 -0.227227 -0.173067  0.257546   \n",
       "2           2 -1.099033 -0.109059 -0.020829  0.007957 -0.059710  0.090729   \n",
       "3           3  0.265591 -0.619277 -0.279149 -0.187955  0.023135 -0.135279   \n",
       "4           4 -1.141518 -0.191579  0.005533 -0.059629  0.026487  0.013643   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -0.877498  0.236507 -0.127787 -0.066931 -0.135133  0.047662   \n",
       "5978     5978 -1.162111 -0.159106  0.019576  0.078777  0.026611 -0.007421   \n",
       "5979     5979  0.199450  1.781264 -0.540263  0.071173  0.331206  1.250888   \n",
       "5980     5980 -0.635700 -0.227858 -0.237216  0.009492 -0.166560 -0.074526   \n",
       "5981     5981 -0.674396 -0.398007  0.607995 -0.003883 -0.024185  0.051853   \n",
       "\n",
       "             6         7         8  ...       100       101       102  \\\n",
       "0     0.325886 -0.936302  0.305429  ... -0.143142  0.118564 -0.002334   \n",
       "1     0.036107 -0.059078 -0.073846  ...  0.030561 -0.018014  0.032755   \n",
       "2     0.179059 -0.068382  0.062943  ...  0.002262  0.035945 -0.017813   \n",
       "3    -0.067050  0.675716 -0.159730  ...  0.131376  0.064030 -0.118500   \n",
       "4    -0.095080  0.043151 -0.057431  ...  0.008822  0.004029  0.004533   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977  0.136196 -0.051518 -0.006189  ...  0.100014 -0.477439 -0.028659   \n",
       "5978 -0.020261  0.025530 -0.021468  ...  0.005455  0.005500  0.013200   \n",
       "5979  0.351207 -0.141133 -0.117322  ...  0.100453  0.028486 -0.128008   \n",
       "5980  0.129139 -0.274902 -0.380003  ... -0.285796  0.651523  0.113559   \n",
       "5981  0.060919  0.352332  0.488067  ...  0.256775  0.148656 -0.043532   \n",
       "\n",
       "           103       104       105       106       107       108       109  \n",
       "0     0.089901 -0.111054 -0.021497  0.140365  0.117912 -0.126485  0.046712  \n",
       "1     0.015114  0.011464 -0.002095 -0.073120 -0.051948  0.005610 -0.007939  \n",
       "2    -0.008323 -0.036238  0.021623  0.038809  0.046284  0.007039  0.014445  \n",
       "3     0.066814 -0.149491 -0.003301  0.486243 -0.482798 -0.028555 -0.156546  \n",
       "4    -0.000862 -0.005974 -0.006765  0.007707  0.013415 -0.002152  0.008844  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977  0.204758 -0.015639  0.083038  0.010453 -0.005576 -0.006840  0.081172  \n",
       "5978  0.006884  0.011396  0.006313 -0.050464 -0.032066 -0.036101 -0.007855  \n",
       "5979  0.088439 -0.112445 -0.079405  0.109667  0.038992 -0.088774  0.054421  \n",
       "5980 -0.241631  0.065565  0.000818 -0.040256 -0.085648 -0.044685 -0.053650  \n",
       "5981 -0.055591 -0.132109 -0.018838  0.036651  0.041369  0.035976 -0.103166  \n",
       "\n",
       "[5982 rows x 111 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                            aggfunc=lambda x: len(x), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "\n",
    "seed = 2020\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:03<00:12,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7463076869613567 SEED: 1782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:06<00:09,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7486012649430082 SEED: 9197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:09<00:06,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7524716777870448 SEED: 1374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:12<00:03,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7474284125660273 SEED: 9647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7472546566583265 SEED: 8924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 대분류 구매건수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.808812</td>\n",
       "      <td>0.047621</td>\n",
       "      <td>-0.382530</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>0.068270</td>\n",
       "      <td>-1.067826</td>\n",
       "      <td>0.097073</td>\n",
       "      <td>0.064659</td>\n",
       "      <td>0.123847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278258</td>\n",
       "      <td>-0.198159</td>\n",
       "      <td>-0.226099</td>\n",
       "      <td>-0.038080</td>\n",
       "      <td>0.324296</td>\n",
       "      <td>-0.434571</td>\n",
       "      <td>-0.308631</td>\n",
       "      <td>-0.016363</td>\n",
       "      <td>0.194200</td>\n",
       "      <td>-0.018975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.942326</td>\n",
       "      <td>0.181016</td>\n",
       "      <td>0.039485</td>\n",
       "      <td>0.224548</td>\n",
       "      <td>0.165447</td>\n",
       "      <td>0.185474</td>\n",
       "      <td>0.260437</td>\n",
       "      <td>0.067577</td>\n",
       "      <td>-0.233817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035076</td>\n",
       "      <td>-0.147834</td>\n",
       "      <td>0.062239</td>\n",
       "      <td>0.060819</td>\n",
       "      <td>-0.054463</td>\n",
       "      <td>-0.387346</td>\n",
       "      <td>0.112056</td>\n",
       "      <td>-0.059185</td>\n",
       "      <td>0.006323</td>\n",
       "      <td>-0.030976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.939487</td>\n",
       "      <td>0.072728</td>\n",
       "      <td>-0.171136</td>\n",
       "      <td>0.257513</td>\n",
       "      <td>-0.163384</td>\n",
       "      <td>-0.015842</td>\n",
       "      <td>-0.109646</td>\n",
       "      <td>0.098185</td>\n",
       "      <td>-0.189282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057341</td>\n",
       "      <td>0.047966</td>\n",
       "      <td>-0.038936</td>\n",
       "      <td>-0.112071</td>\n",
       "      <td>-0.006490</td>\n",
       "      <td>0.062245</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>0.044459</td>\n",
       "      <td>0.039179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.176414</td>\n",
       "      <td>-0.540114</td>\n",
       "      <td>-0.095850</td>\n",
       "      <td>0.039596</td>\n",
       "      <td>0.054163</td>\n",
       "      <td>0.377580</td>\n",
       "      <td>0.169790</td>\n",
       "      <td>-0.019942</td>\n",
       "      <td>0.183439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>-0.014416</td>\n",
       "      <td>-0.061704</td>\n",
       "      <td>0.120444</td>\n",
       "      <td>-0.374014</td>\n",
       "      <td>0.177117</td>\n",
       "      <td>-0.243579</td>\n",
       "      <td>0.075744</td>\n",
       "      <td>-0.142311</td>\n",
       "      <td>-0.229687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.006475</td>\n",
       "      <td>-0.015659</td>\n",
       "      <td>0.083409</td>\n",
       "      <td>-0.036429</td>\n",
       "      <td>0.082568</td>\n",
       "      <td>-0.020770</td>\n",
       "      <td>0.012661</td>\n",
       "      <td>-0.029358</td>\n",
       "      <td>0.030033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008659</td>\n",
       "      <td>0.040425</td>\n",
       "      <td>-0.007492</td>\n",
       "      <td>0.042274</td>\n",
       "      <td>0.049531</td>\n",
       "      <td>-0.034650</td>\n",
       "      <td>-0.017044</td>\n",
       "      <td>-0.018064</td>\n",
       "      <td>0.021210</td>\n",
       "      <td>-0.000362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-0.527805</td>\n",
       "      <td>0.460797</td>\n",
       "      <td>-0.064967</td>\n",
       "      <td>-0.029257</td>\n",
       "      <td>0.232430</td>\n",
       "      <td>-0.141500</td>\n",
       "      <td>0.104384</td>\n",
       "      <td>0.065651</td>\n",
       "      <td>-0.706434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.477599</td>\n",
       "      <td>0.215720</td>\n",
       "      <td>0.023805</td>\n",
       "      <td>0.367133</td>\n",
       "      <td>0.026163</td>\n",
       "      <td>0.424302</td>\n",
       "      <td>-0.089105</td>\n",
       "      <td>-0.048746</td>\n",
       "      <td>0.062714</td>\n",
       "      <td>-0.055811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.038196</td>\n",
       "      <td>0.022598</td>\n",
       "      <td>0.023371</td>\n",
       "      <td>-0.003908</td>\n",
       "      <td>0.008111</td>\n",
       "      <td>0.030509</td>\n",
       "      <td>-0.068062</td>\n",
       "      <td>-0.030968</td>\n",
       "      <td>0.011029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106910</td>\n",
       "      <td>0.017061</td>\n",
       "      <td>0.158339</td>\n",
       "      <td>-0.014808</td>\n",
       "      <td>-0.045128</td>\n",
       "      <td>0.014905</td>\n",
       "      <td>-0.012643</td>\n",
       "      <td>-0.058065</td>\n",
       "      <td>0.033216</td>\n",
       "      <td>-0.046763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>0.672151</td>\n",
       "      <td>1.288926</td>\n",
       "      <td>0.047105</td>\n",
       "      <td>0.442555</td>\n",
       "      <td>-0.225962</td>\n",
       "      <td>0.620944</td>\n",
       "      <td>-0.542938</td>\n",
       "      <td>-0.306575</td>\n",
       "      <td>-0.724191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267473</td>\n",
       "      <td>0.172990</td>\n",
       "      <td>0.246498</td>\n",
       "      <td>-0.123809</td>\n",
       "      <td>-0.203993</td>\n",
       "      <td>-0.005225</td>\n",
       "      <td>-0.029028</td>\n",
       "      <td>0.124641</td>\n",
       "      <td>0.155520</td>\n",
       "      <td>0.100988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.561052</td>\n",
       "      <td>-0.015907</td>\n",
       "      <td>0.100263</td>\n",
       "      <td>0.082922</td>\n",
       "      <td>-0.023670</td>\n",
       "      <td>0.353346</td>\n",
       "      <td>0.424770</td>\n",
       "      <td>-0.039177</td>\n",
       "      <td>0.215357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149904</td>\n",
       "      <td>-0.020741</td>\n",
       "      <td>-0.158265</td>\n",
       "      <td>0.161862</td>\n",
       "      <td>-0.201403</td>\n",
       "      <td>-0.103223</td>\n",
       "      <td>-0.567873</td>\n",
       "      <td>-0.410687</td>\n",
       "      <td>-0.080075</td>\n",
       "      <td>0.002968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.804268</td>\n",
       "      <td>-0.300905</td>\n",
       "      <td>-0.010156</td>\n",
       "      <td>0.041940</td>\n",
       "      <td>0.061396</td>\n",
       "      <td>0.123361</td>\n",
       "      <td>-0.009046</td>\n",
       "      <td>-0.017363</td>\n",
       "      <td>-0.044154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064577</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>-0.091283</td>\n",
       "      <td>0.040444</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>-0.027214</td>\n",
       "      <td>-0.019029</td>\n",
       "      <td>0.156810</td>\n",
       "      <td>0.123323</td>\n",
       "      <td>0.190463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  0.808812  0.047621 -0.382530  0.136842  0.068270 -1.067826   \n",
       "1           1 -0.942326  0.181016  0.039485  0.224548  0.165447  0.185474   \n",
       "2           2 -0.939487  0.072728 -0.171136  0.257513 -0.163384 -0.015842   \n",
       "3           3  0.176414 -0.540114 -0.095850  0.039596  0.054163  0.377580   \n",
       "4           4 -1.006475 -0.015659  0.083409 -0.036429  0.082568 -0.020770   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -0.527805  0.460797 -0.064967 -0.029257  0.232430 -0.141500   \n",
       "5978     5978 -1.038196  0.022598  0.023371 -0.003908  0.008111  0.030509   \n",
       "5979     5979  0.672151  1.288926  0.047105  0.442555 -0.225962  0.620944   \n",
       "5980     5980 -0.561052 -0.015907  0.100263  0.082922 -0.023670  0.353346   \n",
       "5981     5981 -0.804268 -0.300905 -0.010156  0.041940  0.061396  0.123361   \n",
       "\n",
       "             6         7         8  ...        25        26        27  \\\n",
       "0     0.097073  0.064659  0.123847  ...  0.278258 -0.198159 -0.226099   \n",
       "1     0.260437  0.067577 -0.233817  ... -0.035076 -0.147834  0.062239   \n",
       "2    -0.109646  0.098185 -0.189282  ...  0.057341  0.047966 -0.038936   \n",
       "3     0.169790 -0.019942  0.183439  ...  0.156863 -0.014416 -0.061704   \n",
       "4     0.012661 -0.029358  0.030033  ...  0.008659  0.040425 -0.007492   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977  0.104384  0.065651 -0.706434  ... -0.477599  0.215720  0.023805   \n",
       "5978 -0.068062 -0.030968  0.011029  ...  0.106910  0.017061  0.158339   \n",
       "5979 -0.542938 -0.306575 -0.724191  ... -0.267473  0.172990  0.246498   \n",
       "5980  0.424770 -0.039177  0.215357  ...  0.149904 -0.020741 -0.158265   \n",
       "5981 -0.009046 -0.017363 -0.044154  ... -0.064577  0.030567 -0.091283   \n",
       "\n",
       "            28        29        30        31        32        33        34  \n",
       "0    -0.038080  0.324296 -0.434571 -0.308631 -0.016363  0.194200 -0.018975  \n",
       "1     0.060819 -0.054463 -0.387346  0.112056 -0.059185  0.006323 -0.030976  \n",
       "2    -0.112071 -0.006490  0.062245 -0.004990  0.035366  0.044459  0.039179  \n",
       "3     0.120444 -0.374014  0.177117 -0.243579  0.075744 -0.142311 -0.229687  \n",
       "4     0.042274  0.049531 -0.034650 -0.017044 -0.018064  0.021210 -0.000362  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977  0.367133  0.026163  0.424302 -0.089105 -0.048746  0.062714 -0.055811  \n",
       "5978 -0.014808 -0.045128  0.014905 -0.012643 -0.058065  0.033216 -0.046763  \n",
       "5979 -0.123809 -0.203993 -0.005225 -0.029028  0.124641  0.155520  0.100988  \n",
       "5980  0.161862 -0.201403 -0.103223 -0.567873 -0.410687 -0.080075  0.002968  \n",
       "5981  0.040444  0.000616 -0.027214 -0.019029  0.156810  0.123323  0.190463  \n",
       "\n",
       "[5982 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_mclas_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                            aggfunc=lambda x: len(x), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "\n",
    "seed = 2020\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:03<00:14,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7616112385321101 SEED: 9057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:07<00:11,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7567113219349457 SEED: 8840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:11<00:07,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7600995621351126 SEED: 2103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:14<00:03,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7553734014456491 SEED: 3043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:17<00:00,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7567981998887963 SEED: 5221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 중분류 구매여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.031835</td>\n",
       "      <td>0.212301</td>\n",
       "      <td>-0.922307</td>\n",
       "      <td>-0.869803</td>\n",
       "      <td>-0.305008</td>\n",
       "      <td>0.092058</td>\n",
       "      <td>0.831769</td>\n",
       "      <td>-0.575745</td>\n",
       "      <td>-0.703853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102421</td>\n",
       "      <td>-0.318619</td>\n",
       "      <td>-0.375416</td>\n",
       "      <td>-0.050509</td>\n",
       "      <td>0.185363</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>-0.122237</td>\n",
       "      <td>-0.086713</td>\n",
       "      <td>-0.053711</td>\n",
       "      <td>-0.077012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.545397</td>\n",
       "      <td>-0.120407</td>\n",
       "      <td>0.028675</td>\n",
       "      <td>0.202452</td>\n",
       "      <td>0.264835</td>\n",
       "      <td>0.240070</td>\n",
       "      <td>0.129344</td>\n",
       "      <td>0.058812</td>\n",
       "      <td>0.209729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028266</td>\n",
       "      <td>0.074902</td>\n",
       "      <td>0.029886</td>\n",
       "      <td>0.116798</td>\n",
       "      <td>-0.031030</td>\n",
       "      <td>-0.049614</td>\n",
       "      <td>0.024517</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.006372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.415123</td>\n",
       "      <td>-0.065534</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>-0.016653</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0.400511</td>\n",
       "      <td>-0.053878</td>\n",
       "      <td>-0.125965</td>\n",
       "      <td>-0.270389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>-0.011082</td>\n",
       "      <td>-0.020539</td>\n",
       "      <td>-0.010853</td>\n",
       "      <td>-0.012236</td>\n",
       "      <td>-0.003550</td>\n",
       "      <td>-0.000766</td>\n",
       "      <td>0.007166</td>\n",
       "      <td>0.015099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.865498</td>\n",
       "      <td>-1.222110</td>\n",
       "      <td>-0.553205</td>\n",
       "      <td>0.022980</td>\n",
       "      <td>0.309707</td>\n",
       "      <td>-0.115284</td>\n",
       "      <td>-0.423851</td>\n",
       "      <td>0.164560</td>\n",
       "      <td>0.401825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122099</td>\n",
       "      <td>0.050478</td>\n",
       "      <td>0.119724</td>\n",
       "      <td>-0.141897</td>\n",
       "      <td>-0.014390</td>\n",
       "      <td>0.095265</td>\n",
       "      <td>-0.023132</td>\n",
       "      <td>0.142257</td>\n",
       "      <td>0.325156</td>\n",
       "      <td>0.171970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.341069</td>\n",
       "      <td>-0.078229</td>\n",
       "      <td>-0.045112</td>\n",
       "      <td>0.219325</td>\n",
       "      <td>-0.030194</td>\n",
       "      <td>-0.500378</td>\n",
       "      <td>0.017921</td>\n",
       "      <td>0.145940</td>\n",
       "      <td>-0.016312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012135</td>\n",
       "      <td>0.031032</td>\n",
       "      <td>-0.017324</td>\n",
       "      <td>-0.006011</td>\n",
       "      <td>0.007325</td>\n",
       "      <td>-0.018732</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>-0.010431</td>\n",
       "      <td>-0.004584</td>\n",
       "      <td>-0.006360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-1.161452</td>\n",
       "      <td>0.308622</td>\n",
       "      <td>-0.139144</td>\n",
       "      <td>-0.117040</td>\n",
       "      <td>0.072620</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>-0.058142</td>\n",
       "      <td>-0.208458</td>\n",
       "      <td>-0.120671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004387</td>\n",
       "      <td>-0.135064</td>\n",
       "      <td>0.031170</td>\n",
       "      <td>-0.002395</td>\n",
       "      <td>-0.009385</td>\n",
       "      <td>0.033274</td>\n",
       "      <td>0.079296</td>\n",
       "      <td>0.230744</td>\n",
       "      <td>-0.209229</td>\n",
       "      <td>-0.247969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.547982</td>\n",
       "      <td>-0.166368</td>\n",
       "      <td>0.062790</td>\n",
       "      <td>-0.058637</td>\n",
       "      <td>-0.039730</td>\n",
       "      <td>0.145217</td>\n",
       "      <td>0.182716</td>\n",
       "      <td>0.213168</td>\n",
       "      <td>0.241474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017176</td>\n",
       "      <td>-0.011808</td>\n",
       "      <td>-0.000613</td>\n",
       "      <td>-0.003491</td>\n",
       "      <td>-0.025970</td>\n",
       "      <td>-0.001241</td>\n",
       "      <td>0.008654</td>\n",
       "      <td>-0.008849</td>\n",
       "      <td>-0.015761</td>\n",
       "      <td>-0.011166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>0.204833</td>\n",
       "      <td>1.663809</td>\n",
       "      <td>-0.728419</td>\n",
       "      <td>0.605944</td>\n",
       "      <td>0.811258</td>\n",
       "      <td>1.091609</td>\n",
       "      <td>-0.392112</td>\n",
       "      <td>1.131950</td>\n",
       "      <td>-0.592724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523407</td>\n",
       "      <td>0.207549</td>\n",
       "      <td>-0.355611</td>\n",
       "      <td>-0.084467</td>\n",
       "      <td>0.018351</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.144681</td>\n",
       "      <td>0.012927</td>\n",
       "      <td>0.108045</td>\n",
       "      <td>-0.044070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.925491</td>\n",
       "      <td>-0.298534</td>\n",
       "      <td>-0.188338</td>\n",
       "      <td>-0.024190</td>\n",
       "      <td>0.231828</td>\n",
       "      <td>0.287504</td>\n",
       "      <td>0.755301</td>\n",
       "      <td>-0.127350</td>\n",
       "      <td>0.329944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028420</td>\n",
       "      <td>-0.098941</td>\n",
       "      <td>-0.130566</td>\n",
       "      <td>0.075247</td>\n",
       "      <td>-0.050180</td>\n",
       "      <td>0.088172</td>\n",
       "      <td>-0.168739</td>\n",
       "      <td>-0.393781</td>\n",
       "      <td>0.228118</td>\n",
       "      <td>0.392946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.512350</td>\n",
       "      <td>-0.620099</td>\n",
       "      <td>0.938312</td>\n",
       "      <td>-0.100132</td>\n",
       "      <td>-0.191828</td>\n",
       "      <td>0.116272</td>\n",
       "      <td>-0.882415</td>\n",
       "      <td>-0.106105</td>\n",
       "      <td>-0.051885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065957</td>\n",
       "      <td>-0.151601</td>\n",
       "      <td>0.072650</td>\n",
       "      <td>-0.214053</td>\n",
       "      <td>-0.182360</td>\n",
       "      <td>0.173993</td>\n",
       "      <td>0.166463</td>\n",
       "      <td>0.261078</td>\n",
       "      <td>0.370338</td>\n",
       "      <td>0.349272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  1.031835  0.212301 -0.922307 -0.869803 -0.305008  0.092058   \n",
       "1           1 -1.545397 -0.120407  0.028675  0.202452  0.264835  0.240070   \n",
       "2           2 -1.415123 -0.065534  0.005330 -0.016653  0.142608  0.400511   \n",
       "3           3  0.865498 -1.222110 -0.553205  0.022980  0.309707 -0.115284   \n",
       "4           4 -1.341069 -0.078229 -0.045112  0.219325 -0.030194 -0.500378   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -1.161452  0.308622 -0.139144 -0.117040  0.072620  0.152300   \n",
       "5978     5978 -1.547982 -0.166368  0.062790 -0.058637 -0.039730  0.145217   \n",
       "5979     5979  0.204833  1.663809 -0.728419  0.605944  0.811258  1.091609   \n",
       "5980     5980 -0.925491 -0.298534 -0.188338 -0.024190  0.231828  0.287504   \n",
       "5981     5981 -0.512350 -0.620099  0.938312 -0.100132 -0.191828  0.116272   \n",
       "\n",
       "             6         7         8  ...       101       102       103  \\\n",
       "0     0.831769 -0.575745 -0.703853  ...  0.102421 -0.318619 -0.375416   \n",
       "1     0.129344  0.058812  0.209729  ...  0.028266  0.074902  0.029886   \n",
       "2    -0.053878 -0.125965 -0.270389  ...  0.018800  0.036842 -0.011082   \n",
       "3    -0.423851  0.164560  0.401825  ...  0.122099  0.050478  0.119724   \n",
       "4     0.017921  0.145940 -0.016312  ...  0.012135  0.031032 -0.017324   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977 -0.058142 -0.208458 -0.120671  ...  0.004387 -0.135064  0.031170   \n",
       "5978  0.182716  0.213168  0.241474  ... -0.017176 -0.011808 -0.000613   \n",
       "5979 -0.392112  1.131950 -0.592724  ... -0.523407  0.207549 -0.355611   \n",
       "5980  0.755301 -0.127350  0.329944  ... -0.028420 -0.098941 -0.130566   \n",
       "5981 -0.882415 -0.106105 -0.051885  ...  0.065957 -0.151601  0.072650   \n",
       "\n",
       "           104       105       106       107       108       109       110  \n",
       "0    -0.050509  0.185363 -0.019726 -0.122237 -0.086713 -0.053711 -0.077012  \n",
       "1     0.116798 -0.031030 -0.049614  0.024517  0.003476  0.001157  0.006372  \n",
       "2    -0.020539 -0.010853 -0.012236 -0.003550 -0.000766  0.007166  0.015099  \n",
       "3    -0.141897 -0.014390  0.095265 -0.023132  0.142257  0.325156  0.171970  \n",
       "4    -0.006011  0.007325 -0.018732  0.004144 -0.010431 -0.004584 -0.006360  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977 -0.002395 -0.009385  0.033274  0.079296  0.230744 -0.209229 -0.247969  \n",
       "5978 -0.003491 -0.025970 -0.001241  0.008654 -0.008849 -0.015761 -0.011166  \n",
       "5979 -0.084467  0.018351  0.011924  0.144681  0.012927  0.108045 -0.044070  \n",
       "5980  0.075247 -0.050180  0.088172 -0.168739 -0.393781  0.228118  0.392946  \n",
       "5981 -0.214053 -0.182360  0.173993  0.166463  0.261078  0.370338  0.349272  \n",
       "\n",
       "[5982 rows x 112 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                           aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:04<00:17,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7412557339449541 SEED: 9057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:08<00:13,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7484448846260774 SEED: 8840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:12<00:08,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7494830761745899 SEED: 2103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:17<00:04,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7459297678621073 SEED: 3043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7396267723102585 SEED: 5221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 대분류 구매여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.436058</td>\n",
       "      <td>-0.040728</td>\n",
       "      <td>-0.376852</td>\n",
       "      <td>0.244498</td>\n",
       "      <td>-0.337669</td>\n",
       "      <td>0.594535</td>\n",
       "      <td>1.160821</td>\n",
       "      <td>-0.507886</td>\n",
       "      <td>-0.026341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371315</td>\n",
       "      <td>-0.820841</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.359050</td>\n",
       "      <td>-0.052447</td>\n",
       "      <td>0.167750</td>\n",
       "      <td>-0.028469</td>\n",
       "      <td>0.045124</td>\n",
       "      <td>0.103598</td>\n",
       "      <td>0.014635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.495079</td>\n",
       "      <td>0.426246</td>\n",
       "      <td>0.603980</td>\n",
       "      <td>0.267780</td>\n",
       "      <td>0.167863</td>\n",
       "      <td>-0.341812</td>\n",
       "      <td>0.074077</td>\n",
       "      <td>0.082130</td>\n",
       "      <td>-0.268278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027723</td>\n",
       "      <td>-0.408299</td>\n",
       "      <td>-0.796369</td>\n",
       "      <td>-0.191770</td>\n",
       "      <td>0.136208</td>\n",
       "      <td>0.175161</td>\n",
       "      <td>-0.027328</td>\n",
       "      <td>0.023678</td>\n",
       "      <td>0.019509</td>\n",
       "      <td>-0.013586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.486196</td>\n",
       "      <td>0.137751</td>\n",
       "      <td>0.094870</td>\n",
       "      <td>0.455594</td>\n",
       "      <td>0.025952</td>\n",
       "      <td>0.268160</td>\n",
       "      <td>0.156926</td>\n",
       "      <td>-0.156983</td>\n",
       "      <td>-0.082030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102234</td>\n",
       "      <td>0.202870</td>\n",
       "      <td>0.137714</td>\n",
       "      <td>0.092741</td>\n",
       "      <td>-0.046748</td>\n",
       "      <td>-0.249884</td>\n",
       "      <td>0.133006</td>\n",
       "      <td>0.062282</td>\n",
       "      <td>-0.045456</td>\n",
       "      <td>-0.040493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.826307</td>\n",
       "      <td>-0.968225</td>\n",
       "      <td>0.253546</td>\n",
       "      <td>0.553680</td>\n",
       "      <td>-0.338130</td>\n",
       "      <td>-0.531447</td>\n",
       "      <td>-0.663852</td>\n",
       "      <td>0.274229</td>\n",
       "      <td>0.311605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.615561</td>\n",
       "      <td>0.226854</td>\n",
       "      <td>0.448593</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>-0.397996</td>\n",
       "      <td>-0.505601</td>\n",
       "      <td>-0.206288</td>\n",
       "      <td>-0.023323</td>\n",
       "      <td>-0.058132</td>\n",
       "      <td>-0.078507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.388948</td>\n",
       "      <td>0.099101</td>\n",
       "      <td>0.054519</td>\n",
       "      <td>-0.485316</td>\n",
       "      <td>-0.493745</td>\n",
       "      <td>0.040495</td>\n",
       "      <td>0.094735</td>\n",
       "      <td>-0.274916</td>\n",
       "      <td>-0.333733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094130</td>\n",
       "      <td>0.096581</td>\n",
       "      <td>-0.122793</td>\n",
       "      <td>0.078391</td>\n",
       "      <td>-0.059339</td>\n",
       "      <td>-0.210875</td>\n",
       "      <td>0.119112</td>\n",
       "      <td>0.095087</td>\n",
       "      <td>-0.033616</td>\n",
       "      <td>0.010124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-0.758481</td>\n",
       "      <td>0.812565</td>\n",
       "      <td>-0.108292</td>\n",
       "      <td>0.172876</td>\n",
       "      <td>0.196507</td>\n",
       "      <td>-0.143411</td>\n",
       "      <td>0.183231</td>\n",
       "      <td>0.159825</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.780837</td>\n",
       "      <td>-0.074323</td>\n",
       "      <td>-0.534725</td>\n",
       "      <td>-0.266745</td>\n",
       "      <td>-0.442471</td>\n",
       "      <td>-0.236793</td>\n",
       "      <td>-0.081901</td>\n",
       "      <td>0.159949</td>\n",
       "      <td>-0.030645</td>\n",
       "      <td>0.010190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.564872</td>\n",
       "      <td>0.227113</td>\n",
       "      <td>0.097719</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>-0.014250</td>\n",
       "      <td>0.254983</td>\n",
       "      <td>-0.491875</td>\n",
       "      <td>-0.342264</td>\n",
       "      <td>0.369359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097276</td>\n",
       "      <td>-0.100308</td>\n",
       "      <td>0.012817</td>\n",
       "      <td>0.030391</td>\n",
       "      <td>0.023927</td>\n",
       "      <td>0.100438</td>\n",
       "      <td>-0.032773</td>\n",
       "      <td>-0.034280</td>\n",
       "      <td>-0.015645</td>\n",
       "      <td>0.006128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>1.010739</td>\n",
       "      <td>1.482501</td>\n",
       "      <td>1.119399</td>\n",
       "      <td>0.718032</td>\n",
       "      <td>-0.021002</td>\n",
       "      <td>0.340607</td>\n",
       "      <td>-0.337983</td>\n",
       "      <td>0.478843</td>\n",
       "      <td>0.405145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111127</td>\n",
       "      <td>0.135636</td>\n",
       "      <td>-0.055767</td>\n",
       "      <td>-0.729023</td>\n",
       "      <td>-0.262131</td>\n",
       "      <td>0.856770</td>\n",
       "      <td>0.272822</td>\n",
       "      <td>0.424178</td>\n",
       "      <td>-0.637153</td>\n",
       "      <td>-0.439690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.907570</td>\n",
       "      <td>0.014547</td>\n",
       "      <td>0.360717</td>\n",
       "      <td>0.103525</td>\n",
       "      <td>0.415867</td>\n",
       "      <td>-0.849732</td>\n",
       "      <td>0.382268</td>\n",
       "      <td>-0.345673</td>\n",
       "      <td>0.521682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.653179</td>\n",
       "      <td>0.014776</td>\n",
       "      <td>0.284553</td>\n",
       "      <td>-0.030792</td>\n",
       "      <td>-0.207601</td>\n",
       "      <td>-0.053542</td>\n",
       "      <td>-0.043660</td>\n",
       "      <td>-0.118073</td>\n",
       "      <td>-0.046550</td>\n",
       "      <td>-0.100628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.916461</td>\n",
       "      <td>-0.791022</td>\n",
       "      <td>0.136646</td>\n",
       "      <td>0.219709</td>\n",
       "      <td>-0.125223</td>\n",
       "      <td>0.073441</td>\n",
       "      <td>-0.344049</td>\n",
       "      <td>0.796474</td>\n",
       "      <td>-0.018890</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057549</td>\n",
       "      <td>-0.350444</td>\n",
       "      <td>-0.070858</td>\n",
       "      <td>0.011655</td>\n",
       "      <td>-0.310068</td>\n",
       "      <td>-0.501102</td>\n",
       "      <td>0.272890</td>\n",
       "      <td>0.247785</td>\n",
       "      <td>-0.007448</td>\n",
       "      <td>0.010920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  1.436058 -0.040728 -0.376852  0.244498 -0.337669  0.594535   \n",
       "1           1 -1.495079  0.426246  0.603980  0.267780  0.167863 -0.341812   \n",
       "2           2 -1.486196  0.137751  0.094870  0.455594  0.025952  0.268160   \n",
       "3           3  0.826307 -0.968225  0.253546  0.553680 -0.338130 -0.531447   \n",
       "4           4 -1.388948  0.099101  0.054519 -0.485316 -0.493745  0.040495   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -0.758481  0.812565 -0.108292  0.172876  0.196507 -0.143411   \n",
       "5978     5978 -1.564872  0.227113  0.097719  0.005096 -0.014250  0.254983   \n",
       "5979     5979  1.010739  1.482501  1.119399  0.718032 -0.021002  0.340607   \n",
       "5980     5980 -0.907570  0.014547  0.360717  0.103525  0.415867 -0.849732   \n",
       "5981     5981 -0.916461 -0.791022  0.136646  0.219709 -0.125223  0.073441   \n",
       "\n",
       "             6         7         8  ...        26        27        28  \\\n",
       "0     1.160821 -0.507886 -0.026341  ...  0.371315 -0.820841  0.280656   \n",
       "1     0.074077  0.082130 -0.268278  ... -0.027723 -0.408299 -0.796369   \n",
       "2     0.156926 -0.156983 -0.082030  ...  0.102234  0.202870  0.137714   \n",
       "3    -0.663852  0.274229  0.311605  ... -0.615561  0.226854  0.448593   \n",
       "4     0.094735 -0.274916 -0.333733  ...  0.094130  0.096581 -0.122793   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977  0.183231  0.159825 -0.234539  ... -0.780837 -0.074323 -0.534725   \n",
       "5978 -0.491875 -0.342264  0.369359  ... -0.097276 -0.100308  0.012817   \n",
       "5979 -0.337983  0.478843  0.405145  ...  0.111127  0.135636 -0.055767   \n",
       "5980  0.382268 -0.345673  0.521682  ... -0.653179  0.014776  0.284553   \n",
       "5981 -0.344049  0.796474 -0.018890  ... -0.057549 -0.350444 -0.070858   \n",
       "\n",
       "            29        30        31        32        33        34        35  \n",
       "0     0.359050 -0.052447  0.167750 -0.028469  0.045124  0.103598  0.014635  \n",
       "1    -0.191770  0.136208  0.175161 -0.027328  0.023678  0.019509 -0.013586  \n",
       "2     0.092741 -0.046748 -0.249884  0.133006  0.062282 -0.045456 -0.040493  \n",
       "3     0.001418 -0.397996 -0.505601 -0.206288 -0.023323 -0.058132 -0.078507  \n",
       "4     0.078391 -0.059339 -0.210875  0.119112  0.095087 -0.033616  0.010124  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977 -0.266745 -0.442471 -0.236793 -0.081901  0.159949 -0.030645  0.010190  \n",
       "5978  0.030391  0.023927  0.100438 -0.032773 -0.034280 -0.015645  0.006128  \n",
       "5979 -0.729023 -0.262131  0.856770  0.272822  0.424178 -0.637153 -0.439690  \n",
       "5980 -0.030792 -0.207601 -0.053542 -0.043660 -0.118073 -0.046550 -0.100628  \n",
       "5981  0.011655 -0.310068 -0.501102  0.272890  0.247785 -0.007448  0.010920  \n",
       "\n",
       "[5982 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_mclas_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                           aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:04<00:18,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7575974770642201 SEED: 9057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:08<00:12,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7543048026132889 SEED: 8840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:12<00:08,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7567808242980261 SEED: 2103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:17<00:04,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7553299624687241 SEED: 3043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:21<00:00,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7562421809841533 SEED: 5221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 중분류 & 대분류 구매건수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.625380</td>\n",
       "      <td>0.034396</td>\n",
       "      <td>-0.672246</td>\n",
       "      <td>0.441779</td>\n",
       "      <td>0.159294</td>\n",
       "      <td>-0.598718</td>\n",
       "      <td>0.325886</td>\n",
       "      <td>-0.936302</td>\n",
       "      <td>0.305429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143142</td>\n",
       "      <td>0.118564</td>\n",
       "      <td>-0.002334</td>\n",
       "      <td>0.089901</td>\n",
       "      <td>-0.111054</td>\n",
       "      <td>-0.021497</td>\n",
       "      <td>0.140365</td>\n",
       "      <td>0.117912</td>\n",
       "      <td>-0.126485</td>\n",
       "      <td>0.046712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.107635</td>\n",
       "      <td>-0.008147</td>\n",
       "      <td>-0.036582</td>\n",
       "      <td>-0.227227</td>\n",
       "      <td>-0.173067</td>\n",
       "      <td>0.257546</td>\n",
       "      <td>0.036107</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>-0.073846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030561</td>\n",
       "      <td>-0.018014</td>\n",
       "      <td>0.032755</td>\n",
       "      <td>0.015114</td>\n",
       "      <td>0.011464</td>\n",
       "      <td>-0.002095</td>\n",
       "      <td>-0.073120</td>\n",
       "      <td>-0.051948</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>-0.007939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.099033</td>\n",
       "      <td>-0.109059</td>\n",
       "      <td>-0.020829</td>\n",
       "      <td>0.007957</td>\n",
       "      <td>-0.059710</td>\n",
       "      <td>0.090729</td>\n",
       "      <td>0.179059</td>\n",
       "      <td>-0.068382</td>\n",
       "      <td>0.062943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.035945</td>\n",
       "      <td>-0.017813</td>\n",
       "      <td>-0.008323</td>\n",
       "      <td>-0.036238</td>\n",
       "      <td>0.021623</td>\n",
       "      <td>0.038809</td>\n",
       "      <td>0.046284</td>\n",
       "      <td>0.007039</td>\n",
       "      <td>0.014445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.265591</td>\n",
       "      <td>-0.619277</td>\n",
       "      <td>-0.279149</td>\n",
       "      <td>-0.187955</td>\n",
       "      <td>0.023135</td>\n",
       "      <td>-0.135279</td>\n",
       "      <td>-0.067050</td>\n",
       "      <td>0.675716</td>\n",
       "      <td>-0.159730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131376</td>\n",
       "      <td>0.064030</td>\n",
       "      <td>-0.118500</td>\n",
       "      <td>0.066814</td>\n",
       "      <td>-0.149491</td>\n",
       "      <td>-0.003301</td>\n",
       "      <td>0.486243</td>\n",
       "      <td>-0.482798</td>\n",
       "      <td>-0.028555</td>\n",
       "      <td>-0.156546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.141518</td>\n",
       "      <td>-0.191579</td>\n",
       "      <td>0.005533</td>\n",
       "      <td>-0.059629</td>\n",
       "      <td>0.026487</td>\n",
       "      <td>0.013643</td>\n",
       "      <td>-0.095080</td>\n",
       "      <td>0.043151</td>\n",
       "      <td>-0.057431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008822</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>-0.005974</td>\n",
       "      <td>-0.006765</td>\n",
       "      <td>0.007707</td>\n",
       "      <td>0.013415</td>\n",
       "      <td>-0.002152</td>\n",
       "      <td>0.008844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-0.877498</td>\n",
       "      <td>0.236507</td>\n",
       "      <td>-0.127787</td>\n",
       "      <td>-0.066931</td>\n",
       "      <td>-0.135133</td>\n",
       "      <td>0.047662</td>\n",
       "      <td>0.136196</td>\n",
       "      <td>-0.051518</td>\n",
       "      <td>-0.006189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100014</td>\n",
       "      <td>-0.477439</td>\n",
       "      <td>-0.028659</td>\n",
       "      <td>0.204758</td>\n",
       "      <td>-0.015639</td>\n",
       "      <td>0.083038</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>-0.005576</td>\n",
       "      <td>-0.006840</td>\n",
       "      <td>0.081172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.162111</td>\n",
       "      <td>-0.159106</td>\n",
       "      <td>0.019576</td>\n",
       "      <td>0.078777</td>\n",
       "      <td>0.026611</td>\n",
       "      <td>-0.007421</td>\n",
       "      <td>-0.020261</td>\n",
       "      <td>0.025530</td>\n",
       "      <td>-0.021468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005455</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>0.011396</td>\n",
       "      <td>0.006313</td>\n",
       "      <td>-0.050464</td>\n",
       "      <td>-0.032066</td>\n",
       "      <td>-0.036101</td>\n",
       "      <td>-0.007855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>0.199450</td>\n",
       "      <td>1.781264</td>\n",
       "      <td>-0.540263</td>\n",
       "      <td>0.071173</td>\n",
       "      <td>0.331206</td>\n",
       "      <td>1.250888</td>\n",
       "      <td>0.351207</td>\n",
       "      <td>-0.141133</td>\n",
       "      <td>-0.117322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100453</td>\n",
       "      <td>0.028486</td>\n",
       "      <td>-0.128008</td>\n",
       "      <td>0.088439</td>\n",
       "      <td>-0.112445</td>\n",
       "      <td>-0.079405</td>\n",
       "      <td>0.109667</td>\n",
       "      <td>0.038992</td>\n",
       "      <td>-0.088774</td>\n",
       "      <td>0.054421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.635700</td>\n",
       "      <td>-0.227858</td>\n",
       "      <td>-0.237216</td>\n",
       "      <td>0.009492</td>\n",
       "      <td>-0.166560</td>\n",
       "      <td>-0.074526</td>\n",
       "      <td>0.129139</td>\n",
       "      <td>-0.274902</td>\n",
       "      <td>-0.380003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.285796</td>\n",
       "      <td>0.651523</td>\n",
       "      <td>0.113559</td>\n",
       "      <td>-0.241631</td>\n",
       "      <td>0.065565</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>-0.040256</td>\n",
       "      <td>-0.085648</td>\n",
       "      <td>-0.044685</td>\n",
       "      <td>-0.053650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.674396</td>\n",
       "      <td>-0.398007</td>\n",
       "      <td>0.607995</td>\n",
       "      <td>-0.003883</td>\n",
       "      <td>-0.024185</td>\n",
       "      <td>0.051853</td>\n",
       "      <td>0.060919</td>\n",
       "      <td>0.352332</td>\n",
       "      <td>0.488067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256775</td>\n",
       "      <td>0.148656</td>\n",
       "      <td>-0.043532</td>\n",
       "      <td>-0.055591</td>\n",
       "      <td>-0.132109</td>\n",
       "      <td>-0.018838</td>\n",
       "      <td>0.036651</td>\n",
       "      <td>0.041369</td>\n",
       "      <td>0.035976</td>\n",
       "      <td>-0.103166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  0.625380  0.034396 -0.672246  0.441779  0.159294 -0.598718   \n",
       "1           1 -1.107635 -0.008147 -0.036582 -0.227227 -0.173067  0.257546   \n",
       "2           2 -1.099033 -0.109059 -0.020829  0.007957 -0.059710  0.090729   \n",
       "3           3  0.265591 -0.619277 -0.279149 -0.187955  0.023135 -0.135279   \n",
       "4           4 -1.141518 -0.191579  0.005533 -0.059629  0.026487  0.013643   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -0.877498  0.236507 -0.127787 -0.066931 -0.135133  0.047662   \n",
       "5978     5978 -1.162111 -0.159106  0.019576  0.078777  0.026611 -0.007421   \n",
       "5979     5979  0.199450  1.781264 -0.540263  0.071173  0.331206  1.250888   \n",
       "5980     5980 -0.635700 -0.227858 -0.237216  0.009492 -0.166560 -0.074526   \n",
       "5981     5981 -0.674396 -0.398007  0.607995 -0.003883 -0.024185  0.051853   \n",
       "\n",
       "             6         7         8  ...       100       101       102  \\\n",
       "0     0.325886 -0.936302  0.305429  ... -0.143142  0.118564 -0.002334   \n",
       "1     0.036107 -0.059078 -0.073846  ...  0.030561 -0.018014  0.032755   \n",
       "2     0.179059 -0.068382  0.062943  ...  0.002262  0.035945 -0.017813   \n",
       "3    -0.067050  0.675716 -0.159730  ...  0.131376  0.064030 -0.118500   \n",
       "4    -0.095080  0.043151 -0.057431  ...  0.008822  0.004029  0.004533   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977  0.136196 -0.051518 -0.006189  ...  0.100014 -0.477439 -0.028659   \n",
       "5978 -0.020261  0.025530 -0.021468  ...  0.005455  0.005500  0.013200   \n",
       "5979  0.351207 -0.141133 -0.117322  ...  0.100453  0.028486 -0.128008   \n",
       "5980  0.129139 -0.274902 -0.380003  ... -0.285796  0.651523  0.113559   \n",
       "5981  0.060919  0.352332  0.488067  ...  0.256775  0.148656 -0.043532   \n",
       "\n",
       "           103       104       105       106       107       108       109  \n",
       "0     0.089901 -0.111054 -0.021497  0.140365  0.117912 -0.126485  0.046712  \n",
       "1     0.015114  0.011464 -0.002095 -0.073120 -0.051948  0.005610 -0.007939  \n",
       "2    -0.008323 -0.036238  0.021623  0.038809  0.046284  0.007039  0.014445  \n",
       "3     0.066814 -0.149491 -0.003301  0.486243 -0.482798 -0.028555 -0.156546  \n",
       "4    -0.000862 -0.005974 -0.006765  0.007707  0.013415 -0.002152  0.008844  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977  0.204758 -0.015639  0.083038  0.010453 -0.005576 -0.006840  0.081172  \n",
       "5978  0.006884  0.011396  0.006313 -0.050464 -0.032066 -0.036101 -0.007855  \n",
       "5979  0.088439 -0.112445 -0.079405  0.109667  0.038992 -0.088774  0.054421  \n",
       "5980 -0.241631  0.065565  0.000818 -0.040256 -0.085648 -0.044685 -0.053650  \n",
       "5981 -0.055591 -0.132109 -0.018838  0.036651  0.041369  0.035976 -0.103166  \n",
       "\n",
       "[5982 rows x 111 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.808812</td>\n",
       "      <td>0.047621</td>\n",
       "      <td>-0.382530</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>0.068270</td>\n",
       "      <td>-1.067826</td>\n",
       "      <td>0.097073</td>\n",
       "      <td>0.064659</td>\n",
       "      <td>0.123847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278258</td>\n",
       "      <td>-0.198159</td>\n",
       "      <td>-0.226099</td>\n",
       "      <td>-0.038080</td>\n",
       "      <td>0.324296</td>\n",
       "      <td>-0.434571</td>\n",
       "      <td>-0.308631</td>\n",
       "      <td>-0.016363</td>\n",
       "      <td>0.194200</td>\n",
       "      <td>-0.018975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.942326</td>\n",
       "      <td>0.181016</td>\n",
       "      <td>0.039485</td>\n",
       "      <td>0.224548</td>\n",
       "      <td>0.165447</td>\n",
       "      <td>0.185474</td>\n",
       "      <td>0.260437</td>\n",
       "      <td>0.067577</td>\n",
       "      <td>-0.233817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035076</td>\n",
       "      <td>-0.147834</td>\n",
       "      <td>0.062239</td>\n",
       "      <td>0.060819</td>\n",
       "      <td>-0.054463</td>\n",
       "      <td>-0.387346</td>\n",
       "      <td>0.112056</td>\n",
       "      <td>-0.059185</td>\n",
       "      <td>0.006323</td>\n",
       "      <td>-0.030976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.939487</td>\n",
       "      <td>0.072728</td>\n",
       "      <td>-0.171136</td>\n",
       "      <td>0.257513</td>\n",
       "      <td>-0.163384</td>\n",
       "      <td>-0.015842</td>\n",
       "      <td>-0.109646</td>\n",
       "      <td>0.098185</td>\n",
       "      <td>-0.189282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057341</td>\n",
       "      <td>0.047966</td>\n",
       "      <td>-0.038936</td>\n",
       "      <td>-0.112071</td>\n",
       "      <td>-0.006490</td>\n",
       "      <td>0.062245</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>0.044459</td>\n",
       "      <td>0.039179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.176414</td>\n",
       "      <td>-0.540114</td>\n",
       "      <td>-0.095850</td>\n",
       "      <td>0.039596</td>\n",
       "      <td>0.054163</td>\n",
       "      <td>0.377580</td>\n",
       "      <td>0.169790</td>\n",
       "      <td>-0.019942</td>\n",
       "      <td>0.183439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>-0.014416</td>\n",
       "      <td>-0.061704</td>\n",
       "      <td>0.120444</td>\n",
       "      <td>-0.374014</td>\n",
       "      <td>0.177117</td>\n",
       "      <td>-0.243579</td>\n",
       "      <td>0.075744</td>\n",
       "      <td>-0.142311</td>\n",
       "      <td>-0.229687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.006475</td>\n",
       "      <td>-0.015659</td>\n",
       "      <td>0.083409</td>\n",
       "      <td>-0.036429</td>\n",
       "      <td>0.082568</td>\n",
       "      <td>-0.020770</td>\n",
       "      <td>0.012661</td>\n",
       "      <td>-0.029358</td>\n",
       "      <td>0.030033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008659</td>\n",
       "      <td>0.040425</td>\n",
       "      <td>-0.007492</td>\n",
       "      <td>0.042274</td>\n",
       "      <td>0.049531</td>\n",
       "      <td>-0.034650</td>\n",
       "      <td>-0.017044</td>\n",
       "      <td>-0.018064</td>\n",
       "      <td>0.021210</td>\n",
       "      <td>-0.000362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-0.527805</td>\n",
       "      <td>0.460797</td>\n",
       "      <td>-0.064967</td>\n",
       "      <td>-0.029257</td>\n",
       "      <td>0.232430</td>\n",
       "      <td>-0.141500</td>\n",
       "      <td>0.104384</td>\n",
       "      <td>0.065651</td>\n",
       "      <td>-0.706434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.477599</td>\n",
       "      <td>0.215720</td>\n",
       "      <td>0.023805</td>\n",
       "      <td>0.367133</td>\n",
       "      <td>0.026163</td>\n",
       "      <td>0.424302</td>\n",
       "      <td>-0.089105</td>\n",
       "      <td>-0.048746</td>\n",
       "      <td>0.062714</td>\n",
       "      <td>-0.055811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.038196</td>\n",
       "      <td>0.022598</td>\n",
       "      <td>0.023371</td>\n",
       "      <td>-0.003908</td>\n",
       "      <td>0.008111</td>\n",
       "      <td>0.030509</td>\n",
       "      <td>-0.068062</td>\n",
       "      <td>-0.030968</td>\n",
       "      <td>0.011029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106910</td>\n",
       "      <td>0.017061</td>\n",
       "      <td>0.158339</td>\n",
       "      <td>-0.014808</td>\n",
       "      <td>-0.045128</td>\n",
       "      <td>0.014905</td>\n",
       "      <td>-0.012643</td>\n",
       "      <td>-0.058065</td>\n",
       "      <td>0.033216</td>\n",
       "      <td>-0.046763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>0.672151</td>\n",
       "      <td>1.288926</td>\n",
       "      <td>0.047105</td>\n",
       "      <td>0.442555</td>\n",
       "      <td>-0.225962</td>\n",
       "      <td>0.620944</td>\n",
       "      <td>-0.542938</td>\n",
       "      <td>-0.306575</td>\n",
       "      <td>-0.724191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267473</td>\n",
       "      <td>0.172990</td>\n",
       "      <td>0.246498</td>\n",
       "      <td>-0.123809</td>\n",
       "      <td>-0.203993</td>\n",
       "      <td>-0.005225</td>\n",
       "      <td>-0.029028</td>\n",
       "      <td>0.124641</td>\n",
       "      <td>0.155520</td>\n",
       "      <td>0.100988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.561052</td>\n",
       "      <td>-0.015907</td>\n",
       "      <td>0.100263</td>\n",
       "      <td>0.082922</td>\n",
       "      <td>-0.023670</td>\n",
       "      <td>0.353346</td>\n",
       "      <td>0.424770</td>\n",
       "      <td>-0.039177</td>\n",
       "      <td>0.215357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149904</td>\n",
       "      <td>-0.020741</td>\n",
       "      <td>-0.158265</td>\n",
       "      <td>0.161862</td>\n",
       "      <td>-0.201403</td>\n",
       "      <td>-0.103223</td>\n",
       "      <td>-0.567873</td>\n",
       "      <td>-0.410687</td>\n",
       "      <td>-0.080075</td>\n",
       "      <td>0.002968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.804268</td>\n",
       "      <td>-0.300905</td>\n",
       "      <td>-0.010156</td>\n",
       "      <td>0.041940</td>\n",
       "      <td>0.061396</td>\n",
       "      <td>0.123361</td>\n",
       "      <td>-0.009046</td>\n",
       "      <td>-0.017363</td>\n",
       "      <td>-0.044154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064577</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>-0.091283</td>\n",
       "      <td>0.040444</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>-0.027214</td>\n",
       "      <td>-0.019029</td>\n",
       "      <td>0.156810</td>\n",
       "      <td>0.123323</td>\n",
       "      <td>0.190463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  0.808812  0.047621 -0.382530  0.136842  0.068270 -1.067826   \n",
       "1           1 -0.942326  0.181016  0.039485  0.224548  0.165447  0.185474   \n",
       "2           2 -0.939487  0.072728 -0.171136  0.257513 -0.163384 -0.015842   \n",
       "3           3  0.176414 -0.540114 -0.095850  0.039596  0.054163  0.377580   \n",
       "4           4 -1.006475 -0.015659  0.083409 -0.036429  0.082568 -0.020770   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -0.527805  0.460797 -0.064967 -0.029257  0.232430 -0.141500   \n",
       "5978     5978 -1.038196  0.022598  0.023371 -0.003908  0.008111  0.030509   \n",
       "5979     5979  0.672151  1.288926  0.047105  0.442555 -0.225962  0.620944   \n",
       "5980     5980 -0.561052 -0.015907  0.100263  0.082922 -0.023670  0.353346   \n",
       "5981     5981 -0.804268 -0.300905 -0.010156  0.041940  0.061396  0.123361   \n",
       "\n",
       "             6         7         8  ...        25        26        27  \\\n",
       "0     0.097073  0.064659  0.123847  ...  0.278258 -0.198159 -0.226099   \n",
       "1     0.260437  0.067577 -0.233817  ... -0.035076 -0.147834  0.062239   \n",
       "2    -0.109646  0.098185 -0.189282  ...  0.057341  0.047966 -0.038936   \n",
       "3     0.169790 -0.019942  0.183439  ...  0.156863 -0.014416 -0.061704   \n",
       "4     0.012661 -0.029358  0.030033  ...  0.008659  0.040425 -0.007492   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977  0.104384  0.065651 -0.706434  ... -0.477599  0.215720  0.023805   \n",
       "5978 -0.068062 -0.030968  0.011029  ...  0.106910  0.017061  0.158339   \n",
       "5979 -0.542938 -0.306575 -0.724191  ... -0.267473  0.172990  0.246498   \n",
       "5980  0.424770 -0.039177  0.215357  ...  0.149904 -0.020741 -0.158265   \n",
       "5981 -0.009046 -0.017363 -0.044154  ... -0.064577  0.030567 -0.091283   \n",
       "\n",
       "            28        29        30        31        32        33        34  \n",
       "0    -0.038080  0.324296 -0.434571 -0.308631 -0.016363  0.194200 -0.018975  \n",
       "1     0.060819 -0.054463 -0.387346  0.112056 -0.059185  0.006323 -0.030976  \n",
       "2    -0.112071 -0.006490  0.062245 -0.004990  0.035366  0.044459  0.039179  \n",
       "3     0.120444 -0.374014  0.177117 -0.243579  0.075744 -0.142311 -0.229687  \n",
       "4     0.042274  0.049531 -0.034650 -0.017044 -0.018064  0.021210 -0.000362  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977  0.367133  0.026163  0.424302 -0.089105 -0.048746  0.062714 -0.055811  \n",
       "5978 -0.014808 -0.045128  0.014905 -0.012643 -0.058065  0.033216 -0.046763  \n",
       "5979 -0.123809 -0.203993 -0.005225 -0.029028  0.124641  0.155520  0.100988  \n",
       "5980  0.161862 -0.201403 -0.103223 -0.567873 -0.410687 -0.080075  0.002968  \n",
       "5981  0.040444  0.000616 -0.027214 -0.019029  0.156810  0.123323  0.190463  \n",
       "\n",
       "[5982 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                            aggfunc=lambda x: len(x), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train_nm = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test_nm = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_mclas_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                            aggfunc=lambda x: len(x), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train_mclas = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test_mclas = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "### 중분류 구매건수 nm과 대분류 구매건수 mclas를 합친다\n",
    "\n",
    "X_train = pd.concat([X_train_nm, X_train_mclas], axis=1)\n",
    "X_test = pd.concat([X_test_nm, X_test_mclas], axis=1)\n",
    "\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:05<00:22,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7549954823463997 SEED: 1030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:09<00:15,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7503388240200168 SEED: 5345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:14<00:10,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7455952877397832 SEED: 6625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:19<00:04,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7541614539894357 SEED: 2816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:23<00:00,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.751207603558521 SEED: 7695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 중분류 & 대분류 구매여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.031835</td>\n",
       "      <td>0.212301</td>\n",
       "      <td>-0.922307</td>\n",
       "      <td>-0.869803</td>\n",
       "      <td>-0.305008</td>\n",
       "      <td>0.092058</td>\n",
       "      <td>0.831769</td>\n",
       "      <td>-0.575745</td>\n",
       "      <td>-0.703853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102421</td>\n",
       "      <td>-0.318619</td>\n",
       "      <td>-0.375416</td>\n",
       "      <td>-0.050509</td>\n",
       "      <td>0.185363</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>-0.122237</td>\n",
       "      <td>-0.086713</td>\n",
       "      <td>-0.053711</td>\n",
       "      <td>-0.077012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.545397</td>\n",
       "      <td>-0.120407</td>\n",
       "      <td>0.028675</td>\n",
       "      <td>0.202452</td>\n",
       "      <td>0.264835</td>\n",
       "      <td>0.240070</td>\n",
       "      <td>0.129344</td>\n",
       "      <td>0.058812</td>\n",
       "      <td>0.209729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028266</td>\n",
       "      <td>0.074902</td>\n",
       "      <td>0.029886</td>\n",
       "      <td>0.116798</td>\n",
       "      <td>-0.031030</td>\n",
       "      <td>-0.049614</td>\n",
       "      <td>0.024517</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.006372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.415123</td>\n",
       "      <td>-0.065534</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>-0.016653</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0.400511</td>\n",
       "      <td>-0.053878</td>\n",
       "      <td>-0.125965</td>\n",
       "      <td>-0.270389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>-0.011082</td>\n",
       "      <td>-0.020539</td>\n",
       "      <td>-0.010853</td>\n",
       "      <td>-0.012236</td>\n",
       "      <td>-0.003550</td>\n",
       "      <td>-0.000766</td>\n",
       "      <td>0.007166</td>\n",
       "      <td>0.015099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.865498</td>\n",
       "      <td>-1.222110</td>\n",
       "      <td>-0.553205</td>\n",
       "      <td>0.022980</td>\n",
       "      <td>0.309707</td>\n",
       "      <td>-0.115284</td>\n",
       "      <td>-0.423851</td>\n",
       "      <td>0.164560</td>\n",
       "      <td>0.401825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122099</td>\n",
       "      <td>0.050478</td>\n",
       "      <td>0.119724</td>\n",
       "      <td>-0.141897</td>\n",
       "      <td>-0.014390</td>\n",
       "      <td>0.095265</td>\n",
       "      <td>-0.023132</td>\n",
       "      <td>0.142257</td>\n",
       "      <td>0.325156</td>\n",
       "      <td>0.171970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.341069</td>\n",
       "      <td>-0.078229</td>\n",
       "      <td>-0.045112</td>\n",
       "      <td>0.219325</td>\n",
       "      <td>-0.030194</td>\n",
       "      <td>-0.500378</td>\n",
       "      <td>0.017921</td>\n",
       "      <td>0.145940</td>\n",
       "      <td>-0.016312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012135</td>\n",
       "      <td>0.031032</td>\n",
       "      <td>-0.017324</td>\n",
       "      <td>-0.006011</td>\n",
       "      <td>0.007325</td>\n",
       "      <td>-0.018732</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>-0.010431</td>\n",
       "      <td>-0.004584</td>\n",
       "      <td>-0.006360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-1.161452</td>\n",
       "      <td>0.308622</td>\n",
       "      <td>-0.139144</td>\n",
       "      <td>-0.117040</td>\n",
       "      <td>0.072620</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>-0.058142</td>\n",
       "      <td>-0.208458</td>\n",
       "      <td>-0.120671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004387</td>\n",
       "      <td>-0.135064</td>\n",
       "      <td>0.031170</td>\n",
       "      <td>-0.002395</td>\n",
       "      <td>-0.009385</td>\n",
       "      <td>0.033274</td>\n",
       "      <td>0.079296</td>\n",
       "      <td>0.230744</td>\n",
       "      <td>-0.209229</td>\n",
       "      <td>-0.247969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.547982</td>\n",
       "      <td>-0.166368</td>\n",
       "      <td>0.062790</td>\n",
       "      <td>-0.058637</td>\n",
       "      <td>-0.039730</td>\n",
       "      <td>0.145217</td>\n",
       "      <td>0.182716</td>\n",
       "      <td>0.213168</td>\n",
       "      <td>0.241474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017176</td>\n",
       "      <td>-0.011808</td>\n",
       "      <td>-0.000613</td>\n",
       "      <td>-0.003491</td>\n",
       "      <td>-0.025970</td>\n",
       "      <td>-0.001241</td>\n",
       "      <td>0.008654</td>\n",
       "      <td>-0.008849</td>\n",
       "      <td>-0.015761</td>\n",
       "      <td>-0.011166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>0.204833</td>\n",
       "      <td>1.663809</td>\n",
       "      <td>-0.728419</td>\n",
       "      <td>0.605944</td>\n",
       "      <td>0.811258</td>\n",
       "      <td>1.091609</td>\n",
       "      <td>-0.392112</td>\n",
       "      <td>1.131950</td>\n",
       "      <td>-0.592724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523407</td>\n",
       "      <td>0.207549</td>\n",
       "      <td>-0.355611</td>\n",
       "      <td>-0.084467</td>\n",
       "      <td>0.018351</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.144681</td>\n",
       "      <td>0.012927</td>\n",
       "      <td>0.108045</td>\n",
       "      <td>-0.044070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.925491</td>\n",
       "      <td>-0.298534</td>\n",
       "      <td>-0.188338</td>\n",
       "      <td>-0.024190</td>\n",
       "      <td>0.231828</td>\n",
       "      <td>0.287504</td>\n",
       "      <td>0.755301</td>\n",
       "      <td>-0.127350</td>\n",
       "      <td>0.329944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028420</td>\n",
       "      <td>-0.098941</td>\n",
       "      <td>-0.130566</td>\n",
       "      <td>0.075247</td>\n",
       "      <td>-0.050180</td>\n",
       "      <td>0.088172</td>\n",
       "      <td>-0.168739</td>\n",
       "      <td>-0.393781</td>\n",
       "      <td>0.228118</td>\n",
       "      <td>0.392946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.512350</td>\n",
       "      <td>-0.620099</td>\n",
       "      <td>0.938312</td>\n",
       "      <td>-0.100132</td>\n",
       "      <td>-0.191828</td>\n",
       "      <td>0.116272</td>\n",
       "      <td>-0.882415</td>\n",
       "      <td>-0.106105</td>\n",
       "      <td>-0.051885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065957</td>\n",
       "      <td>-0.151601</td>\n",
       "      <td>0.072650</td>\n",
       "      <td>-0.214053</td>\n",
       "      <td>-0.182360</td>\n",
       "      <td>0.173993</td>\n",
       "      <td>0.166463</td>\n",
       "      <td>0.261078</td>\n",
       "      <td>0.370338</td>\n",
       "      <td>0.349272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  1.031835  0.212301 -0.922307 -0.869803 -0.305008  0.092058   \n",
       "1           1 -1.545397 -0.120407  0.028675  0.202452  0.264835  0.240070   \n",
       "2           2 -1.415123 -0.065534  0.005330 -0.016653  0.142608  0.400511   \n",
       "3           3  0.865498 -1.222110 -0.553205  0.022980  0.309707 -0.115284   \n",
       "4           4 -1.341069 -0.078229 -0.045112  0.219325 -0.030194 -0.500378   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -1.161452  0.308622 -0.139144 -0.117040  0.072620  0.152300   \n",
       "5978     5978 -1.547982 -0.166368  0.062790 -0.058637 -0.039730  0.145217   \n",
       "5979     5979  0.204833  1.663809 -0.728419  0.605944  0.811258  1.091609   \n",
       "5980     5980 -0.925491 -0.298534 -0.188338 -0.024190  0.231828  0.287504   \n",
       "5981     5981 -0.512350 -0.620099  0.938312 -0.100132 -0.191828  0.116272   \n",
       "\n",
       "             6         7         8  ...       101       102       103  \\\n",
       "0     0.831769 -0.575745 -0.703853  ...  0.102421 -0.318619 -0.375416   \n",
       "1     0.129344  0.058812  0.209729  ...  0.028266  0.074902  0.029886   \n",
       "2    -0.053878 -0.125965 -0.270389  ...  0.018800  0.036842 -0.011082   \n",
       "3    -0.423851  0.164560  0.401825  ...  0.122099  0.050478  0.119724   \n",
       "4     0.017921  0.145940 -0.016312  ...  0.012135  0.031032 -0.017324   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977 -0.058142 -0.208458 -0.120671  ...  0.004387 -0.135064  0.031170   \n",
       "5978  0.182716  0.213168  0.241474  ... -0.017176 -0.011808 -0.000613   \n",
       "5979 -0.392112  1.131950 -0.592724  ... -0.523407  0.207549 -0.355611   \n",
       "5980  0.755301 -0.127350  0.329944  ... -0.028420 -0.098941 -0.130566   \n",
       "5981 -0.882415 -0.106105 -0.051885  ...  0.065957 -0.151601  0.072650   \n",
       "\n",
       "           104       105       106       107       108       109       110  \n",
       "0    -0.050509  0.185363 -0.019726 -0.122237 -0.086713 -0.053711 -0.077012  \n",
       "1     0.116798 -0.031030 -0.049614  0.024517  0.003476  0.001157  0.006372  \n",
       "2    -0.020539 -0.010853 -0.012236 -0.003550 -0.000766  0.007166  0.015099  \n",
       "3    -0.141897 -0.014390  0.095265 -0.023132  0.142257  0.325156  0.171970  \n",
       "4    -0.006011  0.007325 -0.018732  0.004144 -0.010431 -0.004584 -0.006360  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977 -0.002395 -0.009385  0.033274  0.079296  0.230744 -0.209229 -0.247969  \n",
       "5978 -0.003491 -0.025970 -0.001241  0.008654 -0.008849 -0.015761 -0.011166  \n",
       "5979 -0.084467  0.018351  0.011924  0.144681  0.012927  0.108045 -0.044070  \n",
       "5980  0.075247 -0.050180  0.088172 -0.168739 -0.393781  0.228118  0.392946  \n",
       "5981 -0.214053 -0.182360  0.173993  0.166463  0.261078  0.370338  0.349272  \n",
       "\n",
       "[5982 rows x 112 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.436058</td>\n",
       "      <td>-0.040728</td>\n",
       "      <td>-0.376852</td>\n",
       "      <td>0.244498</td>\n",
       "      <td>-0.337669</td>\n",
       "      <td>0.594535</td>\n",
       "      <td>1.160821</td>\n",
       "      <td>-0.507886</td>\n",
       "      <td>-0.026341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371315</td>\n",
       "      <td>-0.820841</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.359050</td>\n",
       "      <td>-0.052447</td>\n",
       "      <td>0.167750</td>\n",
       "      <td>-0.028469</td>\n",
       "      <td>0.045124</td>\n",
       "      <td>0.103598</td>\n",
       "      <td>0.014635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.495079</td>\n",
       "      <td>0.426246</td>\n",
       "      <td>0.603980</td>\n",
       "      <td>0.267780</td>\n",
       "      <td>0.167863</td>\n",
       "      <td>-0.341812</td>\n",
       "      <td>0.074077</td>\n",
       "      <td>0.082130</td>\n",
       "      <td>-0.268278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027723</td>\n",
       "      <td>-0.408299</td>\n",
       "      <td>-0.796369</td>\n",
       "      <td>-0.191770</td>\n",
       "      <td>0.136208</td>\n",
       "      <td>0.175161</td>\n",
       "      <td>-0.027328</td>\n",
       "      <td>0.023678</td>\n",
       "      <td>0.019509</td>\n",
       "      <td>-0.013586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.486196</td>\n",
       "      <td>0.137751</td>\n",
       "      <td>0.094870</td>\n",
       "      <td>0.455594</td>\n",
       "      <td>0.025952</td>\n",
       "      <td>0.268160</td>\n",
       "      <td>0.156926</td>\n",
       "      <td>-0.156983</td>\n",
       "      <td>-0.082030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102234</td>\n",
       "      <td>0.202870</td>\n",
       "      <td>0.137714</td>\n",
       "      <td>0.092741</td>\n",
       "      <td>-0.046748</td>\n",
       "      <td>-0.249884</td>\n",
       "      <td>0.133006</td>\n",
       "      <td>0.062282</td>\n",
       "      <td>-0.045456</td>\n",
       "      <td>-0.040493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.826307</td>\n",
       "      <td>-0.968225</td>\n",
       "      <td>0.253546</td>\n",
       "      <td>0.553680</td>\n",
       "      <td>-0.338130</td>\n",
       "      <td>-0.531447</td>\n",
       "      <td>-0.663852</td>\n",
       "      <td>0.274229</td>\n",
       "      <td>0.311605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.615561</td>\n",
       "      <td>0.226854</td>\n",
       "      <td>0.448593</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>-0.397996</td>\n",
       "      <td>-0.505601</td>\n",
       "      <td>-0.206288</td>\n",
       "      <td>-0.023323</td>\n",
       "      <td>-0.058132</td>\n",
       "      <td>-0.078507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.388948</td>\n",
       "      <td>0.099101</td>\n",
       "      <td>0.054519</td>\n",
       "      <td>-0.485316</td>\n",
       "      <td>-0.493745</td>\n",
       "      <td>0.040495</td>\n",
       "      <td>0.094735</td>\n",
       "      <td>-0.274916</td>\n",
       "      <td>-0.333733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094130</td>\n",
       "      <td>0.096581</td>\n",
       "      <td>-0.122793</td>\n",
       "      <td>0.078391</td>\n",
       "      <td>-0.059339</td>\n",
       "      <td>-0.210875</td>\n",
       "      <td>0.119112</td>\n",
       "      <td>0.095087</td>\n",
       "      <td>-0.033616</td>\n",
       "      <td>0.010124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-0.758481</td>\n",
       "      <td>0.812565</td>\n",
       "      <td>-0.108292</td>\n",
       "      <td>0.172876</td>\n",
       "      <td>0.196507</td>\n",
       "      <td>-0.143411</td>\n",
       "      <td>0.183231</td>\n",
       "      <td>0.159825</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.780837</td>\n",
       "      <td>-0.074323</td>\n",
       "      <td>-0.534725</td>\n",
       "      <td>-0.266745</td>\n",
       "      <td>-0.442471</td>\n",
       "      <td>-0.236793</td>\n",
       "      <td>-0.081901</td>\n",
       "      <td>0.159949</td>\n",
       "      <td>-0.030645</td>\n",
       "      <td>0.010190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.564872</td>\n",
       "      <td>0.227113</td>\n",
       "      <td>0.097719</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>-0.014250</td>\n",
       "      <td>0.254983</td>\n",
       "      <td>-0.491875</td>\n",
       "      <td>-0.342264</td>\n",
       "      <td>0.369359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097276</td>\n",
       "      <td>-0.100308</td>\n",
       "      <td>0.012817</td>\n",
       "      <td>0.030391</td>\n",
       "      <td>0.023927</td>\n",
       "      <td>0.100438</td>\n",
       "      <td>-0.032773</td>\n",
       "      <td>-0.034280</td>\n",
       "      <td>-0.015645</td>\n",
       "      <td>0.006128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>1.010739</td>\n",
       "      <td>1.482501</td>\n",
       "      <td>1.119399</td>\n",
       "      <td>0.718032</td>\n",
       "      <td>-0.021002</td>\n",
       "      <td>0.340607</td>\n",
       "      <td>-0.337983</td>\n",
       "      <td>0.478843</td>\n",
       "      <td>0.405145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111127</td>\n",
       "      <td>0.135636</td>\n",
       "      <td>-0.055767</td>\n",
       "      <td>-0.729023</td>\n",
       "      <td>-0.262131</td>\n",
       "      <td>0.856770</td>\n",
       "      <td>0.272822</td>\n",
       "      <td>0.424178</td>\n",
       "      <td>-0.637153</td>\n",
       "      <td>-0.439690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.907570</td>\n",
       "      <td>0.014547</td>\n",
       "      <td>0.360717</td>\n",
       "      <td>0.103525</td>\n",
       "      <td>0.415867</td>\n",
       "      <td>-0.849732</td>\n",
       "      <td>0.382268</td>\n",
       "      <td>-0.345673</td>\n",
       "      <td>0.521682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.653179</td>\n",
       "      <td>0.014776</td>\n",
       "      <td>0.284553</td>\n",
       "      <td>-0.030792</td>\n",
       "      <td>-0.207601</td>\n",
       "      <td>-0.053542</td>\n",
       "      <td>-0.043660</td>\n",
       "      <td>-0.118073</td>\n",
       "      <td>-0.046550</td>\n",
       "      <td>-0.100628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.916461</td>\n",
       "      <td>-0.791022</td>\n",
       "      <td>0.136646</td>\n",
       "      <td>0.219709</td>\n",
       "      <td>-0.125223</td>\n",
       "      <td>0.073441</td>\n",
       "      <td>-0.344049</td>\n",
       "      <td>0.796474</td>\n",
       "      <td>-0.018890</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057549</td>\n",
       "      <td>-0.350444</td>\n",
       "      <td>-0.070858</td>\n",
       "      <td>0.011655</td>\n",
       "      <td>-0.310068</td>\n",
       "      <td>-0.501102</td>\n",
       "      <td>0.272890</td>\n",
       "      <td>0.247785</td>\n",
       "      <td>-0.007448</td>\n",
       "      <td>0.010920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  1.436058 -0.040728 -0.376852  0.244498 -0.337669  0.594535   \n",
       "1           1 -1.495079  0.426246  0.603980  0.267780  0.167863 -0.341812   \n",
       "2           2 -1.486196  0.137751  0.094870  0.455594  0.025952  0.268160   \n",
       "3           3  0.826307 -0.968225  0.253546  0.553680 -0.338130 -0.531447   \n",
       "4           4 -1.388948  0.099101  0.054519 -0.485316 -0.493745  0.040495   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -0.758481  0.812565 -0.108292  0.172876  0.196507 -0.143411   \n",
       "5978     5978 -1.564872  0.227113  0.097719  0.005096 -0.014250  0.254983   \n",
       "5979     5979  1.010739  1.482501  1.119399  0.718032 -0.021002  0.340607   \n",
       "5980     5980 -0.907570  0.014547  0.360717  0.103525  0.415867 -0.849732   \n",
       "5981     5981 -0.916461 -0.791022  0.136646  0.219709 -0.125223  0.073441   \n",
       "\n",
       "             6         7         8  ...        26        27        28  \\\n",
       "0     1.160821 -0.507886 -0.026341  ...  0.371315 -0.820841  0.280656   \n",
       "1     0.074077  0.082130 -0.268278  ... -0.027723 -0.408299 -0.796369   \n",
       "2     0.156926 -0.156983 -0.082030  ...  0.102234  0.202870  0.137714   \n",
       "3    -0.663852  0.274229  0.311605  ... -0.615561  0.226854  0.448593   \n",
       "4     0.094735 -0.274916 -0.333733  ...  0.094130  0.096581 -0.122793   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977  0.183231  0.159825 -0.234539  ... -0.780837 -0.074323 -0.534725   \n",
       "5978 -0.491875 -0.342264  0.369359  ... -0.097276 -0.100308  0.012817   \n",
       "5979 -0.337983  0.478843  0.405145  ...  0.111127  0.135636 -0.055767   \n",
       "5980  0.382268 -0.345673  0.521682  ... -0.653179  0.014776  0.284553   \n",
       "5981 -0.344049  0.796474 -0.018890  ... -0.057549 -0.350444 -0.070858   \n",
       "\n",
       "            29        30        31        32        33        34        35  \n",
       "0     0.359050 -0.052447  0.167750 -0.028469  0.045124  0.103598  0.014635  \n",
       "1    -0.191770  0.136208  0.175161 -0.027328  0.023678  0.019509 -0.013586  \n",
       "2     0.092741 -0.046748 -0.249884  0.133006  0.062282 -0.045456 -0.040493  \n",
       "3     0.001418 -0.397996 -0.505601 -0.206288 -0.023323 -0.058132 -0.078507  \n",
       "4     0.078391 -0.059339 -0.210875  0.119112  0.095087 -0.033616  0.010124  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977 -0.266745 -0.442471 -0.236793 -0.081901  0.159949 -0.030645  0.010190  \n",
       "5978  0.030391  0.023927  0.100438 -0.032773 -0.034280 -0.015645  0.006128  \n",
       "5979 -0.729023 -0.262131  0.856770  0.272822  0.424178 -0.637153 -0.439690  \n",
       "5980 -0.030792 -0.207601 -0.053542 -0.043660 -0.118073 -0.046550 -0.100628  \n",
       "5981  0.011655 -0.310068 -0.501102  0.272890  0.247785 -0.007448  0.010920  \n",
       "\n",
       "[5982 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                           aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train_nm = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test_nm = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_mclas_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                           aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train_mclas = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test_mclas = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "### 중분류 구매건수 nm과 대분류 구매건수 mclas를 합친다\n",
    "\n",
    "X_train = pd.concat([X_train_nm, X_train_mclas], axis=1)\n",
    "X_test = pd.concat([X_test_nm, X_test_mclas], axis=1)\n",
    "\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:04<00:18,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7460166458159577 SEED: 3405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:09<00:14,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7483971017514596 SEED: 9450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:13<00:09,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.744726508201279 SEED: 1922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:19<00:04,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7509295941061996 SEED: 3863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:23<00:00,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7482798165137614 SEED: 533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 중분류 구매건수 & 대분류 구매여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.625380</td>\n",
       "      <td>0.034396</td>\n",
       "      <td>-0.672246</td>\n",
       "      <td>0.441779</td>\n",
       "      <td>0.159294</td>\n",
       "      <td>-0.598718</td>\n",
       "      <td>0.325886</td>\n",
       "      <td>-0.936302</td>\n",
       "      <td>0.305429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143142</td>\n",
       "      <td>0.118564</td>\n",
       "      <td>-0.002334</td>\n",
       "      <td>0.089901</td>\n",
       "      <td>-0.111054</td>\n",
       "      <td>-0.021497</td>\n",
       "      <td>0.140365</td>\n",
       "      <td>0.117912</td>\n",
       "      <td>-0.126485</td>\n",
       "      <td>0.046712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.107635</td>\n",
       "      <td>-0.008147</td>\n",
       "      <td>-0.036582</td>\n",
       "      <td>-0.227227</td>\n",
       "      <td>-0.173067</td>\n",
       "      <td>0.257546</td>\n",
       "      <td>0.036107</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>-0.073846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030561</td>\n",
       "      <td>-0.018014</td>\n",
       "      <td>0.032755</td>\n",
       "      <td>0.015114</td>\n",
       "      <td>0.011464</td>\n",
       "      <td>-0.002095</td>\n",
       "      <td>-0.073120</td>\n",
       "      <td>-0.051948</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>-0.007939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.099033</td>\n",
       "      <td>-0.109059</td>\n",
       "      <td>-0.020829</td>\n",
       "      <td>0.007957</td>\n",
       "      <td>-0.059710</td>\n",
       "      <td>0.090729</td>\n",
       "      <td>0.179059</td>\n",
       "      <td>-0.068382</td>\n",
       "      <td>0.062943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.035945</td>\n",
       "      <td>-0.017813</td>\n",
       "      <td>-0.008323</td>\n",
       "      <td>-0.036238</td>\n",
       "      <td>0.021623</td>\n",
       "      <td>0.038809</td>\n",
       "      <td>0.046284</td>\n",
       "      <td>0.007039</td>\n",
       "      <td>0.014445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.265591</td>\n",
       "      <td>-0.619277</td>\n",
       "      <td>-0.279149</td>\n",
       "      <td>-0.187955</td>\n",
       "      <td>0.023135</td>\n",
       "      <td>-0.135279</td>\n",
       "      <td>-0.067050</td>\n",
       "      <td>0.675716</td>\n",
       "      <td>-0.159730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131376</td>\n",
       "      <td>0.064030</td>\n",
       "      <td>-0.118500</td>\n",
       "      <td>0.066814</td>\n",
       "      <td>-0.149491</td>\n",
       "      <td>-0.003301</td>\n",
       "      <td>0.486243</td>\n",
       "      <td>-0.482798</td>\n",
       "      <td>-0.028555</td>\n",
       "      <td>-0.156546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.141518</td>\n",
       "      <td>-0.191579</td>\n",
       "      <td>0.005533</td>\n",
       "      <td>-0.059629</td>\n",
       "      <td>0.026487</td>\n",
       "      <td>0.013643</td>\n",
       "      <td>-0.095080</td>\n",
       "      <td>0.043151</td>\n",
       "      <td>-0.057431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008822</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>-0.005974</td>\n",
       "      <td>-0.006765</td>\n",
       "      <td>0.007707</td>\n",
       "      <td>0.013415</td>\n",
       "      <td>-0.002152</td>\n",
       "      <td>0.008844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-0.877498</td>\n",
       "      <td>0.236507</td>\n",
       "      <td>-0.127787</td>\n",
       "      <td>-0.066931</td>\n",
       "      <td>-0.135133</td>\n",
       "      <td>0.047662</td>\n",
       "      <td>0.136196</td>\n",
       "      <td>-0.051518</td>\n",
       "      <td>-0.006189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100014</td>\n",
       "      <td>-0.477439</td>\n",
       "      <td>-0.028659</td>\n",
       "      <td>0.204758</td>\n",
       "      <td>-0.015639</td>\n",
       "      <td>0.083038</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>-0.005576</td>\n",
       "      <td>-0.006840</td>\n",
       "      <td>0.081172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.162111</td>\n",
       "      <td>-0.159106</td>\n",
       "      <td>0.019576</td>\n",
       "      <td>0.078777</td>\n",
       "      <td>0.026611</td>\n",
       "      <td>-0.007421</td>\n",
       "      <td>-0.020261</td>\n",
       "      <td>0.025530</td>\n",
       "      <td>-0.021468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005455</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>0.011396</td>\n",
       "      <td>0.006313</td>\n",
       "      <td>-0.050464</td>\n",
       "      <td>-0.032066</td>\n",
       "      <td>-0.036101</td>\n",
       "      <td>-0.007855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>0.199450</td>\n",
       "      <td>1.781264</td>\n",
       "      <td>-0.540263</td>\n",
       "      <td>0.071173</td>\n",
       "      <td>0.331206</td>\n",
       "      <td>1.250888</td>\n",
       "      <td>0.351207</td>\n",
       "      <td>-0.141133</td>\n",
       "      <td>-0.117322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100453</td>\n",
       "      <td>0.028486</td>\n",
       "      <td>-0.128008</td>\n",
       "      <td>0.088439</td>\n",
       "      <td>-0.112445</td>\n",
       "      <td>-0.079405</td>\n",
       "      <td>0.109667</td>\n",
       "      <td>0.038992</td>\n",
       "      <td>-0.088774</td>\n",
       "      <td>0.054421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.635700</td>\n",
       "      <td>-0.227858</td>\n",
       "      <td>-0.237216</td>\n",
       "      <td>0.009492</td>\n",
       "      <td>-0.166560</td>\n",
       "      <td>-0.074526</td>\n",
       "      <td>0.129139</td>\n",
       "      <td>-0.274902</td>\n",
       "      <td>-0.380003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.285796</td>\n",
       "      <td>0.651523</td>\n",
       "      <td>0.113559</td>\n",
       "      <td>-0.241631</td>\n",
       "      <td>0.065565</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>-0.040256</td>\n",
       "      <td>-0.085648</td>\n",
       "      <td>-0.044685</td>\n",
       "      <td>-0.053650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.674396</td>\n",
       "      <td>-0.398007</td>\n",
       "      <td>0.607995</td>\n",
       "      <td>-0.003883</td>\n",
       "      <td>-0.024185</td>\n",
       "      <td>0.051853</td>\n",
       "      <td>0.060919</td>\n",
       "      <td>0.352332</td>\n",
       "      <td>0.488067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256775</td>\n",
       "      <td>0.148656</td>\n",
       "      <td>-0.043532</td>\n",
       "      <td>-0.055591</td>\n",
       "      <td>-0.132109</td>\n",
       "      <td>-0.018838</td>\n",
       "      <td>0.036651</td>\n",
       "      <td>0.041369</td>\n",
       "      <td>0.035976</td>\n",
       "      <td>-0.103166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  0.625380  0.034396 -0.672246  0.441779  0.159294 -0.598718   \n",
       "1           1 -1.107635 -0.008147 -0.036582 -0.227227 -0.173067  0.257546   \n",
       "2           2 -1.099033 -0.109059 -0.020829  0.007957 -0.059710  0.090729   \n",
       "3           3  0.265591 -0.619277 -0.279149 -0.187955  0.023135 -0.135279   \n",
       "4           4 -1.141518 -0.191579  0.005533 -0.059629  0.026487  0.013643   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -0.877498  0.236507 -0.127787 -0.066931 -0.135133  0.047662   \n",
       "5978     5978 -1.162111 -0.159106  0.019576  0.078777  0.026611 -0.007421   \n",
       "5979     5979  0.199450  1.781264 -0.540263  0.071173  0.331206  1.250888   \n",
       "5980     5980 -0.635700 -0.227858 -0.237216  0.009492 -0.166560 -0.074526   \n",
       "5981     5981 -0.674396 -0.398007  0.607995 -0.003883 -0.024185  0.051853   \n",
       "\n",
       "             6         7         8  ...       100       101       102  \\\n",
       "0     0.325886 -0.936302  0.305429  ... -0.143142  0.118564 -0.002334   \n",
       "1     0.036107 -0.059078 -0.073846  ...  0.030561 -0.018014  0.032755   \n",
       "2     0.179059 -0.068382  0.062943  ...  0.002262  0.035945 -0.017813   \n",
       "3    -0.067050  0.675716 -0.159730  ...  0.131376  0.064030 -0.118500   \n",
       "4    -0.095080  0.043151 -0.057431  ...  0.008822  0.004029  0.004533   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977  0.136196 -0.051518 -0.006189  ...  0.100014 -0.477439 -0.028659   \n",
       "5978 -0.020261  0.025530 -0.021468  ...  0.005455  0.005500  0.013200   \n",
       "5979  0.351207 -0.141133 -0.117322  ...  0.100453  0.028486 -0.128008   \n",
       "5980  0.129139 -0.274902 -0.380003  ... -0.285796  0.651523  0.113559   \n",
       "5981  0.060919  0.352332  0.488067  ...  0.256775  0.148656 -0.043532   \n",
       "\n",
       "           103       104       105       106       107       108       109  \n",
       "0     0.089901 -0.111054 -0.021497  0.140365  0.117912 -0.126485  0.046712  \n",
       "1     0.015114  0.011464 -0.002095 -0.073120 -0.051948  0.005610 -0.007939  \n",
       "2    -0.008323 -0.036238  0.021623  0.038809  0.046284  0.007039  0.014445  \n",
       "3     0.066814 -0.149491 -0.003301  0.486243 -0.482798 -0.028555 -0.156546  \n",
       "4    -0.000862 -0.005974 -0.006765  0.007707  0.013415 -0.002152  0.008844  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977  0.204758 -0.015639  0.083038  0.010453 -0.005576 -0.006840  0.081172  \n",
       "5978  0.006884  0.011396  0.006313 -0.050464 -0.032066 -0.036101 -0.007855  \n",
       "5979  0.088439 -0.112445 -0.079405  0.109667  0.038992 -0.088774  0.054421  \n",
       "5980 -0.241631  0.065565  0.000818 -0.040256 -0.085648 -0.044685 -0.053650  \n",
       "5981 -0.055591 -0.132109 -0.018838  0.036651  0.041369  0.035976 -0.103166  \n",
       "\n",
       "[5982 rows x 111 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.436058</td>\n",
       "      <td>-0.040728</td>\n",
       "      <td>-0.376852</td>\n",
       "      <td>0.244498</td>\n",
       "      <td>-0.337669</td>\n",
       "      <td>0.594535</td>\n",
       "      <td>1.160821</td>\n",
       "      <td>-0.507886</td>\n",
       "      <td>-0.026341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371315</td>\n",
       "      <td>-0.820841</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.359050</td>\n",
       "      <td>-0.052447</td>\n",
       "      <td>0.167750</td>\n",
       "      <td>-0.028469</td>\n",
       "      <td>0.045124</td>\n",
       "      <td>0.103598</td>\n",
       "      <td>0.014635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.495079</td>\n",
       "      <td>0.426246</td>\n",
       "      <td>0.603980</td>\n",
       "      <td>0.267780</td>\n",
       "      <td>0.167863</td>\n",
       "      <td>-0.341812</td>\n",
       "      <td>0.074077</td>\n",
       "      <td>0.082130</td>\n",
       "      <td>-0.268278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027723</td>\n",
       "      <td>-0.408299</td>\n",
       "      <td>-0.796369</td>\n",
       "      <td>-0.191770</td>\n",
       "      <td>0.136208</td>\n",
       "      <td>0.175161</td>\n",
       "      <td>-0.027328</td>\n",
       "      <td>0.023678</td>\n",
       "      <td>0.019509</td>\n",
       "      <td>-0.013586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.486196</td>\n",
       "      <td>0.137751</td>\n",
       "      <td>0.094870</td>\n",
       "      <td>0.455594</td>\n",
       "      <td>0.025952</td>\n",
       "      <td>0.268160</td>\n",
       "      <td>0.156926</td>\n",
       "      <td>-0.156983</td>\n",
       "      <td>-0.082030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102234</td>\n",
       "      <td>0.202870</td>\n",
       "      <td>0.137714</td>\n",
       "      <td>0.092741</td>\n",
       "      <td>-0.046748</td>\n",
       "      <td>-0.249884</td>\n",
       "      <td>0.133006</td>\n",
       "      <td>0.062282</td>\n",
       "      <td>-0.045456</td>\n",
       "      <td>-0.040493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.826307</td>\n",
       "      <td>-0.968225</td>\n",
       "      <td>0.253546</td>\n",
       "      <td>0.553680</td>\n",
       "      <td>-0.338130</td>\n",
       "      <td>-0.531447</td>\n",
       "      <td>-0.663852</td>\n",
       "      <td>0.274229</td>\n",
       "      <td>0.311605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.615561</td>\n",
       "      <td>0.226854</td>\n",
       "      <td>0.448593</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>-0.397996</td>\n",
       "      <td>-0.505601</td>\n",
       "      <td>-0.206288</td>\n",
       "      <td>-0.023323</td>\n",
       "      <td>-0.058132</td>\n",
       "      <td>-0.078507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.388948</td>\n",
       "      <td>0.099101</td>\n",
       "      <td>0.054519</td>\n",
       "      <td>-0.485316</td>\n",
       "      <td>-0.493745</td>\n",
       "      <td>0.040495</td>\n",
       "      <td>0.094735</td>\n",
       "      <td>-0.274916</td>\n",
       "      <td>-0.333733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094130</td>\n",
       "      <td>0.096581</td>\n",
       "      <td>-0.122793</td>\n",
       "      <td>0.078391</td>\n",
       "      <td>-0.059339</td>\n",
       "      <td>-0.210875</td>\n",
       "      <td>0.119112</td>\n",
       "      <td>0.095087</td>\n",
       "      <td>-0.033616</td>\n",
       "      <td>0.010124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-0.758481</td>\n",
       "      <td>0.812565</td>\n",
       "      <td>-0.108292</td>\n",
       "      <td>0.172876</td>\n",
       "      <td>0.196507</td>\n",
       "      <td>-0.143411</td>\n",
       "      <td>0.183231</td>\n",
       "      <td>0.159825</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.780837</td>\n",
       "      <td>-0.074323</td>\n",
       "      <td>-0.534725</td>\n",
       "      <td>-0.266745</td>\n",
       "      <td>-0.442471</td>\n",
       "      <td>-0.236793</td>\n",
       "      <td>-0.081901</td>\n",
       "      <td>0.159949</td>\n",
       "      <td>-0.030645</td>\n",
       "      <td>0.010190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.564872</td>\n",
       "      <td>0.227113</td>\n",
       "      <td>0.097719</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>-0.014250</td>\n",
       "      <td>0.254983</td>\n",
       "      <td>-0.491875</td>\n",
       "      <td>-0.342264</td>\n",
       "      <td>0.369359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097276</td>\n",
       "      <td>-0.100308</td>\n",
       "      <td>0.012817</td>\n",
       "      <td>0.030391</td>\n",
       "      <td>0.023927</td>\n",
       "      <td>0.100438</td>\n",
       "      <td>-0.032773</td>\n",
       "      <td>-0.034280</td>\n",
       "      <td>-0.015645</td>\n",
       "      <td>0.006128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>1.010739</td>\n",
       "      <td>1.482501</td>\n",
       "      <td>1.119399</td>\n",
       "      <td>0.718032</td>\n",
       "      <td>-0.021002</td>\n",
       "      <td>0.340607</td>\n",
       "      <td>-0.337983</td>\n",
       "      <td>0.478843</td>\n",
       "      <td>0.405145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111127</td>\n",
       "      <td>0.135636</td>\n",
       "      <td>-0.055767</td>\n",
       "      <td>-0.729023</td>\n",
       "      <td>-0.262131</td>\n",
       "      <td>0.856770</td>\n",
       "      <td>0.272822</td>\n",
       "      <td>0.424178</td>\n",
       "      <td>-0.637153</td>\n",
       "      <td>-0.439690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.907570</td>\n",
       "      <td>0.014547</td>\n",
       "      <td>0.360717</td>\n",
       "      <td>0.103525</td>\n",
       "      <td>0.415867</td>\n",
       "      <td>-0.849732</td>\n",
       "      <td>0.382268</td>\n",
       "      <td>-0.345673</td>\n",
       "      <td>0.521682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.653179</td>\n",
       "      <td>0.014776</td>\n",
       "      <td>0.284553</td>\n",
       "      <td>-0.030792</td>\n",
       "      <td>-0.207601</td>\n",
       "      <td>-0.053542</td>\n",
       "      <td>-0.043660</td>\n",
       "      <td>-0.118073</td>\n",
       "      <td>-0.046550</td>\n",
       "      <td>-0.100628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.916461</td>\n",
       "      <td>-0.791022</td>\n",
       "      <td>0.136646</td>\n",
       "      <td>0.219709</td>\n",
       "      <td>-0.125223</td>\n",
       "      <td>0.073441</td>\n",
       "      <td>-0.344049</td>\n",
       "      <td>0.796474</td>\n",
       "      <td>-0.018890</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057549</td>\n",
       "      <td>-0.350444</td>\n",
       "      <td>-0.070858</td>\n",
       "      <td>0.011655</td>\n",
       "      <td>-0.310068</td>\n",
       "      <td>-0.501102</td>\n",
       "      <td>0.272890</td>\n",
       "      <td>0.247785</td>\n",
       "      <td>-0.007448</td>\n",
       "      <td>0.010920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  1.436058 -0.040728 -0.376852  0.244498 -0.337669  0.594535   \n",
       "1           1 -1.495079  0.426246  0.603980  0.267780  0.167863 -0.341812   \n",
       "2           2 -1.486196  0.137751  0.094870  0.455594  0.025952  0.268160   \n",
       "3           3  0.826307 -0.968225  0.253546  0.553680 -0.338130 -0.531447   \n",
       "4           4 -1.388948  0.099101  0.054519 -0.485316 -0.493745  0.040495   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -0.758481  0.812565 -0.108292  0.172876  0.196507 -0.143411   \n",
       "5978     5978 -1.564872  0.227113  0.097719  0.005096 -0.014250  0.254983   \n",
       "5979     5979  1.010739  1.482501  1.119399  0.718032 -0.021002  0.340607   \n",
       "5980     5980 -0.907570  0.014547  0.360717  0.103525  0.415867 -0.849732   \n",
       "5981     5981 -0.916461 -0.791022  0.136646  0.219709 -0.125223  0.073441   \n",
       "\n",
       "             6         7         8  ...        26        27        28  \\\n",
       "0     1.160821 -0.507886 -0.026341  ...  0.371315 -0.820841  0.280656   \n",
       "1     0.074077  0.082130 -0.268278  ... -0.027723 -0.408299 -0.796369   \n",
       "2     0.156926 -0.156983 -0.082030  ...  0.102234  0.202870  0.137714   \n",
       "3    -0.663852  0.274229  0.311605  ... -0.615561  0.226854  0.448593   \n",
       "4     0.094735 -0.274916 -0.333733  ...  0.094130  0.096581 -0.122793   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977  0.183231  0.159825 -0.234539  ... -0.780837 -0.074323 -0.534725   \n",
       "5978 -0.491875 -0.342264  0.369359  ... -0.097276 -0.100308  0.012817   \n",
       "5979 -0.337983  0.478843  0.405145  ...  0.111127  0.135636 -0.055767   \n",
       "5980  0.382268 -0.345673  0.521682  ... -0.653179  0.014776  0.284553   \n",
       "5981 -0.344049  0.796474 -0.018890  ... -0.057549 -0.350444 -0.070858   \n",
       "\n",
       "            29        30        31        32        33        34        35  \n",
       "0     0.359050 -0.052447  0.167750 -0.028469  0.045124  0.103598  0.014635  \n",
       "1    -0.191770  0.136208  0.175161 -0.027328  0.023678  0.019509 -0.013586  \n",
       "2     0.092741 -0.046748 -0.249884  0.133006  0.062282 -0.045456 -0.040493  \n",
       "3     0.001418 -0.397996 -0.505601 -0.206288 -0.023323 -0.058132 -0.078507  \n",
       "4     0.078391 -0.059339 -0.210875  0.119112  0.095087 -0.033616  0.010124  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977 -0.266745 -0.442471 -0.236793 -0.081901  0.159949 -0.030645  0.010190  \n",
       "5978  0.030391  0.023927  0.100438 -0.032773 -0.034280 -0.015645  0.006128  \n",
       "5979 -0.729023 -0.262131  0.856770  0.272822  0.424178 -0.637153 -0.439690  \n",
       "5980 -0.030792 -0.207601 -0.053542 -0.043660 -0.118073 -0.046550 -0.100628  \n",
       "5981  0.011655 -0.310068 -0.501102  0.272890  0.247785 -0.007448  0.010920  \n",
       "\n",
       "[5982 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                            aggfunc=lambda x: len(x), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train_nm = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test_nm = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_mclas_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                           aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train_mclas = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test_mclas = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "### 중분류 구매건수 nm과 대분류 구매건수 mclas를 합친다\n",
    "\n",
    "X_train = pd.concat([X_train_nm, X_train_mclas], axis=1)\n",
    "X_test = pd.concat([X_test_nm, X_test_mclas], axis=1)\n",
    "\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:05<00:20,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7572890603280512 SEED: 9179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:10<00:15,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7502953850430916 SEED: 6916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:15<00:10,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7507037114261884 SEED: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:21<00:05,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7563942174033917 SEED: 4366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:26<00:00,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7567677926049485 SEED: 230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 대분류 구매건수 & 중분류 구매여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.031835</td>\n",
       "      <td>0.212301</td>\n",
       "      <td>-0.922307</td>\n",
       "      <td>-0.869803</td>\n",
       "      <td>-0.305008</td>\n",
       "      <td>0.092058</td>\n",
       "      <td>0.831769</td>\n",
       "      <td>-0.575745</td>\n",
       "      <td>-0.703853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102421</td>\n",
       "      <td>-0.318619</td>\n",
       "      <td>-0.375416</td>\n",
       "      <td>-0.050509</td>\n",
       "      <td>0.185363</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>-0.122237</td>\n",
       "      <td>-0.086713</td>\n",
       "      <td>-0.053711</td>\n",
       "      <td>-0.077012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.545397</td>\n",
       "      <td>-0.120407</td>\n",
       "      <td>0.028675</td>\n",
       "      <td>0.202452</td>\n",
       "      <td>0.264835</td>\n",
       "      <td>0.240070</td>\n",
       "      <td>0.129344</td>\n",
       "      <td>0.058812</td>\n",
       "      <td>0.209729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028266</td>\n",
       "      <td>0.074902</td>\n",
       "      <td>0.029886</td>\n",
       "      <td>0.116798</td>\n",
       "      <td>-0.031030</td>\n",
       "      <td>-0.049614</td>\n",
       "      <td>0.024517</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.006372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.415123</td>\n",
       "      <td>-0.065534</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>-0.016653</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0.400511</td>\n",
       "      <td>-0.053878</td>\n",
       "      <td>-0.125965</td>\n",
       "      <td>-0.270389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>-0.011082</td>\n",
       "      <td>-0.020539</td>\n",
       "      <td>-0.010853</td>\n",
       "      <td>-0.012236</td>\n",
       "      <td>-0.003550</td>\n",
       "      <td>-0.000766</td>\n",
       "      <td>0.007166</td>\n",
       "      <td>0.015099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.865498</td>\n",
       "      <td>-1.222110</td>\n",
       "      <td>-0.553205</td>\n",
       "      <td>0.022980</td>\n",
       "      <td>0.309707</td>\n",
       "      <td>-0.115284</td>\n",
       "      <td>-0.423851</td>\n",
       "      <td>0.164560</td>\n",
       "      <td>0.401825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122099</td>\n",
       "      <td>0.050478</td>\n",
       "      <td>0.119724</td>\n",
       "      <td>-0.141897</td>\n",
       "      <td>-0.014390</td>\n",
       "      <td>0.095265</td>\n",
       "      <td>-0.023132</td>\n",
       "      <td>0.142257</td>\n",
       "      <td>0.325156</td>\n",
       "      <td>0.171970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.341069</td>\n",
       "      <td>-0.078229</td>\n",
       "      <td>-0.045112</td>\n",
       "      <td>0.219325</td>\n",
       "      <td>-0.030194</td>\n",
       "      <td>-0.500378</td>\n",
       "      <td>0.017921</td>\n",
       "      <td>0.145940</td>\n",
       "      <td>-0.016312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012135</td>\n",
       "      <td>0.031032</td>\n",
       "      <td>-0.017324</td>\n",
       "      <td>-0.006011</td>\n",
       "      <td>0.007325</td>\n",
       "      <td>-0.018732</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>-0.010431</td>\n",
       "      <td>-0.004584</td>\n",
       "      <td>-0.006360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-1.161452</td>\n",
       "      <td>0.308622</td>\n",
       "      <td>-0.139144</td>\n",
       "      <td>-0.117040</td>\n",
       "      <td>0.072620</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>-0.058142</td>\n",
       "      <td>-0.208458</td>\n",
       "      <td>-0.120671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004387</td>\n",
       "      <td>-0.135064</td>\n",
       "      <td>0.031170</td>\n",
       "      <td>-0.002395</td>\n",
       "      <td>-0.009385</td>\n",
       "      <td>0.033274</td>\n",
       "      <td>0.079296</td>\n",
       "      <td>0.230744</td>\n",
       "      <td>-0.209229</td>\n",
       "      <td>-0.247969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.547982</td>\n",
       "      <td>-0.166368</td>\n",
       "      <td>0.062790</td>\n",
       "      <td>-0.058637</td>\n",
       "      <td>-0.039730</td>\n",
       "      <td>0.145217</td>\n",
       "      <td>0.182716</td>\n",
       "      <td>0.213168</td>\n",
       "      <td>0.241474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017176</td>\n",
       "      <td>-0.011808</td>\n",
       "      <td>-0.000613</td>\n",
       "      <td>-0.003491</td>\n",
       "      <td>-0.025970</td>\n",
       "      <td>-0.001241</td>\n",
       "      <td>0.008654</td>\n",
       "      <td>-0.008849</td>\n",
       "      <td>-0.015761</td>\n",
       "      <td>-0.011166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>0.204833</td>\n",
       "      <td>1.663809</td>\n",
       "      <td>-0.728419</td>\n",
       "      <td>0.605944</td>\n",
       "      <td>0.811258</td>\n",
       "      <td>1.091609</td>\n",
       "      <td>-0.392112</td>\n",
       "      <td>1.131950</td>\n",
       "      <td>-0.592724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523407</td>\n",
       "      <td>0.207549</td>\n",
       "      <td>-0.355611</td>\n",
       "      <td>-0.084467</td>\n",
       "      <td>0.018351</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.144681</td>\n",
       "      <td>0.012927</td>\n",
       "      <td>0.108045</td>\n",
       "      <td>-0.044070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.925491</td>\n",
       "      <td>-0.298534</td>\n",
       "      <td>-0.188338</td>\n",
       "      <td>-0.024190</td>\n",
       "      <td>0.231828</td>\n",
       "      <td>0.287504</td>\n",
       "      <td>0.755301</td>\n",
       "      <td>-0.127350</td>\n",
       "      <td>0.329944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028420</td>\n",
       "      <td>-0.098941</td>\n",
       "      <td>-0.130566</td>\n",
       "      <td>0.075247</td>\n",
       "      <td>-0.050180</td>\n",
       "      <td>0.088172</td>\n",
       "      <td>-0.168739</td>\n",
       "      <td>-0.393781</td>\n",
       "      <td>0.228118</td>\n",
       "      <td>0.392946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.512350</td>\n",
       "      <td>-0.620099</td>\n",
       "      <td>0.938312</td>\n",
       "      <td>-0.100132</td>\n",
       "      <td>-0.191828</td>\n",
       "      <td>0.116272</td>\n",
       "      <td>-0.882415</td>\n",
       "      <td>-0.106105</td>\n",
       "      <td>-0.051885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065957</td>\n",
       "      <td>-0.151601</td>\n",
       "      <td>0.072650</td>\n",
       "      <td>-0.214053</td>\n",
       "      <td>-0.182360</td>\n",
       "      <td>0.173993</td>\n",
       "      <td>0.166463</td>\n",
       "      <td>0.261078</td>\n",
       "      <td>0.370338</td>\n",
       "      <td>0.349272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  1.031835  0.212301 -0.922307 -0.869803 -0.305008  0.092058   \n",
       "1           1 -1.545397 -0.120407  0.028675  0.202452  0.264835  0.240070   \n",
       "2           2 -1.415123 -0.065534  0.005330 -0.016653  0.142608  0.400511   \n",
       "3           3  0.865498 -1.222110 -0.553205  0.022980  0.309707 -0.115284   \n",
       "4           4 -1.341069 -0.078229 -0.045112  0.219325 -0.030194 -0.500378   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -1.161452  0.308622 -0.139144 -0.117040  0.072620  0.152300   \n",
       "5978     5978 -1.547982 -0.166368  0.062790 -0.058637 -0.039730  0.145217   \n",
       "5979     5979  0.204833  1.663809 -0.728419  0.605944  0.811258  1.091609   \n",
       "5980     5980 -0.925491 -0.298534 -0.188338 -0.024190  0.231828  0.287504   \n",
       "5981     5981 -0.512350 -0.620099  0.938312 -0.100132 -0.191828  0.116272   \n",
       "\n",
       "             6         7         8  ...       101       102       103  \\\n",
       "0     0.831769 -0.575745 -0.703853  ...  0.102421 -0.318619 -0.375416   \n",
       "1     0.129344  0.058812  0.209729  ...  0.028266  0.074902  0.029886   \n",
       "2    -0.053878 -0.125965 -0.270389  ...  0.018800  0.036842 -0.011082   \n",
       "3    -0.423851  0.164560  0.401825  ...  0.122099  0.050478  0.119724   \n",
       "4     0.017921  0.145940 -0.016312  ...  0.012135  0.031032 -0.017324   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977 -0.058142 -0.208458 -0.120671  ...  0.004387 -0.135064  0.031170   \n",
       "5978  0.182716  0.213168  0.241474  ... -0.017176 -0.011808 -0.000613   \n",
       "5979 -0.392112  1.131950 -0.592724  ... -0.523407  0.207549 -0.355611   \n",
       "5980  0.755301 -0.127350  0.329944  ... -0.028420 -0.098941 -0.130566   \n",
       "5981 -0.882415 -0.106105 -0.051885  ...  0.065957 -0.151601  0.072650   \n",
       "\n",
       "           104       105       106       107       108       109       110  \n",
       "0    -0.050509  0.185363 -0.019726 -0.122237 -0.086713 -0.053711 -0.077012  \n",
       "1     0.116798 -0.031030 -0.049614  0.024517  0.003476  0.001157  0.006372  \n",
       "2    -0.020539 -0.010853 -0.012236 -0.003550 -0.000766  0.007166  0.015099  \n",
       "3    -0.141897 -0.014390  0.095265 -0.023132  0.142257  0.325156  0.171970  \n",
       "4    -0.006011  0.007325 -0.018732  0.004144 -0.010431 -0.004584 -0.006360  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977 -0.002395 -0.009385  0.033274  0.079296  0.230744 -0.209229 -0.247969  \n",
       "5978 -0.003491 -0.025970 -0.001241  0.008654 -0.008849 -0.015761 -0.011166  \n",
       "5979 -0.084467  0.018351  0.011924  0.144681  0.012927  0.108045 -0.044070  \n",
       "5980  0.075247 -0.050180  0.088172 -0.168739 -0.393781  0.228118  0.392946  \n",
       "5981 -0.214053 -0.182360  0.173993  0.166463  0.261078  0.370338  0.349272  \n",
       "\n",
       "[5982 rows x 112 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.808812</td>\n",
       "      <td>0.047621</td>\n",
       "      <td>-0.382530</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>0.068270</td>\n",
       "      <td>-1.067826</td>\n",
       "      <td>0.097073</td>\n",
       "      <td>0.064659</td>\n",
       "      <td>0.123847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278258</td>\n",
       "      <td>-0.198159</td>\n",
       "      <td>-0.226099</td>\n",
       "      <td>-0.038080</td>\n",
       "      <td>0.324296</td>\n",
       "      <td>-0.434571</td>\n",
       "      <td>-0.308631</td>\n",
       "      <td>-0.016363</td>\n",
       "      <td>0.194200</td>\n",
       "      <td>-0.018975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.942326</td>\n",
       "      <td>0.181016</td>\n",
       "      <td>0.039485</td>\n",
       "      <td>0.224548</td>\n",
       "      <td>0.165447</td>\n",
       "      <td>0.185474</td>\n",
       "      <td>0.260437</td>\n",
       "      <td>0.067577</td>\n",
       "      <td>-0.233817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035076</td>\n",
       "      <td>-0.147834</td>\n",
       "      <td>0.062239</td>\n",
       "      <td>0.060819</td>\n",
       "      <td>-0.054463</td>\n",
       "      <td>-0.387346</td>\n",
       "      <td>0.112056</td>\n",
       "      <td>-0.059185</td>\n",
       "      <td>0.006323</td>\n",
       "      <td>-0.030976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.939487</td>\n",
       "      <td>0.072728</td>\n",
       "      <td>-0.171136</td>\n",
       "      <td>0.257513</td>\n",
       "      <td>-0.163384</td>\n",
       "      <td>-0.015842</td>\n",
       "      <td>-0.109646</td>\n",
       "      <td>0.098185</td>\n",
       "      <td>-0.189282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057341</td>\n",
       "      <td>0.047966</td>\n",
       "      <td>-0.038936</td>\n",
       "      <td>-0.112071</td>\n",
       "      <td>-0.006490</td>\n",
       "      <td>0.062245</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>0.044459</td>\n",
       "      <td>0.039179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.176414</td>\n",
       "      <td>-0.540114</td>\n",
       "      <td>-0.095850</td>\n",
       "      <td>0.039596</td>\n",
       "      <td>0.054163</td>\n",
       "      <td>0.377580</td>\n",
       "      <td>0.169790</td>\n",
       "      <td>-0.019942</td>\n",
       "      <td>0.183439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>-0.014416</td>\n",
       "      <td>-0.061704</td>\n",
       "      <td>0.120444</td>\n",
       "      <td>-0.374014</td>\n",
       "      <td>0.177117</td>\n",
       "      <td>-0.243579</td>\n",
       "      <td>0.075744</td>\n",
       "      <td>-0.142311</td>\n",
       "      <td>-0.229687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.006475</td>\n",
       "      <td>-0.015659</td>\n",
       "      <td>0.083409</td>\n",
       "      <td>-0.036429</td>\n",
       "      <td>0.082568</td>\n",
       "      <td>-0.020770</td>\n",
       "      <td>0.012661</td>\n",
       "      <td>-0.029358</td>\n",
       "      <td>0.030033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008659</td>\n",
       "      <td>0.040425</td>\n",
       "      <td>-0.007492</td>\n",
       "      <td>0.042274</td>\n",
       "      <td>0.049531</td>\n",
       "      <td>-0.034650</td>\n",
       "      <td>-0.017044</td>\n",
       "      <td>-0.018064</td>\n",
       "      <td>0.021210</td>\n",
       "      <td>-0.000362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-0.527805</td>\n",
       "      <td>0.460797</td>\n",
       "      <td>-0.064967</td>\n",
       "      <td>-0.029257</td>\n",
       "      <td>0.232430</td>\n",
       "      <td>-0.141500</td>\n",
       "      <td>0.104384</td>\n",
       "      <td>0.065651</td>\n",
       "      <td>-0.706434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.477599</td>\n",
       "      <td>0.215720</td>\n",
       "      <td>0.023805</td>\n",
       "      <td>0.367133</td>\n",
       "      <td>0.026163</td>\n",
       "      <td>0.424302</td>\n",
       "      <td>-0.089105</td>\n",
       "      <td>-0.048746</td>\n",
       "      <td>0.062714</td>\n",
       "      <td>-0.055811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.038196</td>\n",
       "      <td>0.022598</td>\n",
       "      <td>0.023371</td>\n",
       "      <td>-0.003908</td>\n",
       "      <td>0.008111</td>\n",
       "      <td>0.030509</td>\n",
       "      <td>-0.068062</td>\n",
       "      <td>-0.030968</td>\n",
       "      <td>0.011029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106910</td>\n",
       "      <td>0.017061</td>\n",
       "      <td>0.158339</td>\n",
       "      <td>-0.014808</td>\n",
       "      <td>-0.045128</td>\n",
       "      <td>0.014905</td>\n",
       "      <td>-0.012643</td>\n",
       "      <td>-0.058065</td>\n",
       "      <td>0.033216</td>\n",
       "      <td>-0.046763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>0.672151</td>\n",
       "      <td>1.288926</td>\n",
       "      <td>0.047105</td>\n",
       "      <td>0.442555</td>\n",
       "      <td>-0.225962</td>\n",
       "      <td>0.620944</td>\n",
       "      <td>-0.542938</td>\n",
       "      <td>-0.306575</td>\n",
       "      <td>-0.724191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267473</td>\n",
       "      <td>0.172990</td>\n",
       "      <td>0.246498</td>\n",
       "      <td>-0.123809</td>\n",
       "      <td>-0.203993</td>\n",
       "      <td>-0.005225</td>\n",
       "      <td>-0.029028</td>\n",
       "      <td>0.124641</td>\n",
       "      <td>0.155520</td>\n",
       "      <td>0.100988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.561052</td>\n",
       "      <td>-0.015907</td>\n",
       "      <td>0.100263</td>\n",
       "      <td>0.082922</td>\n",
       "      <td>-0.023670</td>\n",
       "      <td>0.353346</td>\n",
       "      <td>0.424770</td>\n",
       "      <td>-0.039177</td>\n",
       "      <td>0.215357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149904</td>\n",
       "      <td>-0.020741</td>\n",
       "      <td>-0.158265</td>\n",
       "      <td>0.161862</td>\n",
       "      <td>-0.201403</td>\n",
       "      <td>-0.103223</td>\n",
       "      <td>-0.567873</td>\n",
       "      <td>-0.410687</td>\n",
       "      <td>-0.080075</td>\n",
       "      <td>0.002968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.804268</td>\n",
       "      <td>-0.300905</td>\n",
       "      <td>-0.010156</td>\n",
       "      <td>0.041940</td>\n",
       "      <td>0.061396</td>\n",
       "      <td>0.123361</td>\n",
       "      <td>-0.009046</td>\n",
       "      <td>-0.017363</td>\n",
       "      <td>-0.044154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064577</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>-0.091283</td>\n",
       "      <td>0.040444</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>-0.027214</td>\n",
       "      <td>-0.019029</td>\n",
       "      <td>0.156810</td>\n",
       "      <td>0.123323</td>\n",
       "      <td>0.190463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  0.808812  0.047621 -0.382530  0.136842  0.068270 -1.067826   \n",
       "1           1 -0.942326  0.181016  0.039485  0.224548  0.165447  0.185474   \n",
       "2           2 -0.939487  0.072728 -0.171136  0.257513 -0.163384 -0.015842   \n",
       "3           3  0.176414 -0.540114 -0.095850  0.039596  0.054163  0.377580   \n",
       "4           4 -1.006475 -0.015659  0.083409 -0.036429  0.082568 -0.020770   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -0.527805  0.460797 -0.064967 -0.029257  0.232430 -0.141500   \n",
       "5978     5978 -1.038196  0.022598  0.023371 -0.003908  0.008111  0.030509   \n",
       "5979     5979  0.672151  1.288926  0.047105  0.442555 -0.225962  0.620944   \n",
       "5980     5980 -0.561052 -0.015907  0.100263  0.082922 -0.023670  0.353346   \n",
       "5981     5981 -0.804268 -0.300905 -0.010156  0.041940  0.061396  0.123361   \n",
       "\n",
       "             6         7         8  ...        25        26        27  \\\n",
       "0     0.097073  0.064659  0.123847  ...  0.278258 -0.198159 -0.226099   \n",
       "1     0.260437  0.067577 -0.233817  ... -0.035076 -0.147834  0.062239   \n",
       "2    -0.109646  0.098185 -0.189282  ...  0.057341  0.047966 -0.038936   \n",
       "3     0.169790 -0.019942  0.183439  ...  0.156863 -0.014416 -0.061704   \n",
       "4     0.012661 -0.029358  0.030033  ...  0.008659  0.040425 -0.007492   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977  0.104384  0.065651 -0.706434  ... -0.477599  0.215720  0.023805   \n",
       "5978 -0.068062 -0.030968  0.011029  ...  0.106910  0.017061  0.158339   \n",
       "5979 -0.542938 -0.306575 -0.724191  ... -0.267473  0.172990  0.246498   \n",
       "5980  0.424770 -0.039177  0.215357  ...  0.149904 -0.020741 -0.158265   \n",
       "5981 -0.009046 -0.017363 -0.044154  ... -0.064577  0.030567 -0.091283   \n",
       "\n",
       "            28        29        30        31        32        33        34  \n",
       "0    -0.038080  0.324296 -0.434571 -0.308631 -0.016363  0.194200 -0.018975  \n",
       "1     0.060819 -0.054463 -0.387346  0.112056 -0.059185  0.006323 -0.030976  \n",
       "2    -0.112071 -0.006490  0.062245 -0.004990  0.035366  0.044459  0.039179  \n",
       "3     0.120444 -0.374014  0.177117 -0.243579  0.075744 -0.142311 -0.229687  \n",
       "4     0.042274  0.049531 -0.034650 -0.017044 -0.018064  0.021210 -0.000362  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977  0.367133  0.026163  0.424302 -0.089105 -0.048746  0.062714 -0.055811  \n",
       "5978 -0.014808 -0.045128  0.014905 -0.012643 -0.058065  0.033216 -0.046763  \n",
       "5979 -0.123809 -0.203993 -0.005225 -0.029028  0.124641  0.155520  0.100988  \n",
       "5980  0.161862 -0.201403 -0.103223 -0.567873 -0.410687 -0.080075  0.002968  \n",
       "5981  0.040444  0.000616 -0.027214 -0.019029  0.156810  0.123323  0.190463  \n",
       "\n",
       "[5982 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                           aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train_nm = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test_nm = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_mclas_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                            aggfunc=lambda x: len(x), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train_mclas = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test_mclas = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "### 중분류 구매건수 nm과 대분류 구매건수 mclas를 합친다\n",
    "\n",
    "X_train = pd.concat([X_train_nm, X_train_mclas], axis=1)\n",
    "X_test = pd.concat([X_test_nm, X_test_mclas], axis=1)\n",
    "\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:06<00:25,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7527105921601335 SEED: 1298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:11<00:18,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7483493188768419 SEED: 7070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:16<00:11,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7470809007506255 SEED: 1724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:21<00:05,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7465248818459826 SEED: 9635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:26<00:00,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7466204475952182 SEED: 1611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 중분류 구매건수 -> percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_case3\n",
    "\n",
    "X_train = pd.read_csv('train_numbersOfPurchase_nm_percentile.csv', encoding='cp949')\n",
    "X_test = pd.read_csv('test_numbersOfPurchase_nm_percentile.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:06<00:24,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7620934111759801 SEED: 3134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:12<00:18,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7645216499860996 SEED: 2522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:17<00:11,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7600170280789547 SEED: 8066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:23<00:05,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7632749513483459 SEED: 7454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:29<00:00,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7666675354462053 SEED: 670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 중분류 구매건수 & 구매여부 -> percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_case3\n",
    "\n",
    "X_train = pd.read_csv('train_numbersOfPurchase&isPurchase_nm_percentile.csv', encoding='cp949')\n",
    "X_test = pd.read_csv('test_numbersOfPurchase&isPurchase_nm_percentile.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:17<01:10, 17.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7634313316652767 SEED: 5020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:36<00:54, 18.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7640698846260773 SEED: 6098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:56<00:37, 18.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7666501598554349 SEED: 5732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [01:20<00:20, 20.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7640221017514597 SEED: 6394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:39<00:00, 19.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7656771267723103 SEED: 2191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. 대분류 구매건수 -> percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_case3\n",
    "\n",
    "X_train = pd.read_csv('train_numbersOfPurchase_mclas_percentile.csv', encoding='cp949')\n",
    "X_test = pd.read_csv('test_numbersOfPurchase_mclas_percentile.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:02<00:09,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7629057200444815 SEED: 8295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:04<00:07,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7606077981651377 SEED: 5831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:06<00:04,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765933416736169 SEED: 880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:09<00:02,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7650602932999722 SEED: 645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:12<00:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7597737698081735 SEED: 2863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. 대분류 구매건수 & 구매여부 -> percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_case3\n",
    "\n",
    "X_train = pd.read_csv('train_numbersOfPurchase&isPurchase_mclas_percentile.csv', encoding='cp949')\n",
    "X_test = pd.read_csv('test_numbersOfPurchase&isPurchase_mclas_percentile.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:03<00:12,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7657466291353906 SEED: 3427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:06<00:09,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7669889838754518 SEED: 8049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:08<00:05,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7685354114539894 SEED: 7037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:11<00:02,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7667109744231305 SEED: 4902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:13<00:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7625582082290797 SEED: 7483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. BOW1 -> percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 788 BOW 피쳐(상관관계, 모든전처리, percentile 이후)\n",
    "\n",
    "X_train = pd.read_csv('X_train_after_preprocessing.csv', encoding='cp949')\n",
    "X_test = pd.read_csv('X_test_after_preprocessing.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:02<00:11,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7613636363636364 SEED: 643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:05<00:08,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.759361099527384 SEED: 3340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:08<00:05,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7617372115651933 SEED: 8108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:11<00:02,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7625625521267724 SEED: 1257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:13<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7637831873783709 SEED: 3205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. 중분류 W2V (EmbeddingFeaturzier + gender cosine similarity): 906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train_after_percentile_nm.csv', encoding='cp949')\n",
    "X_test = pd.read_csv('X_test_after_percentile_nm.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:04<00:16,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7634400194606616 SEED: 6562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:09<00:13,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7703598484848484 SEED: 4798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:14<00:09,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7699515221017514 SEED: 4956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:18<00:04,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7690393035863219 SEED: 8838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:23<00:00,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7707638309702529 SEED: 3746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. 대분류 W2V (EF + gender CS): 906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train_after_percentile_mclas_nm.csv', encoding='cp949')\n",
    "X_test = pd.read_csv('X_test_after_percentile_mclas_nm.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:06<00:25,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7677795732554907 SEED: 4590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:12<00:19,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7690957742563247 SEED: 6807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:18<00:12,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7671583958854601 SEED: 794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:27<00:06,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7679533291631916 SEED: 1784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:34<00:00,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7666805671392827 SEED: 8128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. 중분류&대분류(EF + gender CS)1: 1206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train_after_percentile_nm&mclas.csv', encoding='cp949')\n",
    "X_test = pd.read_csv('X_test_after_percentile_nm&mclas.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7731529747011398 SEED: 2762\n",
      "Train on 2800 samples, validate on 700 samples\n",
      "Epoch 1/50\n",
      "2800/2800 [==============================] - 0s 37us/step - loss: 0.1839 - rmse: 0.4276 - val_loss: 0.1868 - val_rmse: 0.4319\n",
      "Epoch 2/50\n",
      "2800/2800 [==============================] - 0s 34us/step - loss: 0.1843 - rmse: 0.4277 - val_loss: 0.1882 - val_rmse: 0.4332\n",
      "Epoch 3/50\n",
      "2800/2800 [==============================] - 0s 28us/step - loss: 0.1845 - rmse: 0.4287 - val_loss: 0.1901 - val_rmse: 0.4354\n",
      "Epoch 4/50\n",
      "2800/2800 [==============================] - 0s 32us/step - loss: 0.1841 - rmse: 0.4281 - val_loss: 0.1866 - val_rmse: 0.4315\n",
      "Epoch 5/50\n",
      "2800/2800 [==============================] - 0s 28us/step - loss: 0.1866 - rmse: 0.4308 - val_loss: 0.1907 - val_rmse: 0.4364\n",
      "Epoch 6/50\n",
      "2800/2800 [==============================] - 0s 29us/step - loss: 0.1848 - rmse: 0.4290 - val_loss: 0.1866 - val_rmse: 0.4317\n",
      "Epoch 7/50\n",
      "2800/2800 [==============================] - 0s 30us/step - loss: 0.1850 - rmse: 0.4293 - val_loss: 0.1870 - val_rmse: 0.4322\n",
      "Epoch 8/50\n",
      "2800/2800 [==============================] - 0s 32us/step - loss: 0.1842 - rmse: 0.4283 - val_loss: 0.1916 - val_rmse: 0.4371\n",
      "Epoch 9/50\n",
      "2800/2800 [==============================] - 0s 35us/step - loss: 0.1841 - rmse: 0.4280 - val_loss: 0.1869 - val_rmse: 0.4318\n",
      "Epoch 10/50\n",
      "2800/2800 [==============================] - 0s 29us/step - loss: 0.1840 - rmse: 0.4279 - val_loss: 0.1917 - val_rmse: 0.4371\n",
      "Epoch 11/50\n",
      "2800/2800 [==============================] - 0s 29us/step - loss: 0.1835 - rmse: 0.4275 - val_loss: 0.1864 - val_rmse: 0.4313\n",
      "Epoch 12/50\n",
      "2800/2800 [==============================] - 0s 30us/step - loss: 0.1811 - rmse: 0.4245 - val_loss: 0.1860 - val_rmse: 0.4309\n",
      "Epoch 13/50\n",
      "2800/2800 [==============================] - 0s 33us/step - loss: 0.1824 - rmse: 0.4264 - val_loss: 0.1886 - val_rmse: 0.4337\n",
      "Epoch 14/50\n",
      "2800/2800 [==============================] - 0s 32us/step - loss: 0.1822 - rmse: 0.4259 - val_loss: 0.1895 - val_rmse: 0.4348\n",
      "Epoch 15/50\n",
      "2800/2800 [==============================] - 0s 29us/step - loss: 0.1842 - rmse: 0.4279 - val_loss: 0.1871 - val_rmse: 0.4323\n",
      "Epoch 16/50\n",
      "2800/2800 [==============================] - 0s 32us/step - loss: 0.1807 - rmse: 0.4240 - val_loss: 0.1997 - val_rmse: 0.4459\n",
      "Epoch 17/50\n",
      "2800/2800 [==============================] - 0s 38us/step - loss: 0.1811 - rmse: 0.4244 - val_loss: 0.1876 - val_rmse: 0.4326\n",
      "Epoch 18/50\n",
      "2800/2800 [==============================] - 0s 32us/step - loss: 0.1822 - rmse: 0.4256 - val_loss: 0.1902 - val_rmse: 0.4353\n",
      "Epoch 19/50\n",
      "2800/2800 [==============================] - 0s 30us/step - loss: 0.1837 - rmse: 0.4276 - val_loss: 0.1872 - val_rmse: 0.4320\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydd3iUVdbAfychISSQSkKHhCIlIYYqwkqxAioooqKiYMO6ruuuq+vuWtdd9LOwrqyKigs2RBTFpQlKUxDpJfQSIJQQIJX05H5/3HfCMKRMkplJmft7nnlm5i33PW/Ke+459xRRSmEwGAwGgz0+tS2AwWAwGOoeRjkYDAaD4QKMcjAYDAbDBRjlYDAYDIYLMMrBYDAYDBdglIPBYDAYLsAoB4PBYDBcgFEOBkMVEJEkEbmytuUwGNyNUQ4Gg8FguACjHAwGFyAi94vIPhE5IyLzRKS1tV1E5E0ROSkiGSKyVUTirH0jRWSHiGSJyFER+WPt3oXBcA6jHAyGGiIilwP/BG4BWgGHgFnW7quBwcBFQChwK3Da2vch8IBSqhkQB/zoQbENhgppVNsCGAwNgDuA6UqpjQAi8mcgTUSigUKgGdAN+FUptdPuvEKgh4hsUUqlAWkeldpgqABjORgMNac12loAQCmVjbYO2iilfgTeBqYCKSIyTUSCrUNvAkYCh0RkhYhc6mG5DYZyMcrBYKg5x4AOti8iEgREAEcBlFJvKaX6ALFo99KT1vZ1SqnRQBTwDTDbw3IbDOVilIPBUHX8RCTA9kI/1O8WkQQRaQz8A1irlEoSkX4icomI+AFngTygWET8ReQOEQlRShUCmUBxrd2RweCAUQ4GQ9VZAOTavS4D/gZ8BRwHOgHjrGODgffR6wmH0O6m16x9dwJJIpIJPAiM95D8BkOliGn2YzAYDAZHjOVgMBgMhgswysFgMBgMF2CUg8FgMBguwCgHg8FgMFxAg8iQbt68uYqOjq5tMQwGg6FesWHDhlNKqciy9jUI5RAdHc369etrWwyDwWCoV4jIofL2GbeSwWAwGC7AKAeDwWAwXIBTykFEhovIbqte/dNl7H/Cqku/VUR+EBH7OjMTRGSv9Zpgt72PiGyzxnxLRMTaHi4iS6zjl4hImCtu1GAwGAzOU+mag4j4oitKXgUkA+tEZJ5SaofdYZuAvkqpHBF5CHgVuFVEwoHngL6AAjZY56YB7wCTgF/Q5QiGAwuBp4EflFKTLUX0NPBUVW+ssLCQ5ORk8vLyqnqqwcMEBATQtm1b/Pz8alsUg8Fg4cyCdH9gn1LqAICIzAJGA6XKQSm1zO74XzhXI+YaYIlS6ox17hJguIgsB4KVUmus7TOBG9DKYTQw1Dp/BrCcaiiH5ORkmjVrRnR0NJZRYqiDKKU4ffo0ycnJxMTE1LY4BoPBwhm3UhvgiN33ZGtbedyLfshXdG4b63NZY7ZQSh0HsN6jyrqIiEwSkfUisj41NfWC/Xl5eURERBjFUMcRESIiIoyFZzDUMZxRDmU9Xcus1ici49EupP+r5FynxywPpdQ0pVRfpVTfyMgyw3SNYqgnmN+TwVD3cEY5JAPt7L63RTc3OQ8RuRL4CzBKKZVfybnJ1ueyxkwRkVbWmK2Ak07IaDAYDM5xfCsc/qW2pajzOKMc1gFdRCRGRPzRdern2R8gIr2A99CKwf5hvhi4WkTCrKijq4HFlrsoS0QGWFFKdwHfWufMA2xRTRPsttcr0tPT+c9//lOtc0eOHEl6errTxz///PO89tprlR9oMBhgyd9g3m9rW4o6T6XKQSlVBDyKftDvBGYrpRJF5EURGWUd9n9AU+BLEdksIvOsc88AL6EVzDrgRdviNPAQ8AGwD9jPuXWKycBVIrIXHSE1uea36XkqUg7FxRU3/FqwYAGhoaHuEMtgMJw5CGlJUFJS25LUaZzKc1BKLVBKXaSU6qSUetna9qxSyqYErlRKtVBKJVivUXbnTldKdbZeH9ltX6+UirPGfFRZXYeUUqeVUlcopbpY72cc5akPPP300+zfv5+EhASefPJJli9fzrBhw7j99tvp2bMnADfccAN9+vQhNjaWadOmlZ4bHR3NqVOnSEpKonv37tx///3ExsZy9dVXk5ubW+F1N2/ezIABA4iPj+fGG28kLS0NgLfeeosePXoQHx/PuHG6SdmKFStISEggISGBXr16kZWV5aafhsFQRygugoxkKC6ArAu84wY7GkRtpcp44btEdhzLdOmYPVoH89z1seXunzx5Mtu3b2fz5s0ALF++nF9//ZXt27eXhmxOnz6d8PBwcnNz6devHzfddBMRERHnjbN3714+//xz3n//fW655Ra++uorxo8vv5vkXXfdxb///W+GDBnCs88+ywsvvMCUKVOYPHkyBw8epHHjxqUuq9dee42pU6cyaNAgsrOzCQgIqOmPxWCo22QeBWVZ7mlJENK2wsO9GVM+w4P079//vFj+t956i4svvpgBAwZw5MgR9u7de8E5MTExJCQkANCnTx+SkpLKHT8jI4P09HSGDBkCwIQJE1i5ciUA8fHx3HHHHXzyySc0aqTnBIMGDeKJJ57grbfeIj09vXS7wdBgSberM3fmYO3JUQ/wiqdBRTN8TxIUFFT6efny5SxdupQ1a9YQGBjI0KFDy4z1b9y4celnX1/fSt1K5TF//nxWrlzJvHnzeOmll0hMTOTpp5/m2muvZcGCBQwYMIClS5fSrVu3ao1vMNQL0uyUQ1pSrYlRHzCWg5to1qxZhT78jIwMwsLCCAwMZNeuXfzyS81D60JCQggLC2PVqlUAfPzxxwwZMoSSkhKOHDnCsGHDePXVV0lPTyc7O5v9+/fTs2dPnnrqKfr27cuuXbtqLIPBUKdJPwTiAyHtIM1YDhXhFZZDbRAREcGgQYOIi4tjxIgRXHvtteftHz58OO+++y7x8fF07dqVAQMGuOS6M2bM4MEHHyQnJ4eOHTvy0UcfUVxczPjx48nIyEApxe9//3tCQ0P529/+xrJly/D19aVHjx6MGDHCJTIYDHUW2zpDeCdjOVSCWEFC9Zq+ffsqx2Y/O3fupHv37rUkkaGqmN+XwSN8cBU0agwRnWHHt/CUd1sPIrJBKdW3rH3GrWQwGLyH9EMQ1gHCYyD3DORl1LZEdRajHAwGg3dQmAvZKRAaDWHReltauV0yvR6jHAwGg3eQfli/h3WAMCuk3CxKl4tRDgaDwTuwWQmhHbSCALMoXQFGORgMBu/AlgAX1gECQqBJuEmEqwCjHAwGg3eQlgSNAqBpC/09PMZYDhVglEMdomnTpgAcO3aMsWPHlnnM0KFDcQzbdWTKlCnk5OSUfq9qCfDyMKXBDfWatCTtUrI1lwqLNmsOFWCUQx2kdevWzJkzp9rnOyoHUwLcYOBcGKuNsBhIP6IrtRouwCgHN/HUU0+d18/h+eef5/XXXyc7O5srrriC3r1707NnT7799sJeRklJScTFxQGQm5vLuHHjiI+P59Zbbz2vttJDDz1E3759iY2N5bnnngN0Mb9jx44xbNgwhg0bBpwrAQ7wxhtvEBcXR1xcHFOmTCm9nikNbmjwpB3WloONsGhdoTXjSLmneDPeUT5j4dNwYptrx2zZE0aU34do3LhxPP744zz88MMAzJ49m0WLFhEQEMDcuXMJDg7m1KlTDBgwgFGjRpXbR/mdd94hMDCQrVu3snXrVnr37l267+WXXyY8PJzi4mKuuOIKtm7dymOPPcYbb7zBsmXLaN68+XljbdiwgY8++oi1a9eilOKSSy5hyJAhhIWFmdLghoZNbhrkZ5xvOYTbwlmTzn02lGIsBzfRq1cvTp48ybFjx9iyZQthYWG0b98epRTPPPMM8fHxXHnllRw9epSUlJRyx1m5cmXpQzo+Pp74+PjSfbNnz6Z379706tWLxMREduzYUaFMP/30EzfeeCNBQUE0bdqUMWPGlBbpM6XBDQ0a+zBWG6WJcGbdoSyc+i8VkeHAvwBf4AOl1GSH/YOBKUA8ME4pNcdu3yuArercS0qpL6ztq4Bm1vYo4Fel1A0iMhTdN9r2G/taKfViNe7tHBXM8N3J2LFjmTNnDidOnCh1sXz66aekpqayYcMG/Pz8iI6OLrNUtz1lWRUHDx7ktddeY926dYSFhTFx4sRKx6mojpYpDW5o0NiHsdpo1gp8/U3EUjlUajmIiC8wFRgB9ABuE5EeDocdBiYCnzmcey3QG0gALgGeFJFgAKXUZba2osAa4Gu7U1fZtRytmWKoRcaNG8esWbOYM2dOafRRRkYGUVFR+Pn5sWzZMg4dqjh9f/DgwXz66acAbN++na1btwKQmZlJUFAQISEhpKSksHDhwtJzyisXPnjwYL755htycnI4e/Ysc+fO5bLLLqvyfZnS4IZ6R1mWg4+v/m6UQ5k4Yzn0B/YppQ4AiMgsYDRQ6sNQSiVZ+xw7dvcAViilioAiEdkCDAdm2w4QkWbA5cDd1b+NuklsbCxZWVm0adOGVq1aAXDHHXdw/fXX07dvXxISEiqdQT/00EPcfffdxMfHk5CQQP/+/QG4+OKL6dWrF7GxsXTs2JFBgwaVnjNp0iRGjBhBq1atWLZsWen23r17M3HixNIx7rvvPnr16lWhC6k8TGlwQ70i/ZCV+OYQtRcWbRLhyqHSkt0iMhYYrpS6z/p+J3CJUurRMo79L/A/m1tJRK4GngOuAgKBX4GpSqnX7c65CxillBprfR8KfAUkA8eAPyqlEsu41iRgEkD79u37OM7ATQno+oX5fRncyic3wdlUeGDl+dsXPAlbZsHTh8/lP3gRNS3ZXdZPzKkmEEqp74EFwGrgc7T7yDGo+DZrn42NQAel1MXAv4Fvyhl7mlKqr1Kqb2RkpDPiGAwGbyXt0PkuJRth0ZCfqaOZDOfhjHJIBtrZfW+LntE7hVLqZWvt4Cq0otlr2yciEWi31Xy74zOVUtnW5wWAn4g0x2AwGKpDSYmuyBpWlnIw1VnLwxnlsA7oIiIxIuIPjAPmOTO4iPhaCgARiUdHM31vd8jNaDdUnt05LcUKzxGR/paMp525niMNocudN2B+Twa3kp0CxfnlWw5g1h3KoNIFaaVUkYg8CixGh7JOV0olisiLwHql1DwR6QfMBcKA60XkBaVULOAHrLKe9ZnAeGtx2sY4wDHOdCzwkIgUAbno0NgqPz0CAgI4ffo0ERER5SaYGWofpRSnT582iXEG91Eaxhp94b7SXIckDwlTf3Aqz8Fy7yxw2Pas3ed1aHeT43l56Iil8sYdWsa2t4G3nZGrItq2bUtycjKpqak1HcrgZgICAmjb9oI/H4PBNZQVxmrDP1BXaTVupQtosKmqfn5+xMSYlHiDweuxWQ6h7cveHxZt2oWWgSmfYTAYGjZpSTob2q8c12WY6etQFkY5GAyGhk15Yaw2wqIhIxmK8j0mUn3AKAeDwdCwcezj4Eh4DKB0bwdDKUY5GAyGhktxIWQerdxyALMo7YBRDgaDoeGScQRUScWWQ5hdXwdDKUY5GAyGhktFYaw2mkaBX6BJhHPAKAeDwdBwKauPgyMiVjhrkickqjcY5WAwGBouaYfApxEEt6n4uLBos+bggFEOBoOh4ZKWBCHtdGOfirBZDqbOVylGORgMhoZLZWGsNsJioDAHsk+6X6Z6glEOBoOh4VJZApwNU4DvAoxyMBgMDZP8bMg55ZzlEG7CWR0xysFgMDRM0g/rd2csh9D2gJhFaTuMcjAYDA2Tivo4ONKosY5oMpZDKUY5GAyGhokzCXD2hEWbRDg7jHIwGAwNk7Qknfkc5GQL+vBoYznY4ZRyEJHhIrJbRPaJyNNl7B8sIhtFpEhExjrse0VEtluvW+22/1dEDorIZuuVYG0XEXnLutZWEeld05s0GAxeSPohbQ042yY4LBqyT0BBjjulqjdUqhxExBeYCoxAt/y8TUQcW38eBiYCnzmcey3QG0gALgGeFJFgu0OeVEolWK/N1rYRQBfrNQl4p6o3ZTAYDE6HsdqwFeBLN13hwDnLoT+wTyl1QClVAMwCRtsfoJRKUkptBUoczu0BrFBKFSmlzgJbgOGVXG80MFNpfgFCRaSVMzdjMBgMgM50djYBzoZNOZh1B8A55dAGsO+CkWxtc4YtwAgRCRSR5sAwoJ3d/pct19GbItK4KtcTkUkisl5E1qempjopjsFg8ApyzkBBdhUth2j9btYdAOeUQ1kOO6cKkCilvgcWAKuBz4E1QJG1+89AN6AfEA48VZXrKaWmKaX6KqX6RkZGOiOOwWDwFtKT9HtVLIfAcGgcbJSDhTPKIZnzZ/ttgWPOXkAp9bK1pnAV+sG/19p+3HId5QMfod1XNb6ewWAwVDmMFazS3R1MIpyFM8phHdBFRGJExB8YB8xzZnAR8RWRCOtzPBAPfG99b2W9C3ADsN06bR5wlxW1NADIUEodr8I9GQwGb8eZPg5lERZjLAeLRpUdoJQqEpFHgcWALzBdKZUoIi8C65VS80SkHzAXCAOuF5EXlFKxgB+wSj//yQTGK6VsbqVPRSQSbU1sBh60ti8ARgL7gBzgbhfdq8Fg8BbSkiAwAho3q9p5YdGwZzGUlICPd6eBVaocAJRSC9APbfttz9p9Xod2/ziel4eOWCprzMvL2a6AR5yRy2AwGMqkqmGsNsJjoDgfso5DiLNxNw0T71aNBoOhYVLVMFYbpRFLZt3BKAeDwdCwKCmG9CPVsxxMOGspRjkYDIaGRdZxKCmsnuUQ0g7E1yTCYZSDwWBoaFQnjNWGrx+EtDWWA0Y5GAyGhkZV+jiURXiMWXPAKAeDwdDQSEsCRFsA1SEs2lgOGOVgMBgaGmmHdFe3Ro0rP7YswmIg5zTkZbpWrnqGUQ4Gg6FhUd0wVhsmYgkwysFgMDQ0qpsAZyPcKt1tlIPBYDA0EIqs7GaXWA7evShtlIPBYGg4pB8BVM0sh4AQaBJmLIfaFsBgMBhcRnX6OJRFWIzXJ8IZ5WAwGBoONUmAs8eEsxrlYDAYGhBpSeDrD81q2HY+PAYyjkBxUeXHNlCMcjAYDA2H9EMQ2r7mvRjCoqGkCDKTXSJWfcQoB4PB0HCoaRirjTArnNWL1x2McjAYDA2HmibA2TCJcM4pBxEZLiK7RWSfiDxdxv7BIrJRRIpEZKzDvldEZLv1utVu+6fWmNtFZLqI+Fnbh4pIhohstl7POl7PYDAYLiAvE3LTXGM5BLcGHz+jHCpCRHyBqcAIdMvP20TEsfXnYWAi8JnDudcCvYEE4BLgSREJtnZ/CnQDegJNgPvsTl2llEqwXi9W9aYMBoMXUlqN1QXKwcdXj+PFiXDOWA79gX1KqQNKqQJgFjDa/gClVJJSaitQ4nBuD2CFUqpIKXUW2AIMt85ZoCyAXymjB7XBYDA4javCWG14eTirM8qhDXDE7nuytc0ZtgAjRCRQRJoDw4B29gdY7qQ7gUV2my8VkS0islBEYssaWEQmich6EVmfmprqpDgGg6HBYnuQV7ePgyNhMXAmCZRyzXj1jEZOHCNlbHPqp6WU+l5E+gGrgVRgDeAYOPwfYKVSapX1fSPQQSmVLSIjgW+ALmWMPQ2YBtC3b1/v/O0ZDIZzpB8C/2a69IUrCIuG/Ay9jhEY7pox6xHOWA7JnD/bbwscc/YCSqmXrbWDq9CKZq9tn4g8B0QCT9gdn6mUyrY+LwD8LKvDYDAYyiftkH6gS1nz2WpQWp3VO9cdnFEO64AuIhIjIv7AOGCeM4OLiK+IRFif44F44Hvr+33ANcBtSqkSu3Naiujfroj0t2Q87fwtGQwGr8RVYaw2vDyctVK3klKqSEQeBRYDvsB0pVSiiLwIrFdKzbNcR3OBMOB6EXlBKRUL+AGrrGd9JjBeKWVzK70LHALWWPu/tiKTxgIPiUgRkAuMsxatDQaDoWyUgvTD0OkK141pUw5emgjnzJqDzb2zwGHbs3af11FGtJFSKg8dsVTWmGVeWyn1NvC2M3IZDAYDAGdToTDHtZaDfxAERXmt5WAypA0GQ/3H1WGsNrw4nNUoB4PBXZQUw5qpOtrF4F5cmQBnT3iMUQ4GQ4NBKZj/B9j3Q+3KsW8pLH4G1n1Yu3J4A7aIotD2rh03LBoykqGowLXj1gOMcjA0PI5vhnUfwKrXa1eOxLn6ffeCio8z1Jy0Q3p9wD/IteOGxQDWYreXYZSDoeGxbY5+P7QaMo/XjgxF+bBrPjRqAkc31J4c3oKrw1hteHE4q1EOhoZFSTFs/wpaxAEKdnxTO3Ls/xHyM+Hyv+jvexbWjhzegqv6ODjixYlwRjkYGhaHfoas43DZH7SC2P517ciROBcCQuGSB/Xsc5dxLbmN4iK9LuAOy6FpC239GcvBYKjnbPsS/JvCRcMhbgwk/+p5f3FhnlYG3a8HXz/oei0cXAH52Z6Vw1vIPAqq2D2Wg4hW7l6YCGeUg6HhUJQPO76FbteBfyDEjtHbbQvDnmL/D1CQBbE36u/dRkJxgd5ucD3uCmO14aW5DkY5GBoO+5ZCXgb0vFl/D4+B1r0871pK/EZXBo0ZrL+3G6C/G9eSe7A9uN1hOcA55eBlVXyMcjA0HLZ9CYHNoeOQc9tix+jQ1tP7PSNDYa4OXbW5lAB8G0GXa2DvYu0fN7iWtEMgPhDipn5h4TFQeFaX6PAijHIwnCPtEKTuqW0pqkd+FuxeqF05tocynHPteMq1tO8HKMg+d10b3UbqTOnDazwjhzeRfkgrBvvfuyvx0nBWoxwM55h9J8wcVT9nt7vmQ1HeOZeSjdB20O4Sz7mWEudCk3CIHnz+9k5XgG9jrcAMrsVdYaw2wqxwVi9blDbKwaA5thmOb9FhoHu/r21pqs62L3XphHb9L9wXOwZOJkLqbvfKUJirH/49RmlXkj2Nm2p31+75Xue7djvuSoCzEdoeEGM5GLyUjTOgUQAERerP9YnsVNi/DOLGlt0FrMdoQNxvPexdon3Tji4lG11H6AfMyZ3ulcObKMyF7BQIjXbfNfwCILi11yXCGeVggIKzuuREjxug13htOWQcrW2pnGfHNzrO3dGlZCO4FXQYBIlfu3fWnjhXL4h3+E3Z+y8aod93z3efDN6GLYfFnZYDeGU4q1EOBh16mZ8JfSZA77tAlcDmT2tbKufZ9iVExUKLMvtKaeLGwKk9kJLoHhkKcmDPIitKqZweWsGtoE0fs+7gStzVx8GRsBiz5lAWIjJcRHaLyD4RebqM/YNFZKOIFInIWId9r4jIdut1q932GBFZKyJ7ReQLqz81ItLY+r7P2h9ds1s0VMrGGRDRBdpfCuEddXz+xo+hpKTyc2ubtCQ4shZ6jq34uB6jQXx13SV3sG+J7kRWnkvJRteRphCfK7HN5j1hOWSf0JMAV1BSAnMf1NWD6yiVKgcR8QWmAiPQLT9vExHHKdphYCLwmcO51wK9gQTgEuBJEQm2dr8CvKmU6gKkAfda2+8F0pRSnYE3reMaNie2w/qPaufaJ3fph2vvu87563tPgIzDcGBZ7chUFWwP+7ibKj4uqLlWeu5yLSXO1es1HQZVfFy3a/W7KcTnGtIP6bWypi3cex1bOKstG7umbPgItnwOi/8KmcdcM6aLccZy6A/sU0odUEoVALOA0fYHKKWSlFJbAcepZg9ghVKqSCl1FtgCDBcRAS4HrNrKzABusD6Ptr5j7b/COr5hkpYEM0fD/x6HI796/vobZ4KPHyTcfm5b9+t1OGZ9WJjeNkdnIDszc4wbo3/exza5VoaCs7BnMXQvI0rJkchuphCfK0lL0i4ldz8iSquzJtV8rKwTsPQFnb1fUgTLJ9d8TDfgjHJoAxyx+55sbXOGLcAIEQkUkebAMKAdEAGkK6VsAfX2Y5Zez9qfYR1/HiIySUTWi8j61NR6mrmYlwGf3ar/QAJC4acpnr1+Ub6evXS7Vs+sbTRqDBffph9g2XX4Z5uSCCd3VO5SstHtOq0IE10ctbT3e+dcSqAfYqWF+LJcK4c34u4wVhuuTIRb9Gedk3PTh9DvPtj0sfvDrKuBM8qhLJXslF2ulPoeWACsBj4H1gBFlYzp1PWUUtOUUn2VUn0jIyOdEaduUVwEX06E0/vg1o91aefd87Wbx1Ps+h/kntEuJUf6TICSQtjy2YX76grbvtTrCM48lAECw6HT5XoB3pXrKYlzdReyDgOdO760EN+PrpPBW0k77P7FaIDACPBvVvNF6b1L9ORk8JMQ0QkG/xH8guCHF10jpwtxRjkko2f7NtoCTjvJlFIvK6USlFJXoR/8e4FTQKiI2Gxw+zFLr2ftDwHOOHu9eoFSsPBP+uFw/b+0L7z/JF03fvVbnpNjwwwIaQ8dh124L7KrdtdsnFk3k7ZKSmDbV/phb2/1VEbcGMg4AsnrXCNHfjbs+V4vePv4OneOKcTnGnLTID/DM5aDrXR3TSyHghyY/wQ07wqDHtPbgprDb36nJ2qHf3GFpC7DGeWwDuhiRRf5A+OAec4MLiK+IhJhfY4H4oHvlVIKWAbY/AETgG+tz/Os71j7f7SObzisfRfWfwiDHtd5BQBBEXq2vvUL3bjE3Zw5oF0bve8En3L+DHrfpS2bQz+7X56qkvyrXjQvL7ehPLqO1GUsXOVa2rsYinKdt17AFOJzFZ4KY7URHl2zRLgVr+i8jOve1K5bGwMehqYtYclzdWoiVqlysPz+jwKLgZ3AbKVUooi8KCKjAESkn4gkAzcD74mILZjcD1glIjuAacB4u3WGp4AnRGQfek3hQ2v7h0CEtf0J4ILQ2XrN7kXa59j9erjiufP3XfqI/uNY8x/3y7HpE13JMuGO8o+JvQEaB2sLo66x7UttaXUbWbXzAoKhy1WWa6m45nIkztWRMu0HVO08byvEV5ADq16Hbx+B4kLXjOnuPg6OhEVrhVQdl+SJ7bD639DrToh2iGjzD4KhT8ORX3RF3zpCJaEVGqXUAvTagf22Z+0+r0O7hhzPy0NHLJU15gF0JFRZ51RxOlhPOLEN5twDrS6GG6ddOGMPba9nwhv+q32RgeHukaO4CDZ9Cl2uhpAKYgv8g7Q8mz6Bka9qV0hdoLhQP5S7joDGzap+ftwYy4xfA9HlZDM7Q3629iH3vnNX6BEAACAASURBVMt5l5KN0kJ8CyDmsurLUNcpLtIJlcv/qet2gXZlDn2q5mO7u4+DI2ExUJyv76Oi/xtHSkrgu9/p/5+ryllb6HUnrJmqo5i6XFN51JsHMBnSniLrhI5MahIKt83SncrKYtDvdH0edybH7F2sE3rKWoh2pM8E/Q+xdbb75KkqB5ZDzumqu5RsXDQc/AJrXmtpzyIddVIVl5INWyG+XQ20EJ9SOhP83UHw3WO6pPbdi/TvbOWrutBjTUk7pKP8moTWfCxnqG7E0obpcHQ9XPOP8id8vo3gyufg1O46U53AKAdPUJADn4+D3HStGIJblX9six764bX2XddlYzqycab2cXa5pvJjW10MrRK0a6muPMS2fakfCp2vrN75/kFw0TW6pWhNfP6Jc/XPsV0VXUo2uo7UrpGGVogveT18NFL/zRcXwi0z4d4l0OFSGPGqrj8190EdSl0TPBXGaqNUOVRh3cGW09BxKMTfUvGx3a6Dtv21leWu//0qYJSDuykpgbkP6JnS2A+hVXzl5wx6XM+MN33ienkyjuq4/F53OG+69pmgS14f3eB6eapKwVnY+T8dHdTIv/rjxI6BnFOQtLJ65+dnaZdS7A3lL+hXxkXD9XtDKcR3ej/Mvgs+uAJO74VrX4dH1lqlS6wI9cBwGPVvSN2pH4I1wd19HBwJba/X6apiOSx6WivBa9+oPFFPBK56Qbut1r5bI1FdgVEO7ubHl2DnPLjmZe0jd4YOl+rZ6Op/u27xzsbmT3VhPVuUlDPEjdVumA3/da0s1WH3Qu12q65LyUaXq3TcenVdS7sXaXdbdVxKNmyF+Op7SGt2Ksz/I0ztD3uXwpCn4bFNOsGrrO5sF12tfew//6v6VQFKSnTkjyctB18/7R5zNtdhz/fauhxi5TQ4Q4eBunrvT1Mgp3Yj+I1ycCebPoGf3oA+d+twtarwm8d1qKYr21uWlOiCejFDdIE9ZwkI1ou427+u/azebXOgWevKaxhVhp8V6bTzOygqqPr5iXO1HG3LaC5UFbqOhGMb62chvvxsWP4KvJUA66frmlyPbYJhf648UOCaf0BwG+1eqo4LJTtFK2dPWg6gF6WdsRwKzsL8P+ichoG/q9o1rnwOCrJ0dFctYpSDu0j6Cb57XCeYjfy/qtd+6XINRHbXMwhX+foPLNMKp8+Eyo91pPdEPWPfNqfSQ91Gzhld/bTnTdV35dgTOwby0vUCd1XIy9Ry1MSlZKM+FuIrLtLK4K1esPwf0GmYdh9d9wY0c7IAXkAwjJ4KZ/ZXLzu4NIw1uurn1gRnE+GWT9b/a9dPqbr7M6q7rnX267RzuRy1gFEO7uD0fvhivJ6d3/zf6jU+9/HRkUsnE7Vv2xVsnKkL6nW7rurntu2rlVVtFuPb8a2uQ1VTl5KNTpdDQEjVy3jvXqjLX/S4ofJjK6M+FeJTSq/3/GcA/O/32lVy7xK49RNo3qXq43UcoisDrH0HDlZx7cfTYaw2wmP0WlVFFvSJbTostfddzpdUcWToM3p9Y9k/qne+CzDKwdXknIFPb9a/2Nu/qFmYXc+xENwWfnZBQb6zp3TY5MW3nZ+d6Swi2uI4tgmOb625PNVh2xxofhG0dGJR3xka+etkxF3zoTDP+fMS52qXSNt+NZehvhTiS0uC6dfAF3domcd9BncvLLtnd1W48nk9ifrmkardf2l2dPuaXb+qVBbOWlKsPQaB4XDlC9W/TkgbXW9t6xda2dQCRjm4kqICHa2RcUT/89jK/FYXXz8Y+KguX1HTct6bP9OF9JzJbSiP+Ft14tbGmTWTpTpkJOufQ8+bXVueOXaM9u/uW+rc8XkZsP8HbTW4wrUF5wrx7fvBNeO5GqX0w/vkTl0L7KE12h3mit+DfxDc8C5kJsPivzh/XvohaNZK93f2JGHW/3R5i9LrbTkN/6x5Eutvfq8t26XP12ycamKUg6tQCub/HpJWaV9qVcsplEfvu3RmZU3KeSulH+jtLoGobtUfJzAceozSCXGejsPe/jWgKm/qU1VihuiKm87WWrK5lGoSpeSIrRBfXW0fumUWHPoJrn4J+kx0ffZu+0tg4G+1y3Kvk0ra02GsNiqyHDKP6/WTjsOcLyNfEU1CdaWEfUvhwIqaj1dFjHJwFT9P0dFJQ56qPNmlKvgHQf8HalbO+/AaHXfeuxoL0Y70nqArYe74tvJjXcm2L3XYp7Mhgc7i20g36dm9UEeYVEbiXAhpp9dgXCnDRcPrZiG+nDPw/V90VFavGlidlTH0Gb3+Mu9RXXOqMjydAGejSahOwCwrEW7RU3ricJ0TOQ3O0u9+/fe25FmPt+01ysEV7JinTb+4m2Don10/fk3LeW+YoQvoxbpgATX6NxDeybML06m74cRW1y1EOxI3Rjfr2bO44uNy07Xrxz6py1V0HVE3C/EtfV7f93Vvus6NVhZ+AXDju5B9EhZWUnepuBAyj9aO5QDaXexoOexepCdMg5+sWph4ZfgFwLC/wPHNsOMb143rBEY51JRDq+HrSXpxcvR/3NOusCblvHPT9R9Vz7HaCqkpItrVdXiN57pXbZujF/hd6cqxp8MgXVm1MtfS7gV63SZ2jOtlsC/EV1c4vFZPAgY8BC3j3H+91r30w3XrFzr/pDwyjuhEztqwHEC7luzXHArOwoI/astn4GOuv178LdAiTrusqpOTU02McqgJG2bAjFEQ3FovQLtzcay65by3famLw7nCpWQj4XbwaeSZhWml9D3EDIZmLd1zDR9fvcC8d0nFETOJc3VF0Ta9XS9DXSvEV1yow1WD27jHGi6PwX/U0WjfPa4j7MrC030cHAmL0QrK5gJc/k/9/bpq5DQ4g4+vjupKO+hRi90oh+pQXKjLBXz3mC63fP8P0DTKvde0L+ftbFq9UlqBtYyH1gmuk6VplM7s3fxZzYunVcbRjfqfwl0uJRtxY7QSLW9RODdNd+6LdYNLyUZdKsS39l2dYzPiFa24PIWvn3Yv5WfC/x4vW1HaXDq1aTmUFGnX1vGtesLWZ6Iue+MuOl8J0Zfp5DoPhTwb5VBVzp6Gj2+Ede/DpY/C7V96rs9BVct5H9sEKduqlxFdGb0n6P7Tu9xcNG7bl9rd0v16916nbX89Sy6v1tKu+fqB4C7XFtSdQnwZybDsn1qe6iRM1pQWsTDsGe1aKisjP/2QtlyDq9BTwZXYQtTP7Nd9GgLD9czendiK8uWcgtVvu/daFkY5VIWURHh/qM45uOFdXUzPk005qlrOe+MMvZDtjll3p2E6isKdZm5Jsc5evuhqHe/tTnysNY19S8uOlkmcq6231m5wKdmoK4X4Fj6lffojXnWflVQZAx/T63gL/nhh3am0Q/pvr6oNllyFLZz1hxd1Xazhkz0zQWzTR7s/V/8bslLcfjmnlIOIDBeR3SKyT0QuaNspIoNFZKOIFInIWId9r4pIoojsFJG3RNNMRDbbvU6JyBTr+Ikikmq37z7X3GoN2TEPPrhKLwjdvRASbqsdOZwt552frWddsTe658Hq46srax5Y7nyVyqpycCWcPel+l5KNuDF6wdnRGso5o+8z9kb3PyxruxDf7kW6S96QP9We2wb039cN72q35XePne9eqq0wVhvBbcDHT1vmnS53fe5NRVzxrC44uPJVt1+qUuUgIr7AVGAEuuXnbSLi2PrzMDAR+Mzh3IHAICAeiAP6AUOUUllKqQTbCzgE2NvzX9jtd2NLNCcoKdEm9uw7dUGsScuhbZ/ak8fZct6Jc6Eg2z0uJRu9xusook0fu2f8bXN0CG6Xq90zviOte+tZoaNryRMuJRu2Qny1EbVUkAMLntRRN5c+6vnrO9K8s3bX7P3+/L+x2kqAs+Hjq63IRgG6Z4UnrauITnp9Y8N/dQ03N+KM5dAf2KeUOqCUKgBmAaPtD1BKJSmltgKOWRoKCAD8gcaAH3CePSQiXYAoYFW17sCd5GdrpbBiMlx8O0ycX3EXN0/hTDnvjTN0ueB2l7hPjpA20Pkq3Y/a1clbhXm6D0b363V5bU8gohXAgeV6bclG4lytNFq5cFG/PCK76WiY2siWXvmq/ru69g33RN1Uh/6T9ELsomd0/4b8bO13r03LAfSayI3vuTanwVmGPKXX4apTzbYKOKMc2gBH7L4nW9sqRSm1BlgGHLdei5VSjqEYt6EtBfuwhJtEZKuIzBGRdmWNLSKTRGS9iKxPTU11RpyqkZYEH16tZ3DX/ANu+I/n67iUR2XlvFN2QPI6nY/g7llNnwm6H/XeShLIqsre73XEiivKEFSF2DGgimGnlQHuSZcSWIX4Rnq+EN/JndoaTbgDomvYK8OV+PjA6LcBBd8+UnvVWB3pOdY1SaXVoWmULjey4xtIdl93RmeUQ1n/EU4FYotIZ6A70BatUC4XkcEOh40DPrf7/h0QrZSKB5YCZa54KqWmKaX6KqX6RkZGOiOO8xxcCdOG6WJgd8zROQa1tTBXFpWV8944E3z9dQVWd9PlGt1HeYOLF6a3fQlBURDt+OfiZlr2hIgu51xLO7/TysITLiUbni7EpxT87wndoOcq985Gq0VYtA7+OLgSlj53bps3M/BRCIrUPw835cU4oxySAfvZe1vgmJPj3wj8opTKVkplAwuB0op0InIx0EgpVar+lFKnlVK24Pn3Ac85+JWCtdNg5g36B3//Muh8hccuXyVs5bx/evP87YV5sHWWDkEMinC/HL6NdD/qfUt0f2pXkJehS1nEjfFsNBjoSUDcGF0BNivFcinFuK5MuDOUFuLz0LrD5s/g8GpdYjqouWeuWVV6T9Cx/rbqubVtOdQ2jZtp91LSKucrClcRZ5TDOqCLiMSIiD96pj/PyfEPA0NEpJGI+AFDAHu30m2cbzUgIvZO/VEOx7uPogIdFbHwSb0Aet9S1xd5cyW2ct6HV+syBzZ2/U+HYtakNHdV6TVehz5u/tQ14+38n47I8FSUkiOxY/T9rJ+uZ6uecinZsBXi27PY9T3EHck5A9//Va9N9brTvdeqCSIw6t868s4vsO4qMU/SZ6Lub3LCPf1VKlUOSqki4FFgMfpBPVsplSgiL4rIKAAR6SciycDNwHsikmidPgfYD2wDtgBblFL2RVNuwUE5AI9Zoa9bgMfQUVDuJfskzLheu2Mu+4MuhREQ7PbL1hhbOW/7ZkAb/qtnVTFDPCdHeEd9vY0f16xyZPphPYtd/W/tNmhTS1FhUd0gqofu4etpl5KNriN0C9PDv7j3Okuf05aauwvruYLg1nDzDO36qktu3trC1w8eWKWfWW7AKZtdKbUAWOCw7Vm7z+vQ7ibH84qBByoY94KlfqXUnwHPFXM5tglm3aFnUGOnezZmuabYynmvmKzLefv6aTPz8r96/h+9zwSYcw8c+FGb/86Qflj32k76ScudflhvbxJeuwlYoF1LP/5dV6Bt2dPz17cvxBdzmXuucfgXPSEa+FudlVwf6DRMvwwaNwbJeNihW8fYs1h3bgtsDvcscm39IU/Rf5Iu5f3zv3Rzd/GFhPGel6PbdfqhvmFG+cqhImUQ/Ru49Lf6PbJb7c9iYy3lEDemdpSUfSG+a/7hehlKC+u1hSEX5LUaDF6uHCK76hna9VPcXzjPXQRFaPfSug90wthF19ROLkajxrpa61qrJn/TqPqlDByJ6AT3Lq3dGXXXkTqk9+QO18vxyzt63HGfebawnqHeIKoulAeuIX379lXr16+vbTFqj/TD8K8E7R+/bZb2V1eDjJxC8ouLiWpWTVM1dTdM7Q9t+uqyF47KIPqyuqsM6iJZJ+D1rtpNOPhJ142bfkT/nmKGwO2zXDeuod4hIhuUUmW2NfRuy6GhENpeh5MeXKkzlqtBflExN7+3mpTMfL588FIuatGs6oNEdtV5D8nrdCLVpY9ayqC7UQbVoVnLc4X4XKkcbJ3WRrq/Po+h/mKUQ0Ph2jd14lQ18wLeWb6fPSnZBAc04s4P1zLnwYG0Cw+s+kB3zNb5IiaaxDV0HQk/vnRuLSekhmWqdy3QJcGvfF5PKgyGcjDTuYaCbyPwr8bDHNibksXUZfsYndCaLx8cSG5BMXd+uJbUrGo28jGKwXXE36rLU3/3GLzZA97up4vj7ZqvQ1CrQsFZWPgnbcnVhcJ6hjqNWXPwckpKFGPfXc3BU2dZ+sQQIpo2ZsOhNMZ/sJaY5kHMemAAwQF+tS2md1NSohePDyzXr0M/Q2GOrojbpg90HKpfbfvpwIDyWPKsjmq7eyF0GOgJyQ11nIrWHIxy8HJmrE7iuXmJvHHLxYzpfS5VZfnuk9w3Yz29O4Qx857+BPjVUmMVw4UUFUDyr+eUxdENOqPbL1A/9DsO1a+o2HNrPSk74L3LIH4c3DC1tiQ31DGMcjCUydH0XK5+YwV9osOZcXc/xMEdNG/LMX43axNXdIvi3fF9aORrvJB1krwMHS5sUxan9ujtQZE6IqnjUN0c6tRueHSDZ2puGeoFJlrJcAFKKf46dxslCl6+Ie4CxQAw6uLWZOQW8rdvtvPUV9v4v7Hx+PiY9YQ6R0CIbhJkaxSUcVSX/LYpi+1WH+ZRbxvFYHAaoxy8lO+2HmfZ7lT+dl2PCqOS7hzQgbSzBbyxZA+hgX789druZSoSQx0ipI1OSEy4XUeOpe7SrVyrmf9i8E6McvBC0s4W8MK8RC5uF8rEgdGVHv/byztz5mwBH/50kPAgfx4Z1tn9Qhpcg4hubxvVvbYlMdQzjHLwQl6av4OM3EI+vaknvk64iUSEZ6/rQUZuIf+3eDehgX7ccYmX19M3GBo4Rjl4GSv3pPL1xqP89vLOdGvpfFlyHx/h1bHxZOQW8tdvthPaxJ9r4+tAP22DweAWTPiJF5FTUMQzc7fRMTKoWq4hP18fpt7em74dwnj8i02s3OOG3t0Gg6FOYJSDF/H693tITsvllZviq5230MTflw8m9KNzVDMe+HgDGw+nuVhKg8FQFzDKwUvYfCSdj34+yPgB7ekXHV6jsUKa+DHjnn5EBTfmnv+uY09KloukNBgMdQWnlIOIDBeR3SKyT0Qu6AwiIoNFZKOIFInIWId9r1ptP3eKyFtixUGKyHJrzM3WK8ra3lhEvrCutVZEomt+m95NYXEJT3+1lahmAfxpeDeXjBnVLIBP7r0Ef18f7vxwLUfO5LhkXIPBUDeoVDmIiC8wFRgB9ABuE5EeDocdRvd6/szh3IHAICAeiAP6AfbNje9QSiVYr5PWtnuBNKVUZ+BN4JWq3pThfN5bsZ9dJ7J46YY4l9ZJahceyMx7+5NbUMxd03/lVHY1C/UZDIY6hzPRSv2BfUqpAwAiMgsYDeywHaCUSrL2OXaXV0AA4A8I4AekVHK90cDz1uc5wNsiIqoh1PmoBfadzOatH/ZxbXwrrurRwuXjd2sZzEd392f8B2uZMP1XPp/kXKE+pRRZ+UWkZORxIjOPExl5nMzKt97z6NYymAkDowkP8ne5zAaDoXKcUQ5tgCN235OBS5wZXCm1RkSWAcfRyuFtpdROu0M+EpFi4Cvg75YCKL2eUqpIRDKACOCU/dgiMgmYBNC+valLXxYlJYpnvt5GE39fnr/efe0u+3QI453xvblvxnrum7Ge6RP7kZFbyImMPFKsB39KVl6pIkjJzCclM4+cguILxgoN9CM8yJ/FiSlMW3mA2/q35/7BMbQKaeI2+Q0Gw4U4oxzKypJyahYvIp2B7oCt3OcSERmslFqJdikdFZFmaOVwJzDT2esppaYB00AX3nNGHm/j83WH+TXpDK+OjSeyWQWlnF3A0K5RvHFrAr+btYm45xZfsN/f14eo4Ma0DA6gR+tgLu8WRcvgAFqEBNCiWWNahgTQIjigNIpqb0oW76zYz4w1SXz8SxJjerXlwaGdiGke5Nb7MBgMGmeUQzLQzu57W+CYk+PfCPyilMoGEJGFwABgpVLqKIBSKktEPkO7r2baXS9ZRBoBIcAZJ69nsDiRkcfkBbsY1DmCm/u0rfwEFzDq4tY0a9yIzUfSaRkSoB/+wQG0DAkgLNCvSjWZurRoxhu3JPD7Ky/i/VUH+GLdEb7ccIQRPVvx8NBOxLYOceOdGAwGZ5TDOqCLiMQAR4FxwO1Ojn8YuF9E/om2CIYAU6yHfqhS6pSI+AHXAUutc+YBE4A1wFjgR7PeUDWUUvz1m+0UlpTwjxt7erRQ3rBuUQzrFuWy8dqFB/Li6Dh+e3kXpv98kI/XHGL+1uMM7RrJw0M70z+mZmG5BoOhbJzq5yAiI4EpgC8wXSn1soi8CKxXSs0TkX7AXCAMyANOKKVirUin/wCD0a6hRUqpJ0QkCFiJXqD2RSuGJ5RSxSISAHwM9EJbDONsi+HlUd/6OeQWFLM/NZt9J7PZezKLvSnZ5BYWM6hzc4Z1jeKiFk1r9EBfsO04D3+6kWdGdmPS4E4ulLz2ycgt5JNfDjH9p4OcPltAv+gwHh7amaFdI021WIOhiphmP7VEdn6RVgApWew7aVMG2RxJy8H2Y2/kI3SICKSRjw+7rWSy1iEBDOkaxbCukQzq3Jygxs6XwMrIKeSKN1bQKiSAuQ8PbLANenILivli3WGmrTzAsYw8urcK5uGhnRjZs5VTxQQNBoNRDm4nI6eQfanaAthrKYB9KVkcy8grPcbf14eOkUF0jmpKl6hmdGnRlC5RTekQEYR/I/0AP56Ry4rdqSzbfZKf950mO78If18f+seEM7RrJEO7RtEpMqjCGfKf5mzhq41HmffoIK/wyxcUlfDt5qO8u2I/+1PPEtM8iAcGd+TG3m1o3Mi0NjUYKsIoBxehlOJ4Rh6JxzJJPJah349mnKcEAvx8ShWAfm9K56imtA8PrNIsvqCohPWHzrB8dyrLd59kT0o2AO3CmzCsaxRDu0ZyacfmNPE/9wD8ed8p7vhgLQ8N7cRTLsqEri+UlCi+33GCqcv2s+1oBi2DA+gXE05Ik0aENvEnNNCP4CZ+hDbxIzTQn5AmfoQG+hHSxM/0x65HZOQUkpZTQLSJWnMJRjlUg5ISRdLpsyQey2T7sQx2HMsk8VgmZ84WALqHSsfmQcS2DqFH62AuaqEVQpvQJm5ppZmclmMpilR+3neK3MJi/Bv5cGnHCIZ2jWRgp+ZM+ng9Aix6fLDXPvCUUvy07xTTfzrIodM5pOcWkpFbSHFJ+X/njRv5lCqK0Cb+hJR+9itVIsF2SsW2L7iJn3FheZDMvELG/Gc1KRl5/PT05YQ0cV22v7dilEMlFBSVsPdkFonHMtlxLJPtRzPYeTyTs1aSlp+vcFGLZsS1DiG2TTCxrYPp1jK4SmsBriS/qJhfD2qrYtnukxxIPVu67/P7B3BpJ9Mn2B6lFNn5RaTnaEWRkVtIek4h6bkF+ntOYem+9NwC0nMKycwtJC2nkNzCCxP17GnWuNE5ZWK965dWJOFBflzasTntI8pvxWqonOISxX0z1rFy7ymKSxR/Gt6Vh4eajoQ1xSiHcli1N5VXFu1iz4lsCop15Y9Af196tNIKILZNCLGtg+kS1ax0XaAucuj0WZbvTqWJvy+39G1X+QkGpykoKrEUSsF5isX+c6bts8M2298UQPdWwYyIa8nwuJZ0iapZNJo38s+FO3lvxQFeuiGOJTtS2HEsk5+eGua1FrKrqEg5eHUnuEB/7Y+++zfRxLbWiiAmIsgtbiF30iEiiAkDjQ/WHfg38iGyWeMqZ5grpcgrLOF4Ri4/7jrJ4sQTvLl0D28s2UPH5kEMtxRFzzYhRlFUwtxNyby34gDjB7TnzgEd6NQ8iNs/WMvcTUe5rb8pneMuvNpyMBg8ycnMPL7fkcLixBOs3n+a4hJFm9AmXBOrFUWfDmFmDcOBTYfTuHXaL/RuH8rH916Cn68PSilGT/2ZrLwilj4xxKt/ZvlFxTWKyjNuJYOhjpGeU8DSnSdZtP04K/eeoqCohOZNG3N1bAuGx7bk0k4R+DXQHBVnOZGRx6i3f6Kxnw/fPvKb8yr02hI937mjNyN6emcv85OZedwzYx1je7dl4qCYao1h3EoGQx0jNNCfsX3aMrZPW7Lzi1i++yQLt5/gm01H+WztYYIDGnFljxaMiGvFZV2ae51vPa+wmEkfr+dsfhEf3zvogtLt18S2JDoikHdX7Gd4XEuvc83tScni7o/WkZZTQIcI97iUjXIwGGqZpo0bcV18a66Lb01eYTGr9p5i0fYTLN2Zwtcbj9IsoBEz7ulP7/ZhtS2qR1BK8dRXW9l2NIP3xveha8tmFxzj6yPcP7gjf5m7nV8OnPGqCL3V+0/xwMcbCPDzZfYDlxLXxj3Jrt5ttxoMdYwAP1+u6tGC12+5mPV/vZKP7+1PcIAfT3yxmZyCotoWzyO8u+IA324+xh+v7srVsS3LPe6m3m1p3tSfd1fs96B0tcvcTclMmP4rLYN1eRx3KQYwysFgqLP4+fpwWZdIXrv5YpJO5zB54a7aFsntLN2RwquLd3H9xa15eGjFRSMD/Hy5e1AMK/aksvN4pockrB2UUrz9415+/8UW+nQIY85DA2kb5t7cGaMcDIY6zqWdIrhnUAwz1xxi1d7U2hbHbexJydLNolqH8OpN8U6tI4y/pANB/r6814Cth8LiEv789TZe+34PNyS0ZsY9/T2SHW6Ug8FQD/jT8K50igziyS+3kpFbWNviuJy0swXcN2M9TfwbMe2uPufVDKuIkEA/buvfnu+2Hic5LcfNUnqe7Pwi7p2xnlnrjvDosM68eWuCxwpKGuVgMNQDAvx8efPWBFKz83lhXmJti+NSCotLeOSzjZzIyGPaXX2q3C/83sti8BH4YNVBN0lYO6Rk5nHLu2v4ed8pJo/pyR+v6erRqCyjHAyGekJ821AeGdaZrzcdZdH247Utjsv4+/92sHr/af4xpme1IrJahTRhdEIbvlh3hDSrMGZ9Z/eJLG6c+jOHTp/lwwl9GVcLmeBGORgMIX/H8gAAEMdJREFU9YjfXt6ZuDbBPDN3O6lZ+bUtTo35bO1hZqw5xP2XxTC2Br3OHxjckdzCYmauOeRC6WqH1ftOMfad1RSVKGY/eClDu7qu7W5VcEo5iMhwEdktIvtE5Oky9g8WkY0iUiQiYx32vSoiiSKyU0TeEk2giMwXkV3Wvsl2x08UkVQR2Wy97qv5bRoMDQM/Xx/evCWB7Pwinpm7jfpc4WDtgdM8++12hlwUydMjutdorC4tmnFl9yhmrEkit6DiSrp1ma83JjPho19pFRrA3Edqt2FXpcrB6gM9FRgB9ABuE5EeDocdBiYCnzmcOxAYBMQDcUA/YIi1+zWlVDd0r+hBIjLC7tQvlFIJ1uuDKt+VwdCA6dKiGX+6pitLdqQwZ0NybYtTLY6cyeGhTzfSPiKQt27r5ZL6SA8M6cSZswXMXn/EBRJ6FqUUb/2wlydmb6FfdDhfPjiQNqFVW3txNc5YDv2BfUqpA0qpAmAWMNr+AKVUklJqK1DicK4CAgB/oDHgB6QopXKUUsuscwuAjUD1bUqDwcu4Z1AM/WPCefG7HRxNz3X79ZRSHEvPpaDI8V+86pzNL+L+mespKi7hg7v6uiwss190OH06hPH+qgMUFddcTk9RWFzCU19t5Y0lexjTuw3/vdszoaqV4Uz5jDaAvSpOBi5xZnCl1BoRWQYcBwR4Wym10/4YEQkFrgf+Zbf5JhEZDOwBfq+UumAqICKTgEkA7dubsr0G78LHR3j95osZPmUlT365hU/uvcRtpeaz8gr53azN/LjrJD4CbcKaEB0RpF/Ng4iOCCS6eRDtwgIr7XtSUqJ4YvZm9qRk8d+7+9MxsqlLZX1wSCfun7me+duOMzqhjUvHdgdZeYU8/OlGVu09xWNXdOH3V3apM3WinFEOZUnqlKNTRDoD3TlnFSwRkcFKqZXW/kbA58BbSqkD1jHfAZ8rpfJF5EFgBnD5BQIoNQ2YBroqqzPyGAwNiXbhgfztuh48/fU2ZqxJ4u5qVuasiKRTZ7lv5nqSTp3lt5d3RoCk0zkknT7LN5uPkpV3rqSHM4pjyg97WZyYwt+u68HgiyJdLu8V3aLoHNWU91YcYNTFrevMg7YsTmTkMfGjX9l7MptXb4rnln51q1GXM8ohGbCXui1wzMnxbwR+UUplA4jIQmAAsNLaPw3Yq5SaYjtBKXXa7vz3gVecvJbB4HXc2q8d3+9IYfLCXQy+KJJOLpyJ/7zvFA9/uhERmHlvfwZ2an7efqUUaTmFJJ0+S9Kps1ppnDpbruJoHdqE5LRcbunblnsGRbtMTnt8fIRJgzvypzlbWbX3lFsUkCtYuSeVP3y5hZz8IqZP7MeQOiinM8phHdBFRGKAo8A44HYnxz8M3C8i/0RbIEOAKQAi8ncgBDgvGklEWimlbEHco4Dz3FAGg+EcIsLkMT25espKnpi9ha8evJRGNewDoZRi5ppDvPi/HXSObMr7d/Utswe2iBAe5E94kP8F+QnlKY7Lu0Xxl2u7u3VGPzqhNa9/v5t3V+yvc8ohr7CYyQt38d/VSVzUoikf39ufbi2Da1usMqlUOSilikTkUWAx4AtMV0olisiLwHql1DwR6QfMBcKA60XkBaVULDAH7RLahnZFLVJKfScibYG/ALuAjdYfyttWZNJjIjIKKALOoKOgDAZDOUQFB/D3G+J49LNNvLN8P7+9oku1xyooKuG5eYl8/uthruzeginjEmjauOqV/StSHO6mcSNf7v1NDP9YsIutyenEtw316PXLY+fxTH43axN7UrKZODCap0d0q9N9OkwnOIOhgfDbzzexcNtxvnlkULVKOZ/OzuehTzbya9IZHhnWiT9c1bXe9VO3kZVXyMDJPzK4SyRT7+hdq7KUlCim/3yQVxftJiTQj9duvrjOuJEq6gRnMqQNhgbCS6NjCQ/y5w+zt5BXWLVEsJ3HMxn19s9sSU7nX+MSePKabvVWMQA0C/Bj/IAOLNx+nKRTZ2tNjhMZedw5fS1/n7+ToV0jWfz44DqjGCrDKAeDoYEQGujPK2Pj2Z2SxZtL9jh93qLtJ7jpndUUlZQw+4FL60UIqDPcPSiaRj4+vL/qQOUH/3979x4cVX0FcPx7IDwKRiC8iUESHoKKPAxvAZUWIVpRixTsVKyWR5UWa+1Ih45jnXF8wqjVSmll1FYrglqxE4sMZUargBDkZVVehoBGQMJDRMCE0z/ub+O62U0usHt3N3s+Mzt7997fvXvyy809ua9zE6B4UzlXPPoW63Ye5IHrevPnn15c43GnqcySgzH1yGXntWPSwM7Mf3sHa0oram0buit3+t9L6NE+m9dnXEKfvNQ4Ph8P7bKb8qOLc1lUsjvQOlRHjldy56IN3Pr8Orq0bkbxzOFMHNg5pS+rjcaSgzH1zO+v7EVeq2b85qUNfHU8+qNFvz5RxYwX3vfuyu2Xy4tTB9Pu7KYBR5p4U4YX8E3VSZ59tzSQ7yvZWUHRY2/zyrrd/Orybiz+xVDy2zQP5LvjzZKDMfVM8yZZPHJ9H3YdOMp9xTWvBP/s4NeMn/cuxZvL+d3YnsyZ0Celr5o5EwVtz2LMBR14bmUpR2IkyniorDrJ3GVbuH7eSk6q8tK0Idwx+jwaneFlxcmUvpEbY2IamJ/DlOEFvLC6jBUf760eX7KzgqufeIey/UdZMHkA00Z2TbvDHadq2siuHD5WyYvvlSVk+aVffMX4eSt5fPlWrumXyxszh1PYJSch3xUkSw7G1FN3/KAHPdqfxV2LN3Lw6AkWrd3FpPmrad6kIa/eNpTLeibnOQFB65vXksEFOTz930/iUjgwRFVZuKaMosffZse+IzxxQz/mTuhLdtPkF82LB0sOxtRTTRs1ZO6EvlR8dYJxT77DbxdvZEB+K167bRjd2mUnO7xATR/ZlfJDx1iywW/ln9qF7gm56+VN9M1rydJfj+CqizrFZdmp4tRvfTTGpI0Lc1swc1R35izbwk1DuzD7yl5pfRz8dI3s0ZaeHbKZ/9Z2ruuXe8r3cBw9Ucma0gOs3L6flTv2s/nTQzQUYXZRL265JD+t7wmJxZKDMfXcjMu7Ma5vbtT6SJlCRJg+siu3L1zPio/3MqpX+1rbH/umipKd3yaDDbsOUnlSyWog9M1rya2XduWHfTrRo3393QOz5GBMPSciGZ0YQq68qCMPL/UK8kUmh+OVVbxfdrA6GawvO8iJqpM0bCD0zm3BlBEFDCloTWGXVjRrnBmbzcz4KY0xGa9Rwwb8fHg+f3j9f6zasZ+sBlKdDEp2HuB45UlE4MJOLbhpWJfqZFBfTjCfKksOxpiM8eMBeTy2fCsT56+qHtezQzY3DOrMkILWDMpvTYtmmZkMIllyMMZkjGaNs7j/2t6s/qSCQfk5DCponVb1joJkycEYk1HG9u7I2N4dkx1Gysu8a9qMMcbUyZKDMcaYGnwlBxEZIyIfi8g2EZkVZfoIEVknIpUiMj5i2kMi8oGIfCgij4sr5CIiF4vIJrfM8PE5IrJMRLa692CfMWiMMabu5CAiDYEngbHA+cAkETk/olkZ3rOeX4iYdygwDLgIuBAYAIx0k58CpgLd3WuMGz8LWK6q3YHl7rMxxpgA+dlzGAhsU9UdqnoCeBEYF95AVUtVdSMQWdVKgaZAY6AJ0AjYIyIdgbNVdaV6D7F+DrjGzTMOeNYNPxs23hhjTED8JIdcYFfY591uXJ1UdSWwAih3r6Wq+qGbf3eMZbZX1XI3fzkQtXSkiEwVkbUisnbfvn1+wjHGGOOTn+QQraKU+lm4iHQDegHn4G38LxeREWeyzOrGqvNVtVBVC9u2TY8HdhtjTLrwkxx2A3lhn88B/Na9vRZYpapHVPUI8AYw2C3znBjLDB12wr3vxRhjTKD83AS3BuguIvnAp8BE4Aafyy8DpojI/Xh7CyOBR1W1XES+FJHBwGrgRuCPbp4lwGTgAff+Wl1fUlJS8oWI7PQZU6Q2wBenOW/Q0iVWizP+0iVWizO+Eh3nubEmiHc+uHYiUgQ8CjQEFqjqfSJyL7BWVZeIyADgVaAVcAz4XFUvcFc6/QkYgXfY6N+qeodbZiHwDPA9vD2KX6qqikhr4CWgM15yuV5VK07v566biKxV1cJELT+e0iVWizP+0iVWizO+khmnr/IZqloMFEeMuztseA3fPUwUGl8FTIuxzLV4l7dGjt8PjPITlzHGmMSwO6SNMcbUYMkB5ic7gFOQLrFanPGXLrFanPGVtDh9nXMwxhiTWWzPwRhjTA2WHIwxxtSQMcnBR2XZJiKy0E1fLSJdkhBjnoiscBVsPxCRmVHaXCoih0RkvXvdHW1ZQRCRUldZd72IrI0yXVzF3W0islFE+ichxvPC+mq9iBwWkdsj2iStT0VkgYjsFZHNYeN8VSYWkcmuzVYRmZyEOB8WkY/c7/ZVEWkZY95a15MA4rxHRD4N+/0WxZi31m1EAHEuDIuxVETWx5g3mP5U1Xr/wrs/YztQgFcEcANwfkSbW4F5bngisDAJcXYE+rvhbGBLlDgvBf6V7D51sZQCbWqZXoR3D4vg3Rm/OgXWg8+Bc1OlT/HuAeoPbA4b9xAwyw3PAh6MMl8OsMO9t3LDrQKOczSQ5YYfjBann/UkgDjvAe70sW7Uuo1IdJwR0+cAdyezPzNlz6HOyrJ8txrsYmBU6BkTQVHVclVd54a/BEJFCtPVOOA59awCWoZKoyTJKGC7qp7u3fRxp6pvAZE3efqpTHwFsExVK1T1ALCMb8veBxKnqr6pqpXu4yqi3OsUtBj96YefbUTc1Ban2+5MAP6RqO/3I1OSg5/KstVt3Ap/CGgdSHRRuMNa/fDKi0QaIiIbROQNEbkg0MC+S4E3RaRERKZGmX7aFX0TZCKx/+BSpU/BX2XiVOvbm/H2EqOpaz0Jwgx3+GtBjMN0qdSfw4E9qro1xvRA+jNTkoOfKrBnXCk2XkTkLOBl4HZVPRwxeR3eYZE+ePWo/hl0fGGGqWp/vAdB3SZexd1wqdSnjYGrgUVRJqdSn/qVSn07G6gEno/RpK71JNGeAroCffEeHTAnSpuU6U9gErXvNQTSn5mSHPxUlq1uIyJZQAtOb/f0jIhII7zE8LyqvhI5XVUPq1fhFvXKmjQSkTYBhxmK5TP3vhevttbAiCZnUtE33sYC61R1T+SEVOpTx09l4pToW3ci/CrgJ+oOiEfysZ4klKruUdUqVT0J/CXG96dKf2YB1wELY7UJqj8zJTlUV5Z1/0FOxKv+Gi5UDRZgPPCfWCt7orhjjU8DH6rq3BhtOoTOhYjIQLzf4f7goqyOo7mIZIeG8U5Obo5otgS40V21NBg4FDpckgQx/xtLlT4NE74uxqpMvBQYLSKt3GGS0W5cYERkDHAXcLWqHo3Rxs96klAR57mujfH9frYRQfg+8JGq7o42MdD+TPQZ71R54V05swXvioTZbty9eCs2eI8zXQRsA94DCpIQ4yV4u7IbgfXuVQRMB6a7NjOAD/CuplgFDE1Sfxa4GDa4eEJ9Gh6r4D1/fDuwCShMUqzN8Db2LcLGpUSf4iWscuAbvP9eb8E717Uc2Orec1zbQuCvYfPe7NbXbcDPkhDnNrzj9KF1NXS1XyeguLb1JOA4/+bWv414G/yOkXG6zzW2EUHG6cY/E1ovw9ompT+tfIYxxpgaMuWwkjHGmFNgycEYY0wNlhyMMcbUYMnBGGNMDZYcjDHG1GDJwRhjTA2WHIwxxtTwf7ma16IEW797AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▊                                                                   | 1/5 [00:06<00:26,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7776445649152072 SEED: 8079\n",
      "Train on 2800 samples, validate on 700 samples\n",
      "Epoch 1/50\n",
      "2800/2800 [==============================] - 0s 34us/step - loss: 0.1863 - rmse: 0.4304 - val_loss: 0.1846 - val_rmse: 0.4293\n",
      "Epoch 2/50\n",
      "2800/2800 [==============================] - 0s 32us/step - loss: 0.1846 - rmse: 0.4288 - val_loss: 0.1887 - val_rmse: 0.4336\n",
      "Epoch 3/50\n",
      "2800/2800 [==============================] - 0s 31us/step - loss: 0.1846 - rmse: 0.4285 - val_loss: 0.1859 - val_rmse: 0.4307\n",
      "Epoch 4/50\n",
      "2800/2800 [==============================] - 0s 34us/step - loss: 0.1856 - rmse: 0.4293 - val_loss: 0.1878 - val_rmse: 0.4329\n",
      "Epoch 5/50\n",
      "2800/2800 [==============================] - 0s 41us/step - loss: 0.1823 - rmse: 0.4262 - val_loss: 0.1859 - val_rmse: 0.4306\n",
      "Epoch 6/50\n",
      "2800/2800 [==============================] - 0s 37us/step - loss: 0.1847 - rmse: 0.4286 - val_loss: 0.1854 - val_rmse: 0.4303\n",
      "Epoch 7/50\n",
      "2800/2800 [==============================] - 0s 36us/step - loss: 0.1885 - rmse: 0.4326 - val_loss: 0.1911 - val_rmse: 0.4365\n",
      "Epoch 8/50\n",
      "2800/2800 [==============================] - 0s 36us/step - loss: 0.1849 - rmse: 0.4294 - val_loss: 0.1888 - val_rmse: 0.4337\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:13<00:20,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7756854670558799 SEED: 4839\n",
      "Train on 2800 samples, validate on 700 samples\n",
      "Epoch 1/50\n",
      "2800/2800 [==============================] - 0s 37us/step - loss: 0.1862 - rmse: 0.4308 - val_loss: 0.1858 - val_rmse: 0.4304\n",
      "Epoch 2/50\n",
      "2800/2800 [==============================] - 0s 33us/step - loss: 0.1877 - rmse: 0.4319 - val_loss: 0.1870 - val_rmse: 0.4322\n",
      "Epoch 3/50\n",
      "2800/2800 [==============================] - 0s 35us/step - loss: 0.1848 - rmse: 0.4285 - val_loss: 0.1888 - val_rmse: 0.4339\n",
      "Epoch 4/50\n",
      "2800/2800 [==============================] - 0s 38us/step - loss: 0.1815 - rmse: 0.4250 - val_loss: 0.1852 - val_rmse: 0.4300\n",
      "Epoch 5/50\n",
      "2800/2800 [==============================] - 0s 36us/step - loss: 0.1826 - rmse: 0.4265 - val_loss: 0.1863 - val_rmse: 0.4312\n",
      "Epoch 6/50\n",
      "2800/2800 [==============================] - 0s 34us/step - loss: 0.1848 - rmse: 0.4290 - val_loss: 0.1859 - val_rmse: 0.4307\n",
      "Epoch 7/50\n",
      "2800/2800 [==============================] - 0s 37us/step - loss: 0.1831 - rmse: 0.4272 - val_loss: 0.1848 - val_rmse: 0.4294\n",
      "Epoch 8/50\n",
      "2800/2800 [==============================] - 0s 35us/step - loss: 0.1837 - rmse: 0.4275 - val_loss: 0.1850 - val_rmse: 0.4296\n",
      "Epoch 9/50\n",
      "2800/2800 [==============================] - 0s 37us/step - loss: 0.1806 - rmse: 0.4239 - val_loss: 0.1886 - val_rmse: 0.4339\n",
      "Epoch 10/50\n",
      "2800/2800 [==============================] - 0s 34us/step - loss: 0.1835 - rmse: 0.4271 - val_loss: 0.1915 - val_rmse: 0.4372\n",
      "Epoch 11/50\n",
      "2800/2800 [==============================] - 0s 38us/step - loss: 0.1861 - rmse: 0.4305 - val_loss: 0.1946 - val_rmse: 0.4403\n",
      "Epoch 12/50\n",
      "2800/2800 [==============================] - 0s 33us/step - loss: 0.1830 - rmse: 0.4271 - val_loss: 0.1857 - val_rmse: 0.4307\n",
      "Epoch 13/50\n",
      "2800/2800 [==============================] - 0s 46us/step - loss: 0.1830 - rmse: 0.4269 - val_loss: 0.1881 - val_rmse: 0.4331\n",
      "Epoch 14/50\n",
      "2800/2800 [==============================] - 0s 47us/step - loss: 0.1825 - rmse: 0.4260 - val_loss: 0.1945 - val_rmse: 0.4402\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:21<00:14,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7766889074228525 SEED: 4334\n",
      "Train on 2800 samples, validate on 700 samples\n",
      "Epoch 1/50\n",
      "2800/2800 [==============================] - 0s 39us/step - loss: 0.1826 - rmse: 0.4263 - val_loss: 0.1856 - val_rmse: 0.4305\n",
      "Epoch 2/50\n",
      "2800/2800 [==============================] - 0s 34us/step - loss: 0.1840 - rmse: 0.4282 - val_loss: 0.1855 - val_rmse: 0.4301\n",
      "Epoch 3/50\n",
      "2800/2800 [==============================] - 0s 35us/step - loss: 0.1854 - rmse: 0.4297 - val_loss: 0.1871 - val_rmse: 0.4321\n",
      "Epoch 4/50\n",
      "2800/2800 [==============================] - 0s 35us/step - loss: 0.1848 - rmse: 0.4288 - val_loss: 0.1883 - val_rmse: 0.4334\n",
      "Epoch 5/50\n",
      "2800/2800 [==============================] - 0s 33us/step - loss: 0.1866 - rmse: 0.4315 - val_loss: 0.1895 - val_rmse: 0.4347\n",
      "Epoch 6/50\n",
      "2800/2800 [==============================] - 0s 35us/step - loss: 0.1834 - rmse: 0.4270 - val_loss: 0.1868 - val_rmse: 0.4318\n",
      "Epoch 7/50\n",
      "2800/2800 [==============================] - 0s 34us/step - loss: 0.1823 - rmse: 0.4257 - val_loss: 0.1879 - val_rmse: 0.4331\n",
      "Epoch 8/50\n",
      "2800/2800 [==============================] - 0s 42us/step - loss: 0.1859 - rmse: 0.4299 - val_loss: 0.1885 - val_rmse: 0.4337\n",
      "Epoch 9/50\n",
      "2800/2800 [==============================] - 0s 33us/step - loss: 0.1862 - rmse: 0.4309 - val_loss: 0.1910 - val_rmse: 0.4363\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:30<00:07,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7766715318320823 SEED: 3815\n",
      "Train on 2800 samples, validate on 700 samples\n",
      "Epoch 1/50\n",
      "2800/2800 [==============================] - 0s 31us/step - loss: 0.1899 - rmse: 0.4345 - val_loss: 0.1852 - val_rmse: 0.4298\n",
      "Epoch 2/50\n",
      "2800/2800 [==============================] - 0s 37us/step - loss: 0.1845 - rmse: 0.4286 - val_loss: 0.1872 - val_rmse: 0.4323\n",
      "Epoch 3/50\n",
      "2800/2800 [==============================] - 0s 33us/step - loss: 0.1855 - rmse: 0.4296 - val_loss: 0.1879 - val_rmse: 0.4329\n",
      "Epoch 4/50\n",
      "2800/2800 [==============================] - 0s 34us/step - loss: 0.1854 - rmse: 0.4291 - val_loss: 0.1846 - val_rmse: 0.4291\n",
      "Epoch 5/50\n",
      "2800/2800 [==============================] - 0s 35us/step - loss: 0.1837 - rmse: 0.4270 - val_loss: 0.1874 - val_rmse: 0.4322\n",
      "Epoch 6/50\n",
      "2800/2800 [==============================] - 0s 41us/step - loss: 0.1862 - rmse: 0.4304 - val_loss: 0.1880 - val_rmse: 0.4330\n",
      "Epoch 7/50\n",
      "2800/2800 [==============================] - 0s 37us/step - loss: 0.1834 - rmse: 0.4276 - val_loss: 0.1904 - val_rmse: 0.4356\n",
      "Epoch 8/50\n",
      "2800/2800 [==============================] - 0s 36us/step - loss: 0.1836 - rmse: 0.4269 - val_loss: 0.1859 - val_rmse: 0.4307\n",
      "Epoch 9/50\n",
      "2800/2800 [==============================] - 0s 47us/step - loss: 0.1846 - rmse: 0.4288 - val_loss: 0.1865 - val_rmse: 0.4316\n",
      "Epoch 10/50\n",
      "2800/2800 [==============================] - 0s 39us/step - loss: 0.1867 - rmse: 0.4309 - val_loss: 0.2049 - val_rmse: 0.4515\n",
      "Epoch 11/50\n",
      "2800/2800 [==============================] - 0s 38us/step - loss: 0.1867 - rmse: 0.4310 - val_loss: 0.1877 - val_rmse: 0.4330\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:38<00:00,  7.67s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    \n",
    "    # Train the Model\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=64, \n",
    "                        validation_data=(X_valid,y_valid), callbacks=[EarlyStopping(patience=7)], verbose=0)\n",
    "\n",
    "    plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. 중분류&대분류(EF + gender CS)2: 1206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train_after_percentile_nm&mclas2.csv', encoding='cp949')\n",
    "X_test = pd.read_csv('X_test_after_percentile_nm&mclas2.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:06<00:26,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7684919724770644 SEED: 4573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:14<00:20,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7718411175979984 SEED: 8799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:20<00:13,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.769056679177092 SEED: 8580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:27<00:06,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7669498887962192 SEED: 7408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:35<00:00,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7701079024186822 SEED: 2448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):    \n",
    "    SEED = np.random.randint(1, 10000)              \n",
    "    random.seed(SEED)       \n",
    "    np.random.seed(SEED)     \n",
    "    if tf.__version__[0] < '2':  \n",
    "        tf.set_random_seed(SEED)\n",
    "    else:\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "    # Define the NN architecture\n",
    "    input = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='elu')(input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(64)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(32)(x)\n",
    "    x = Add()([x1,x])\n",
    "    x = Dense(16, activation='elu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x1 = Dense(16)(x)\n",
    "    x = Add()([x1,x])\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "    model = Model(input, output)  \n",
    "    \n",
    "    # Choose the optimizer and the cost function\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    \n",
    "    # Train the model\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, \n",
    "                 callbacks=callbacks, shuffle=False, verbose=0)\n",
    "    \n",
    "    print(roc_auc_score(y_valid, model.predict(X_valid)), 'SEED:', SEED)\n",
    "    \n",
    "    # Make submissions\n",
    "    submission = pd.DataFrame({\n",
    "        \"item_id\": IDtest, \n",
    "        \"item_cnt_month\": model.predict(X_test).clip(0, 20).flatten()\n",
    "    })\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = f\"{folder}/dnn_submission_{t.month:02}{t.day:02}_s{SEED:05}.csv\"\n",
    "    submission.to_csv(fname, index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
