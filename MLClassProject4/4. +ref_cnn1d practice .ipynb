{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Data Wrangling\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from vecstack import stacking\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "\n",
    "# Keras\n",
    "import tensorflow as tf\n",
    "# Tensorflow warning off\n",
    "if tf.__version__[0] < '2':\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import * #Input, Dense\n",
    "from keras.models import * #Model\n",
    "from keras.optimizers import *\n",
    "from keras.initializers import *\n",
    "from keras.regularizers import *\n",
    "from keras.constraints import *\n",
    "from keras.utils.np_utils import *\n",
    "from keras.utils.vis_utils import * #model_to_dot\n",
    "from keras.preprocessing.image import *\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import *\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras import Input\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.constraints import max_norm\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv').gender\n",
    "IDtest = df_test.cust_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.goods_id.nunique(), df_train.gds_grp_nm.nunique(), df_train.gds_grp_mclas_nm.nunique()\n",
    "#df_train.store_nm.astype('category').cat.codes\n",
    "#pd.to_datetime(df_train.tran_date).dt.weekday\n",
    "\n",
    "max_features = 100000\n",
    "max_len = 100\n",
    "emb_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### low level: goods_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 100), (2482, 100))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts a \"goods_id\" to a sequence of indexes in a fixed-size hashing space\n",
    "df_train.goods_id = df_train.goods_id.apply(lambda x: str(x))\n",
    "df_test.goods_id = df_test.goods_id.apply(lambda x: str(x))\n",
    "X_train = df_train.groupby('cust_id')['goods_id'].apply(lambda x: [one_hot(i, max_features)[0] for i in x]).values\n",
    "X_test = df_test.groupby('cust_id')['goods_id'].apply(lambda x: [one_hot(i, max_features)[0] for i in x]).values\n",
    "\n",
    "# Pads sequences to the same length\n",
    "X_train_low = pad_sequences(X_train, maxlen=max_len, padding='post', value=0)\n",
    "X_test_low = pad_sequences(X_test, maxlen=max_len, padding='post', value=0)\n",
    "\n",
    "X_train_low.shape, X_test_low.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### middle level: gds_grp_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 100), (2482, 100))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts a \"gds_grp_nm\" to a sequence of indexes in a fixed-size hashing space\n",
    "X_train = df_train.groupby('cust_id')['gds_grp_nm'].apply(lambda x: [one_hot(i, max_features//10)[0] for i in x]).values\n",
    "X_test = df_test.groupby('cust_id')['gds_grp_nm'].apply(lambda x: [one_hot(i, max_features//10)[0] for i in x]).values\n",
    "\n",
    "# Pads sequences to the same length\n",
    "X_train_mid = pad_sequences(X_train, maxlen=max_len, padding='post', value=0)\n",
    "X_test_mid = pad_sequences(X_test, maxlen=max_len, padding='post', value=0)\n",
    "\n",
    "X_train_mid.shape, X_test_mid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### high level: gds_grp_mclas_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 100), (2482, 100))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts a \"gds_grp_mclas_nm\" to a sequence of indexes in a fixed-size hashing space\n",
    "X_train = df_train.groupby('cust_id')['gds_grp_mclas_nm'].apply(lambda x: [one_hot(i, max_features//100)[0] for i in x]).values\n",
    "X_test = df_test.groupby('cust_id')['gds_grp_mclas_nm'].apply(lambda x: [one_hot(i, max_features//100)[0] for i in x]).values\n",
    "\n",
    "# Pads sequences to the same length\n",
    "X_train_high = pad_sequences(X_train, maxlen=max_len, padding='post', value=0)\n",
    "X_test_high = pad_sequences(X_test, maxlen=max_len, padding='post', value=0)\n",
    "\n",
    "X_train_high.shape, X_test_high.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 100), (2482, 100))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts a \"store_nm\" to a sequence of indexes in a fixed-size hashing space\n",
    "X_train = df_train.groupby('cust_id')['store_nm'].apply(lambda x: [one_hot(i, max_features//100)[0] for i in x]).values\n",
    "X_test = df_test.groupby('cust_id')['store_nm'].apply(lambda x: [one_hot(i, max_features//100)[0] for i in x]).values\n",
    "\n",
    "# Pads sequences to the same length\n",
    "X_train_str = pad_sequences(X_train, maxlen=max_len, padding='post', value=0)\n",
    "X_test_str = pad_sequences(X_test, maxlen=max_len, padding='post', value=0)\n",
    "\n",
    "X_train_str.shape, X_test_str.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 100), (2482, 100))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts a \"tran_date\" to a sequence of indexes in a fixed-size hashing space\n",
    "df_train.tran_date = pd.to_datetime(df_train.tran_date).dt.weekday\n",
    "df_test.tran_date = pd.to_datetime(df_test.tran_date).dt.weekday\n",
    "X_train = df_train.groupby('cust_id')['tran_date'].apply(lambda x: list(x)).values\n",
    "X_test = df_test.groupby('cust_id')['tran_date'].apply(lambda x: list(x)).values\n",
    "\n",
    "# Pads sequences to the same length\n",
    "X_train_day = pad_sequences(X_train, maxlen=max_len, padding='post', value=0)\n",
    "X_test_day = pad_sequences(X_test, maxlen=max_len, padding='post', value=0)\n",
    "\n",
    "X_train_day.shape, X_test_day.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "low (InputLayer)                (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mid (InputLayer)                (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "high (InputLayer)               (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "store (InputLayer)              (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "day (InputLayer)                (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 128)     12800000    low[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 100, 128)     1280000     mid[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 100, 128)     128000      high[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 100, 128)     128000      store[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 100, 128)     896         day[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 96, 32)       20512       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 98, 32)       12320       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 100, 32)      4128        embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 100, 32)      4128        embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 100, 32)      4128        embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 19, 32)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 32, 32)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 100, 32)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 100, 32)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 100, 32)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 32)       5152        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 30, 32)       3104        max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 100, 32)      1056        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 100, 32)      1056        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 100, 32)      1056        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 32)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 32)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 32)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 32)           0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 32)           0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32)           0           global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32)           0           global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "my_layer (Add)                  (None, 32)           0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            33          my_layer[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 14,393,569\n",
      "Trainable params: 14,393,569\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2800 samples, validate on 700 samples\n",
      "Epoch 1/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.6976 - acc: 0.5357 - val_loss: 0.6567 - val_acc: 0.6229\n",
      "Epoch 2/50\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 0.6473 - acc: 0.6257 - val_loss: 0.6418 - val_acc: 0.6571\n",
      "Epoch 3/50\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 0.6329 - acc: 0.6407 - val_loss: 0.6336 - val_acc: 0.6671\n",
      "Epoch 4/50\n",
      "2800/2800 [==============================] - 13s 4ms/step - loss: 0.6286 - acc: 0.6382 - val_loss: 0.6288 - val_acc: 0.6600\n",
      "Epoch 5/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.6194 - acc: 0.6486 - val_loss: 0.6230 - val_acc: 0.6671\n",
      "Epoch 6/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.6145 - acc: 0.6618 - val_loss: 0.6179 - val_acc: 0.6700\n",
      "Epoch 7/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.6054 - acc: 0.6707 - val_loss: 0.6128 - val_acc: 0.6700\n",
      "Epoch 8/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5951 - acc: 0.6736 - val_loss: 0.6071 - val_acc: 0.6757\n",
      "Epoch 9/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5908 - acc: 0.6714 - val_loss: 0.6012 - val_acc: 0.6843\n",
      "Epoch 10/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5857 - acc: 0.6846 - val_loss: 0.5955 - val_acc: 0.6800\n",
      "Epoch 11/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5808 - acc: 0.6936 - val_loss: 0.5912 - val_acc: 0.6843\n",
      "Epoch 12/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5708 - acc: 0.6993 - val_loss: 0.5848 - val_acc: 0.6800\n",
      "Epoch 13/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5626 - acc: 0.7061 - val_loss: 0.5793 - val_acc: 0.6771\n",
      "Epoch 14/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5617 - acc: 0.7154 - val_loss: 0.5747 - val_acc: 0.6886\n",
      "Epoch 15/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5513 - acc: 0.7214 - val_loss: 0.5699 - val_acc: 0.6914\n",
      "Epoch 16/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5458 - acc: 0.7321 - val_loss: 0.5660 - val_acc: 0.6871\n",
      "Epoch 17/50\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 0.5415 - acc: 0.7304 - val_loss: 0.5623 - val_acc: 0.6929\n",
      "Epoch 18/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5336 - acc: 0.7307 - val_loss: 0.5608 - val_acc: 0.7014\n",
      "Epoch 19/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5261 - acc: 0.7461 - val_loss: 0.5562 - val_acc: 0.6971\n",
      "Epoch 20/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5184 - acc: 0.7561 - val_loss: 0.5586 - val_acc: 0.7086\n",
      "Epoch 21/50\n",
      "2800/2800 [==============================] - 13s 4ms/step - loss: 0.5197 - acc: 0.7454 - val_loss: 0.5543 - val_acc: 0.7043\n",
      "Epoch 22/50\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 0.5135 - acc: 0.7507 - val_loss: 0.5520 - val_acc: 0.7014\n",
      "Epoch 23/50\n",
      "2800/2800 [==============================] - 13s 4ms/step - loss: 0.5092 - acc: 0.7554 - val_loss: 0.5487 - val_acc: 0.6971\n",
      "Epoch 24/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4973 - acc: 0.7661 - val_loss: 0.5489 - val_acc: 0.7000\n",
      "Epoch 25/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4952 - acc: 0.7668 - val_loss: 0.5488 - val_acc: 0.7043\n",
      "Epoch 26/50\n",
      "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4931 - acc: 0.7743 - val_loss: 0.5511 - val_acc: 0.7114\n",
      "Epoch 27/50\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4841 - acc: 0.7786 - val_loss: 0.5478 - val_acc: 0.7043\n",
      "Epoch 28/50\n",
      "2800/2800 [==============================] - 13s 4ms/step - loss: 0.4787 - acc: 0.7875 - val_loss: 0.5515 - val_acc: 0.7043\n",
      "Epoch 29/50\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4724 - acc: 0.7757 - val_loss: 0.5532 - val_acc: 0.7071\n",
      "Epoch 30/50\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4673 - acc: 0.7896 - val_loss: 0.5463 - val_acc: 0.6986\n",
      "Epoch 31/50\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4538 - acc: 0.7929 - val_loss: 0.5534 - val_acc: 0.7071\n",
      "Epoch 32/50\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4552 - acc: 0.7936 - val_loss: 0.5513 - val_acc: 0.7057\n",
      "Epoch 33/50\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4414 - acc: 0.8043 - val_loss: 0.5511 - val_acc: 0.7000\n",
      "Epoch 34/50\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4369 - acc: 0.8086 - val_loss: 0.5529 - val_acc: 0.7057\n",
      "Epoch 35/50\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4307 - acc: 0.8125 - val_loss: 0.5538 - val_acc: 0.7100\n",
      "Epoch 36/50\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4293 - acc: 0.8154 - val_loss: 0.5547 - val_acc: 0.7071\n",
      "Epoch 37/50\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4182 - acc: 0.8207 - val_loss: 0.5545 - val_acc: 0.7086\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the Model & its Architecture\n",
    "in_low = Input(shape=(max_len,), dtype='int32', name='low')\n",
    "x = Embedding(max_features, emb_dim)(in_low)\n",
    "x = Conv1D(32, 5, activation='elu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(32, 5, activation='elu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "out_low = Dropout(0.5)(x)\n",
    "\n",
    "in_mid = Input(shape=(max_len,), dtype='int32', name='mid')\n",
    "x = Embedding(max_features//10, emb_dim)(in_mid)\n",
    "x = Conv1D(32, 3, activation='elu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(32, 3, activation='elu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "out_mid = Dropout(0.5)(x)\n",
    "\n",
    "in_high = Input(shape=(max_len,), dtype='int32', name='high')\n",
    "x = Embedding(max_features//100, emb_dim)(in_high)\n",
    "x = Conv1D(32, 1, activation='elu')(x)\n",
    "x = MaxPooling1D(1)(x)\n",
    "x = Conv1D(32, 1, activation='elu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "out_high = Dropout(0.5)(x)\n",
    "\n",
    "in_str = Input(shape=(max_len,), dtype='int32', name='store')\n",
    "x = Embedding(max_features//100, emb_dim)(in_str)\n",
    "x = Conv1D(32, 1, activation='elu')(x)\n",
    "x = MaxPooling1D(1)(x)\n",
    "x = Conv1D(32, 1, activation='elu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "out_str = Dropout(0.5)(x)\n",
    "\n",
    "in_day = Input(shape=(max_len,), dtype='int32', name='day')\n",
    "x = Embedding(7, emb_dim)(in_day)\n",
    "x = Conv1D(32, 1, activation='elu')(x)\n",
    "x = MaxPooling1D(1)(x)\n",
    "x = Conv1D(32, 1, activation='elu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "out_day = Dropout(0.5)(x)\n",
    "\n",
    "x = add([out_low, out_mid, out_high, out_str, out_day], name='my_layer')\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model([in_low, in_mid, in_high, in_str, in_day], out)\n",
    "model.summary()\n",
    "\n",
    "# Choose the Optimizer and the Cost function\n",
    "model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit([X_train_low, X_train_mid, X_train_high, X_train_str, X_train_day], y_train, epochs=50, batch_size=64, \n",
    "                    validation_split=0.2, callbacks=[EarlyStopping(patience=7)])\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Features from the Conv1dNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0727174 ,  1.15197   ,  1.0216544 , ...,  0.24971154,\n",
       "         0.19686052,  1.2806371 ],\n",
       "       [ 0.09913812, -0.17606592, -0.08223954, ..., -0.03841991,\n",
       "         0.18293887, -0.2368778 ],\n",
       "       [ 0.11789942, -0.19283518, -0.10816038, ..., -0.02684091,\n",
       "         0.23207286, -0.26180995],\n",
       "       ...,\n",
       "       [ 0.03455506, -0.22690201, -0.16441786, ..., -0.21557958,\n",
       "         0.2042957 , -0.2218439 ],\n",
       "       [ 0.04102264, -0.01179045, -0.07082593, ..., -0.24163355,\n",
       "         0.15322608, -0.27614096],\n",
       "       [-0.00326405,  0.9113719 ,  0.83503973, ...,  0.31698442,\n",
       "         0.12330534,  1.0945086 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_name = 'my_layer'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict([X_train_low, X_train_mid, X_train_high, X_train_str, X_train_day])\n",
    "intermediate_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23966347,  0.2030532 ,  0.21165237, ...,  0.4597959 ,\n",
       "         0.2883425 ,  0.20440693],\n",
       "       [-0.02129774,  0.82032186,  0.7113014 , ...,  0.10757239,\n",
       "         0.23304266,  0.8421644 ],\n",
       "       [-0.01829028,  0.43226397,  0.29338574, ...,  0.14806008,\n",
       "         0.1288276 ,  0.33098933],\n",
       "       ...,\n",
       "       [ 0.3342398 ,  0.52689344,  0.26990807, ...,  0.628958  ,\n",
       "         0.2786358 ,  0.3384126 ],\n",
       "       [ 0.11073056,  0.44869766,  0.2808383 , ..., -0.008699  ,\n",
       "         0.15223399,  0.18503076],\n",
       "       [ 0.083348  , -0.00820611,  0.07381316, ..., -0.06953269,\n",
       "         0.20604846, -0.04798091]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_name = 'my_layer'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict([X_test_low, X_test_mid, X_test_high, X_test_str, X_test_day])\n",
    "intermediate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn_hidden = pd.DataFrame(intermediate_output).rename(columns=dict(zip(origin, new_name)))\n",
    "X_train_cnn_hidden.to_csv('X_train_cnn_hidden.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cnn_hidden = pd.DataFrame(intermediate_output).rename(columns=dict(zip(origin, new_name)))\n",
    "X_test_cnn_hidden.to_csv('X_test_cnn_hidden.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = range(32)\n",
    "new_name = range(128, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>...</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.072717</td>\n",
       "      <td>1.151970</td>\n",
       "      <td>1.021654</td>\n",
       "      <td>0.343519</td>\n",
       "      <td>0.104696</td>\n",
       "      <td>0.095997</td>\n",
       "      <td>0.450215</td>\n",
       "      <td>1.041780</td>\n",
       "      <td>0.799742</td>\n",
       "      <td>1.081851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136313</td>\n",
       "      <td>0.271594</td>\n",
       "      <td>0.324329</td>\n",
       "      <td>0.152943</td>\n",
       "      <td>0.335057</td>\n",
       "      <td>1.076024</td>\n",
       "      <td>0.154671</td>\n",
       "      <td>0.249712</td>\n",
       "      <td>0.196861</td>\n",
       "      <td>1.280637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.099138</td>\n",
       "      <td>-0.176066</td>\n",
       "      <td>-0.082240</td>\n",
       "      <td>-0.012129</td>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.122720</td>\n",
       "      <td>-0.010819</td>\n",
       "      <td>-0.043584</td>\n",
       "      <td>-0.100474</td>\n",
       "      <td>-0.227882</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042621</td>\n",
       "      <td>0.020277</td>\n",
       "      <td>0.062644</td>\n",
       "      <td>0.107495</td>\n",
       "      <td>-0.174846</td>\n",
       "      <td>-0.223387</td>\n",
       "      <td>0.233890</td>\n",
       "      <td>-0.038420</td>\n",
       "      <td>0.182939</td>\n",
       "      <td>-0.236878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117899</td>\n",
       "      <td>-0.192835</td>\n",
       "      <td>-0.108160</td>\n",
       "      <td>0.137689</td>\n",
       "      <td>0.235482</td>\n",
       "      <td>0.061874</td>\n",
       "      <td>0.133768</td>\n",
       "      <td>-0.017286</td>\n",
       "      <td>-0.001746</td>\n",
       "      <td>-0.213690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045991</td>\n",
       "      <td>-0.051799</td>\n",
       "      <td>0.374233</td>\n",
       "      <td>0.309879</td>\n",
       "      <td>0.010004</td>\n",
       "      <td>-0.344870</td>\n",
       "      <td>0.179501</td>\n",
       "      <td>-0.026841</td>\n",
       "      <td>0.232073</td>\n",
       "      <td>-0.261810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.217766</td>\n",
       "      <td>0.577409</td>\n",
       "      <td>0.301428</td>\n",
       "      <td>0.259764</td>\n",
       "      <td>0.202313</td>\n",
       "      <td>0.240055</td>\n",
       "      <td>0.282168</td>\n",
       "      <td>0.418546</td>\n",
       "      <td>0.568550</td>\n",
       "      <td>0.414526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136881</td>\n",
       "      <td>0.262325</td>\n",
       "      <td>0.315605</td>\n",
       "      <td>0.119778</td>\n",
       "      <td>0.166527</td>\n",
       "      <td>0.237733</td>\n",
       "      <td>0.278135</td>\n",
       "      <td>0.247320</td>\n",
       "      <td>0.284798</td>\n",
       "      <td>0.516480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.033859</td>\n",
       "      <td>-0.006845</td>\n",
       "      <td>-0.046622</td>\n",
       "      <td>-0.175963</td>\n",
       "      <td>-0.032874</td>\n",
       "      <td>-0.314592</td>\n",
       "      <td>-0.130444</td>\n",
       "      <td>0.221746</td>\n",
       "      <td>0.053102</td>\n",
       "      <td>-0.052906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050461</td>\n",
       "      <td>-0.145454</td>\n",
       "      <td>-0.168152</td>\n",
       "      <td>-0.129829</td>\n",
       "      <td>-0.218793</td>\n",
       "      <td>-0.072110</td>\n",
       "      <td>0.003770</td>\n",
       "      <td>-0.178679</td>\n",
       "      <td>0.153426</td>\n",
       "      <td>-0.194410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>0.123471</td>\n",
       "      <td>-0.168256</td>\n",
       "      <td>-0.152098</td>\n",
       "      <td>-0.087484</td>\n",
       "      <td>0.128232</td>\n",
       "      <td>-0.224098</td>\n",
       "      <td>-0.085233</td>\n",
       "      <td>-0.090171</td>\n",
       "      <td>-0.192667</td>\n",
       "      <td>-0.266065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015676</td>\n",
       "      <td>-0.026644</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>-0.035064</td>\n",
       "      <td>-0.189709</td>\n",
       "      <td>-0.439881</td>\n",
       "      <td>-0.005227</td>\n",
       "      <td>-0.032546</td>\n",
       "      <td>0.155606</td>\n",
       "      <td>-0.355877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>0.131011</td>\n",
       "      <td>0.342843</td>\n",
       "      <td>0.596099</td>\n",
       "      <td>0.202867</td>\n",
       "      <td>0.223004</td>\n",
       "      <td>-0.016649</td>\n",
       "      <td>0.321470</td>\n",
       "      <td>0.190107</td>\n",
       "      <td>0.207011</td>\n",
       "      <td>0.361827</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060877</td>\n",
       "      <td>0.168078</td>\n",
       "      <td>0.220438</td>\n",
       "      <td>0.111723</td>\n",
       "      <td>0.139578</td>\n",
       "      <td>0.417648</td>\n",
       "      <td>0.075763</td>\n",
       "      <td>0.080808</td>\n",
       "      <td>0.195932</td>\n",
       "      <td>0.333475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>0.034555</td>\n",
       "      <td>-0.226902</td>\n",
       "      <td>-0.164418</td>\n",
       "      <td>-0.241066</td>\n",
       "      <td>0.055689</td>\n",
       "      <td>-0.206608</td>\n",
       "      <td>-0.103901</td>\n",
       "      <td>-0.131636</td>\n",
       "      <td>-0.116755</td>\n",
       "      <td>-0.170313</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052735</td>\n",
       "      <td>-0.106495</td>\n",
       "      <td>-0.128169</td>\n",
       "      <td>-0.066085</td>\n",
       "      <td>-0.218923</td>\n",
       "      <td>-0.310986</td>\n",
       "      <td>0.008550</td>\n",
       "      <td>-0.215580</td>\n",
       "      <td>0.204296</td>\n",
       "      <td>-0.221844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>0.041023</td>\n",
       "      <td>-0.011790</td>\n",
       "      <td>-0.070826</td>\n",
       "      <td>-0.226190</td>\n",
       "      <td>-0.003742</td>\n",
       "      <td>-0.296224</td>\n",
       "      <td>-0.211133</td>\n",
       "      <td>0.175627</td>\n",
       "      <td>-0.053102</td>\n",
       "      <td>0.009704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063987</td>\n",
       "      <td>-0.191577</td>\n",
       "      <td>-0.185437</td>\n",
       "      <td>-0.185781</td>\n",
       "      <td>-0.227640</td>\n",
       "      <td>0.025232</td>\n",
       "      <td>-0.067466</td>\n",
       "      <td>-0.241634</td>\n",
       "      <td>0.153226</td>\n",
       "      <td>-0.276141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>-0.003264</td>\n",
       "      <td>0.911372</td>\n",
       "      <td>0.835040</td>\n",
       "      <td>0.437795</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.238675</td>\n",
       "      <td>0.488981</td>\n",
       "      <td>0.784584</td>\n",
       "      <td>0.738062</td>\n",
       "      <td>0.752201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181027</td>\n",
       "      <td>0.324669</td>\n",
       "      <td>0.317933</td>\n",
       "      <td>0.112152</td>\n",
       "      <td>0.430550</td>\n",
       "      <td>0.782488</td>\n",
       "      <td>0.265786</td>\n",
       "      <td>0.316984</td>\n",
       "      <td>0.123305</td>\n",
       "      <td>1.094509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3500 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           128       129       130       131       132       133       134  \\\n",
       "0     0.072717  1.151970  1.021654  0.343519  0.104696  0.095997  0.450215   \n",
       "1     0.099138 -0.176066 -0.082240 -0.012129  0.129237  0.122720 -0.010819   \n",
       "2     0.117899 -0.192835 -0.108160  0.137689  0.235482  0.061874  0.133768   \n",
       "3     0.217766  0.577409  0.301428  0.259764  0.202313  0.240055  0.282168   \n",
       "4     0.033859 -0.006845 -0.046622 -0.175963 -0.032874 -0.314592 -0.130444   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3495  0.123471 -0.168256 -0.152098 -0.087484  0.128232 -0.224098 -0.085233   \n",
       "3496  0.131011  0.342843  0.596099  0.202867  0.223004 -0.016649  0.321470   \n",
       "3497  0.034555 -0.226902 -0.164418 -0.241066  0.055689 -0.206608 -0.103901   \n",
       "3498  0.041023 -0.011790 -0.070826 -0.226190 -0.003742 -0.296224 -0.211133   \n",
       "3499 -0.003264  0.911372  0.835040  0.437795  0.002011  0.238675  0.488981   \n",
       "\n",
       "           135       136       137  ...       150       151       152  \\\n",
       "0     1.041780  0.799742  1.081851  ...  0.136313  0.271594  0.324329   \n",
       "1    -0.043584 -0.100474 -0.227882  ... -0.042621  0.020277  0.062644   \n",
       "2    -0.017286 -0.001746 -0.213690  ...  0.045991 -0.051799  0.374233   \n",
       "3     0.418546  0.568550  0.414526  ...  0.136881  0.262325  0.315605   \n",
       "4     0.221746  0.053102 -0.052906  ... -0.050461 -0.145454 -0.168152   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3495 -0.090171 -0.192667 -0.266065  ... -0.015676 -0.026644  0.003923   \n",
       "3496  0.190107  0.207011  0.361827  ... -0.060877  0.168078  0.220438   \n",
       "3497 -0.131636 -0.116755 -0.170313  ... -0.052735 -0.106495 -0.128169   \n",
       "3498  0.175627 -0.053102  0.009704  ... -0.063987 -0.191577 -0.185437   \n",
       "3499  0.784584  0.738062  0.752201  ...  0.181027  0.324669  0.317933   \n",
       "\n",
       "           153       154       155       156       157       158       159  \n",
       "0     0.152943  0.335057  1.076024  0.154671  0.249712  0.196861  1.280637  \n",
       "1     0.107495 -0.174846 -0.223387  0.233890 -0.038420  0.182939 -0.236878  \n",
       "2     0.309879  0.010004 -0.344870  0.179501 -0.026841  0.232073 -0.261810  \n",
       "3     0.119778  0.166527  0.237733  0.278135  0.247320  0.284798  0.516480  \n",
       "4    -0.129829 -0.218793 -0.072110  0.003770 -0.178679  0.153426 -0.194410  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3495 -0.035064 -0.189709 -0.439881 -0.005227 -0.032546  0.155606 -0.355877  \n",
       "3496  0.111723  0.139578  0.417648  0.075763  0.080808  0.195932  0.333475  \n",
       "3497 -0.066085 -0.218923 -0.310986  0.008550 -0.215580  0.204296 -0.221844  \n",
       "3498 -0.185781 -0.227640  0.025232 -0.067466 -0.241634  0.153226 -0.276141  \n",
       "3499  0.112152  0.430550  0.782488  0.265786  0.316984  0.123305  1.094509  \n",
       "\n",
       "[3500 rows x 32 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>...</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.239663</td>\n",
       "      <td>0.203053</td>\n",
       "      <td>0.211652</td>\n",
       "      <td>0.490150</td>\n",
       "      <td>0.202313</td>\n",
       "      <td>0.384219</td>\n",
       "      <td>0.543067</td>\n",
       "      <td>0.273852</td>\n",
       "      <td>0.129966</td>\n",
       "      <td>0.095189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224514</td>\n",
       "      <td>0.353526</td>\n",
       "      <td>0.518396</td>\n",
       "      <td>0.331150</td>\n",
       "      <td>0.513941</td>\n",
       "      <td>0.245423</td>\n",
       "      <td>0.353304</td>\n",
       "      <td>0.459796</td>\n",
       "      <td>0.288343</td>\n",
       "      <td>0.204407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.021298</td>\n",
       "      <td>0.820322</td>\n",
       "      <td>0.711301</td>\n",
       "      <td>0.065627</td>\n",
       "      <td>-0.096797</td>\n",
       "      <td>0.083673</td>\n",
       "      <td>0.108636</td>\n",
       "      <td>0.449139</td>\n",
       "      <td>0.581754</td>\n",
       "      <td>0.766175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138434</td>\n",
       "      <td>0.122817</td>\n",
       "      <td>0.125837</td>\n",
       "      <td>-0.020796</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.514368</td>\n",
       "      <td>0.084156</td>\n",
       "      <td>0.107572</td>\n",
       "      <td>0.233043</td>\n",
       "      <td>0.842164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.018290</td>\n",
       "      <td>0.432264</td>\n",
       "      <td>0.293386</td>\n",
       "      <td>0.246595</td>\n",
       "      <td>-0.029753</td>\n",
       "      <td>0.084518</td>\n",
       "      <td>0.268102</td>\n",
       "      <td>0.382796</td>\n",
       "      <td>0.350881</td>\n",
       "      <td>0.343281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203197</td>\n",
       "      <td>0.113513</td>\n",
       "      <td>0.173711</td>\n",
       "      <td>-0.080101</td>\n",
       "      <td>0.203387</td>\n",
       "      <td>0.227056</td>\n",
       "      <td>0.078559</td>\n",
       "      <td>0.148060</td>\n",
       "      <td>0.128828</td>\n",
       "      <td>0.330989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.074018</td>\n",
       "      <td>0.221001</td>\n",
       "      <td>0.197247</td>\n",
       "      <td>-0.027954</td>\n",
       "      <td>0.100513</td>\n",
       "      <td>-0.004311</td>\n",
       "      <td>0.068047</td>\n",
       "      <td>0.288493</td>\n",
       "      <td>0.141528</td>\n",
       "      <td>0.151269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047177</td>\n",
       "      <td>0.037650</td>\n",
       "      <td>0.053312</td>\n",
       "      <td>0.056165</td>\n",
       "      <td>-0.115582</td>\n",
       "      <td>0.219509</td>\n",
       "      <td>0.082625</td>\n",
       "      <td>0.019277</td>\n",
       "      <td>0.241543</td>\n",
       "      <td>0.239141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.132519</td>\n",
       "      <td>0.081886</td>\n",
       "      <td>-0.000803</td>\n",
       "      <td>-0.088258</td>\n",
       "      <td>0.145599</td>\n",
       "      <td>-0.160031</td>\n",
       "      <td>-0.077442</td>\n",
       "      <td>0.210066</td>\n",
       "      <td>0.266823</td>\n",
       "      <td>-0.014303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051277</td>\n",
       "      <td>0.020178</td>\n",
       "      <td>0.072508</td>\n",
       "      <td>0.032216</td>\n",
       "      <td>-0.071959</td>\n",
       "      <td>-0.140762</td>\n",
       "      <td>0.067219</td>\n",
       "      <td>0.064556</td>\n",
       "      <td>0.174991</td>\n",
       "      <td>-0.070311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>0.294620</td>\n",
       "      <td>0.227006</td>\n",
       "      <td>0.483461</td>\n",
       "      <td>0.169475</td>\n",
       "      <td>0.186013</td>\n",
       "      <td>0.390870</td>\n",
       "      <td>0.180025</td>\n",
       "      <td>0.198881</td>\n",
       "      <td>0.137480</td>\n",
       "      <td>0.409893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199587</td>\n",
       "      <td>0.174566</td>\n",
       "      <td>0.372471</td>\n",
       "      <td>0.310510</td>\n",
       "      <td>0.097748</td>\n",
       "      <td>0.189682</td>\n",
       "      <td>0.342038</td>\n",
       "      <td>0.321449</td>\n",
       "      <td>0.401212</td>\n",
       "      <td>0.243888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>-0.045011</td>\n",
       "      <td>-0.183046</td>\n",
       "      <td>-0.200494</td>\n",
       "      <td>-0.234413</td>\n",
       "      <td>-0.047414</td>\n",
       "      <td>-0.324323</td>\n",
       "      <td>-0.141153</td>\n",
       "      <td>-0.072773</td>\n",
       "      <td>0.020016</td>\n",
       "      <td>-0.280969</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031590</td>\n",
       "      <td>-0.138855</td>\n",
       "      <td>-0.147988</td>\n",
       "      <td>-0.155919</td>\n",
       "      <td>-0.243374</td>\n",
       "      <td>-0.435308</td>\n",
       "      <td>-0.026877</td>\n",
       "      <td>-0.253871</td>\n",
       "      <td>0.209793</td>\n",
       "      <td>-0.224932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>0.334240</td>\n",
       "      <td>0.526893</td>\n",
       "      <td>0.269908</td>\n",
       "      <td>0.572501</td>\n",
       "      <td>0.341670</td>\n",
       "      <td>0.550093</td>\n",
       "      <td>0.422654</td>\n",
       "      <td>0.247356</td>\n",
       "      <td>0.396279</td>\n",
       "      <td>0.415655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433305</td>\n",
       "      <td>0.435302</td>\n",
       "      <td>0.568159</td>\n",
       "      <td>0.479667</td>\n",
       "      <td>0.432182</td>\n",
       "      <td>0.225369</td>\n",
       "      <td>0.285250</td>\n",
       "      <td>0.628958</td>\n",
       "      <td>0.278636</td>\n",
       "      <td>0.338413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>0.110731</td>\n",
       "      <td>0.448698</td>\n",
       "      <td>0.280838</td>\n",
       "      <td>0.032660</td>\n",
       "      <td>0.179449</td>\n",
       "      <td>-0.010180</td>\n",
       "      <td>0.165796</td>\n",
       "      <td>0.374709</td>\n",
       "      <td>0.350158</td>\n",
       "      <td>0.315197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067899</td>\n",
       "      <td>0.082950</td>\n",
       "      <td>0.041644</td>\n",
       "      <td>0.049028</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.148909</td>\n",
       "      <td>0.074517</td>\n",
       "      <td>-0.008699</td>\n",
       "      <td>0.152234</td>\n",
       "      <td>0.185031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>0.083348</td>\n",
       "      <td>-0.008206</td>\n",
       "      <td>0.073813</td>\n",
       "      <td>-0.075465</td>\n",
       "      <td>0.034941</td>\n",
       "      <td>-0.108262</td>\n",
       "      <td>0.008327</td>\n",
       "      <td>0.084784</td>\n",
       "      <td>-0.045550</td>\n",
       "      <td>-0.097916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022221</td>\n",
       "      <td>0.028743</td>\n",
       "      <td>0.161923</td>\n",
       "      <td>0.060471</td>\n",
       "      <td>0.039111</td>\n",
       "      <td>-0.269837</td>\n",
       "      <td>0.107276</td>\n",
       "      <td>-0.069533</td>\n",
       "      <td>0.206048</td>\n",
       "      <td>-0.047981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2482 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           128       129       130       131       132       133       134  \\\n",
       "0     0.239663  0.203053  0.211652  0.490150  0.202313  0.384219  0.543067   \n",
       "1    -0.021298  0.820322  0.711301  0.065627 -0.096797  0.083673  0.108636   \n",
       "2    -0.018290  0.432264  0.293386  0.246595 -0.029753  0.084518  0.268102   \n",
       "3     0.074018  0.221001  0.197247 -0.027954  0.100513 -0.004311  0.068047   \n",
       "4     0.132519  0.081886 -0.000803 -0.088258  0.145599 -0.160031 -0.077442   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2477  0.294620  0.227006  0.483461  0.169475  0.186013  0.390870  0.180025   \n",
       "2478 -0.045011 -0.183046 -0.200494 -0.234413 -0.047414 -0.324323 -0.141153   \n",
       "2479  0.334240  0.526893  0.269908  0.572501  0.341670  0.550093  0.422654   \n",
       "2480  0.110731  0.448698  0.280838  0.032660  0.179449 -0.010180  0.165796   \n",
       "2481  0.083348 -0.008206  0.073813 -0.075465  0.034941 -0.108262  0.008327   \n",
       "\n",
       "           135       136       137  ...       150       151       152  \\\n",
       "0     0.273852  0.129966  0.095189  ...  0.224514  0.353526  0.518396   \n",
       "1     0.449139  0.581754  0.766175  ...  0.138434  0.122817  0.125837   \n",
       "2     0.382796  0.350881  0.343281  ...  0.203197  0.113513  0.173711   \n",
       "3     0.288493  0.141528  0.151269  ...  0.047177  0.037650  0.053312   \n",
       "4     0.210066  0.266823 -0.014303  ...  0.051277  0.020178  0.072508   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2477  0.198881  0.137480  0.409893  ...  0.199587  0.174566  0.372471   \n",
       "2478 -0.072773  0.020016 -0.280969  ... -0.031590 -0.138855 -0.147988   \n",
       "2479  0.247356  0.396279  0.415655  ...  0.433305  0.435302  0.568159   \n",
       "2480  0.374709  0.350158  0.315197  ...  0.067899  0.082950  0.041644   \n",
       "2481  0.084784 -0.045550 -0.097916  ... -0.022221  0.028743  0.161923   \n",
       "\n",
       "           153       154       155       156       157       158       159  \n",
       "0     0.331150  0.513941  0.245423  0.353304  0.459796  0.288343  0.204407  \n",
       "1    -0.020796  0.003384  0.514368  0.084156  0.107572  0.233043  0.842164  \n",
       "2    -0.080101  0.203387  0.227056  0.078559  0.148060  0.128828  0.330989  \n",
       "3     0.056165 -0.115582  0.219509  0.082625  0.019277  0.241543  0.239141  \n",
       "4     0.032216 -0.071959 -0.140762  0.067219  0.064556  0.174991 -0.070311  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2477  0.310510  0.097748  0.189682  0.342038  0.321449  0.401212  0.243888  \n",
       "2478 -0.155919 -0.243374 -0.435308 -0.026877 -0.253871  0.209793 -0.224932  \n",
       "2479  0.479667  0.432182  0.225369  0.285250  0.628958  0.278636  0.338413  \n",
       "2480  0.049028  0.055153  0.148909  0.074517 -0.008699  0.152234  0.185031  \n",
       "2481  0.060471  0.039111 -0.269837  0.107276 -0.069533  0.206048 -0.047981  \n",
       "\n",
       "[2482 rows x 32 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('X_test_cnn_hidden.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>...</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.072717</td>\n",
       "      <td>1.151970</td>\n",
       "      <td>1.021654</td>\n",
       "      <td>0.343519</td>\n",
       "      <td>0.104696</td>\n",
       "      <td>0.095997</td>\n",
       "      <td>0.450215</td>\n",
       "      <td>1.041780</td>\n",
       "      <td>0.799742</td>\n",
       "      <td>1.081851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136313</td>\n",
       "      <td>0.271594</td>\n",
       "      <td>0.324329</td>\n",
       "      <td>0.152943</td>\n",
       "      <td>0.335057</td>\n",
       "      <td>1.076024</td>\n",
       "      <td>0.154671</td>\n",
       "      <td>0.249712</td>\n",
       "      <td>0.196861</td>\n",
       "      <td>1.280637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.099138</td>\n",
       "      <td>-0.176066</td>\n",
       "      <td>-0.082240</td>\n",
       "      <td>-0.012129</td>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.122720</td>\n",
       "      <td>-0.010819</td>\n",
       "      <td>-0.043584</td>\n",
       "      <td>-0.100474</td>\n",
       "      <td>-0.227882</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042621</td>\n",
       "      <td>0.020277</td>\n",
       "      <td>0.062644</td>\n",
       "      <td>0.107495</td>\n",
       "      <td>-0.174846</td>\n",
       "      <td>-0.223387</td>\n",
       "      <td>0.233890</td>\n",
       "      <td>-0.038420</td>\n",
       "      <td>0.182939</td>\n",
       "      <td>-0.236878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117899</td>\n",
       "      <td>-0.192835</td>\n",
       "      <td>-0.108160</td>\n",
       "      <td>0.137689</td>\n",
       "      <td>0.235482</td>\n",
       "      <td>0.061874</td>\n",
       "      <td>0.133768</td>\n",
       "      <td>-0.017286</td>\n",
       "      <td>-0.001746</td>\n",
       "      <td>-0.213690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045991</td>\n",
       "      <td>-0.051799</td>\n",
       "      <td>0.374233</td>\n",
       "      <td>0.309879</td>\n",
       "      <td>0.010004</td>\n",
       "      <td>-0.344870</td>\n",
       "      <td>0.179501</td>\n",
       "      <td>-0.026841</td>\n",
       "      <td>0.232073</td>\n",
       "      <td>-0.261810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.217766</td>\n",
       "      <td>0.577409</td>\n",
       "      <td>0.301428</td>\n",
       "      <td>0.259764</td>\n",
       "      <td>0.202313</td>\n",
       "      <td>0.240055</td>\n",
       "      <td>0.282168</td>\n",
       "      <td>0.418546</td>\n",
       "      <td>0.568550</td>\n",
       "      <td>0.414526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136881</td>\n",
       "      <td>0.262325</td>\n",
       "      <td>0.315605</td>\n",
       "      <td>0.119778</td>\n",
       "      <td>0.166527</td>\n",
       "      <td>0.237733</td>\n",
       "      <td>0.278135</td>\n",
       "      <td>0.247320</td>\n",
       "      <td>0.284798</td>\n",
       "      <td>0.516480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.033859</td>\n",
       "      <td>-0.006845</td>\n",
       "      <td>-0.046622</td>\n",
       "      <td>-0.175963</td>\n",
       "      <td>-0.032874</td>\n",
       "      <td>-0.314592</td>\n",
       "      <td>-0.130444</td>\n",
       "      <td>0.221746</td>\n",
       "      <td>0.053102</td>\n",
       "      <td>-0.052906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050461</td>\n",
       "      <td>-0.145454</td>\n",
       "      <td>-0.168152</td>\n",
       "      <td>-0.129829</td>\n",
       "      <td>-0.218794</td>\n",
       "      <td>-0.072110</td>\n",
       "      <td>0.003770</td>\n",
       "      <td>-0.178679</td>\n",
       "      <td>0.153426</td>\n",
       "      <td>-0.194410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>0.123471</td>\n",
       "      <td>-0.168256</td>\n",
       "      <td>-0.152098</td>\n",
       "      <td>-0.087484</td>\n",
       "      <td>0.128232</td>\n",
       "      <td>-0.224098</td>\n",
       "      <td>-0.085233</td>\n",
       "      <td>-0.090171</td>\n",
       "      <td>-0.192667</td>\n",
       "      <td>-0.266065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015676</td>\n",
       "      <td>-0.026644</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>-0.035064</td>\n",
       "      <td>-0.189709</td>\n",
       "      <td>-0.439881</td>\n",
       "      <td>-0.005227</td>\n",
       "      <td>-0.032546</td>\n",
       "      <td>0.155606</td>\n",
       "      <td>-0.355877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>0.131011</td>\n",
       "      <td>0.342843</td>\n",
       "      <td>0.596099</td>\n",
       "      <td>0.202867</td>\n",
       "      <td>0.223004</td>\n",
       "      <td>-0.016649</td>\n",
       "      <td>0.321470</td>\n",
       "      <td>0.190107</td>\n",
       "      <td>0.207011</td>\n",
       "      <td>0.361828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060877</td>\n",
       "      <td>0.168078</td>\n",
       "      <td>0.220438</td>\n",
       "      <td>0.111723</td>\n",
       "      <td>0.139578</td>\n",
       "      <td>0.417648</td>\n",
       "      <td>0.075763</td>\n",
       "      <td>0.080808</td>\n",
       "      <td>0.195932</td>\n",
       "      <td>0.333475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>0.034555</td>\n",
       "      <td>-0.226902</td>\n",
       "      <td>-0.164418</td>\n",
       "      <td>-0.241066</td>\n",
       "      <td>0.055689</td>\n",
       "      <td>-0.206608</td>\n",
       "      <td>-0.103901</td>\n",
       "      <td>-0.131636</td>\n",
       "      <td>-0.116755</td>\n",
       "      <td>-0.170313</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052735</td>\n",
       "      <td>-0.106495</td>\n",
       "      <td>-0.128169</td>\n",
       "      <td>-0.066085</td>\n",
       "      <td>-0.218923</td>\n",
       "      <td>-0.310986</td>\n",
       "      <td>0.008550</td>\n",
       "      <td>-0.215580</td>\n",
       "      <td>0.204296</td>\n",
       "      <td>-0.221844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>0.041023</td>\n",
       "      <td>-0.011790</td>\n",
       "      <td>-0.070826</td>\n",
       "      <td>-0.226190</td>\n",
       "      <td>-0.003742</td>\n",
       "      <td>-0.296224</td>\n",
       "      <td>-0.211133</td>\n",
       "      <td>0.175627</td>\n",
       "      <td>-0.053102</td>\n",
       "      <td>0.009704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063987</td>\n",
       "      <td>-0.191577</td>\n",
       "      <td>-0.185437</td>\n",
       "      <td>-0.185781</td>\n",
       "      <td>-0.227640</td>\n",
       "      <td>0.025232</td>\n",
       "      <td>-0.067466</td>\n",
       "      <td>-0.241634</td>\n",
       "      <td>0.153226</td>\n",
       "      <td>-0.276141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>-0.003264</td>\n",
       "      <td>0.911372</td>\n",
       "      <td>0.835040</td>\n",
       "      <td>0.437795</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.238675</td>\n",
       "      <td>0.488981</td>\n",
       "      <td>0.784584</td>\n",
       "      <td>0.738062</td>\n",
       "      <td>0.752201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181027</td>\n",
       "      <td>0.324669</td>\n",
       "      <td>0.317933</td>\n",
       "      <td>0.112152</td>\n",
       "      <td>0.430550</td>\n",
       "      <td>0.782488</td>\n",
       "      <td>0.265786</td>\n",
       "      <td>0.316984</td>\n",
       "      <td>0.123305</td>\n",
       "      <td>1.094509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3500 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           128       129       130       131       132       133       134  \\\n",
       "0     0.072717  1.151970  1.021654  0.343519  0.104696  0.095997  0.450215   \n",
       "1     0.099138 -0.176066 -0.082240 -0.012129  0.129237  0.122720 -0.010819   \n",
       "2     0.117899 -0.192835 -0.108160  0.137689  0.235482  0.061874  0.133768   \n",
       "3     0.217766  0.577409  0.301428  0.259764  0.202313  0.240055  0.282168   \n",
       "4     0.033859 -0.006845 -0.046622 -0.175963 -0.032874 -0.314592 -0.130444   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3495  0.123471 -0.168256 -0.152098 -0.087484  0.128232 -0.224098 -0.085233   \n",
       "3496  0.131011  0.342843  0.596099  0.202867  0.223004 -0.016649  0.321470   \n",
       "3497  0.034555 -0.226902 -0.164418 -0.241066  0.055689 -0.206608 -0.103901   \n",
       "3498  0.041023 -0.011790 -0.070826 -0.226190 -0.003742 -0.296224 -0.211133   \n",
       "3499 -0.003264  0.911372  0.835040  0.437795  0.002011  0.238675  0.488981   \n",
       "\n",
       "           135       136       137  ...       150       151       152  \\\n",
       "0     1.041780  0.799742  1.081851  ...  0.136313  0.271594  0.324329   \n",
       "1    -0.043584 -0.100474 -0.227882  ... -0.042621  0.020277  0.062644   \n",
       "2    -0.017286 -0.001746 -0.213690  ...  0.045991 -0.051799  0.374233   \n",
       "3     0.418546  0.568550  0.414526  ...  0.136881  0.262325  0.315605   \n",
       "4     0.221746  0.053102 -0.052906  ... -0.050461 -0.145454 -0.168152   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3495 -0.090171 -0.192667 -0.266065  ... -0.015676 -0.026644  0.003923   \n",
       "3496  0.190107  0.207011  0.361828  ... -0.060877  0.168078  0.220438   \n",
       "3497 -0.131636 -0.116755 -0.170313  ... -0.052735 -0.106495 -0.128169   \n",
       "3498  0.175627 -0.053102  0.009704  ... -0.063987 -0.191577 -0.185437   \n",
       "3499  0.784584  0.738062  0.752201  ...  0.181027  0.324669  0.317933   \n",
       "\n",
       "           153       154       155       156       157       158       159  \n",
       "0     0.152943  0.335057  1.076024  0.154671  0.249712  0.196861  1.280637  \n",
       "1     0.107495 -0.174846 -0.223387  0.233890 -0.038420  0.182939 -0.236878  \n",
       "2     0.309879  0.010004 -0.344870  0.179501 -0.026841  0.232073 -0.261810  \n",
       "3     0.119778  0.166527  0.237733  0.278135  0.247320  0.284798  0.516480  \n",
       "4    -0.129829 -0.218794 -0.072110  0.003770 -0.178679  0.153426 -0.194410  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3495 -0.035064 -0.189709 -0.439881 -0.005227 -0.032546  0.155606 -0.355877  \n",
       "3496  0.111723  0.139578  0.417648  0.075763  0.080808  0.195932  0.333475  \n",
       "3497 -0.066085 -0.218923 -0.310986  0.008550 -0.215580  0.204296 -0.221844  \n",
       "3498 -0.185781 -0.227640  0.025232 -0.067466 -0.241634  0.153226 -0.276141  \n",
       "3499  0.112152  0.430550  0.782488  0.265786  0.316984  0.123305  1.094509  \n",
       "\n",
       "[3500 rows x 32 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('X_train_cnn_hidden.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'conv1d_mul_submit.csv' is ready to submit.\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([X_test_low, X_test_mid, X_test_high, X_test_str, X_test_day])[:,0]\n",
    "fname = 'conv1d_mul_submit.csv'\n",
    "submissions = pd.concat([pd.Series(IDtest, name=\"cust_id\"), pd.Series(pred, name=\"gender\")] ,axis=1)\n",
    "submissions.to_csv(fname, index=False)\n",
    "print(\"'{}' is ready to submit.\" .format(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
