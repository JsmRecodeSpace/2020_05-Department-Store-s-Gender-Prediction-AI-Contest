{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run import_modules.py  \n",
    "%matplotlib inline\n",
    "\n",
    "# For DNN modeling\n",
    "import tensorflow as tf\n",
    "\n",
    "# Tensorflow warning off\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "tf.random.set_seed(2020)\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import * #Input, Dense\n",
    "from keras.models import * #Model\n",
    "from keras.optimizers import *\n",
    "from keras.initializers import *\n",
    "from keras.regularizers import *\n",
    "from keras.utils.np_utils import *\n",
    "from keras.utils.vis_utils import * #model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 중분류 구매건수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.625380</td>\n",
       "      <td>0.034396</td>\n",
       "      <td>-0.672246</td>\n",
       "      <td>0.441779</td>\n",
       "      <td>0.159294</td>\n",
       "      <td>-0.598718</td>\n",
       "      <td>0.325886</td>\n",
       "      <td>-0.936302</td>\n",
       "      <td>0.305429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143142</td>\n",
       "      <td>0.118564</td>\n",
       "      <td>-0.002334</td>\n",
       "      <td>0.089901</td>\n",
       "      <td>-0.111054</td>\n",
       "      <td>-0.021497</td>\n",
       "      <td>0.140365</td>\n",
       "      <td>0.117912</td>\n",
       "      <td>-0.126485</td>\n",
       "      <td>0.046712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.107635</td>\n",
       "      <td>-0.008147</td>\n",
       "      <td>-0.036582</td>\n",
       "      <td>-0.227227</td>\n",
       "      <td>-0.173067</td>\n",
       "      <td>0.257546</td>\n",
       "      <td>0.036107</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>-0.073846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030561</td>\n",
       "      <td>-0.018014</td>\n",
       "      <td>0.032755</td>\n",
       "      <td>0.015114</td>\n",
       "      <td>0.011464</td>\n",
       "      <td>-0.002095</td>\n",
       "      <td>-0.073120</td>\n",
       "      <td>-0.051948</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>-0.007939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.099033</td>\n",
       "      <td>-0.109059</td>\n",
       "      <td>-0.020829</td>\n",
       "      <td>0.007957</td>\n",
       "      <td>-0.059710</td>\n",
       "      <td>0.090729</td>\n",
       "      <td>0.179059</td>\n",
       "      <td>-0.068382</td>\n",
       "      <td>0.062943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.035945</td>\n",
       "      <td>-0.017813</td>\n",
       "      <td>-0.008323</td>\n",
       "      <td>-0.036238</td>\n",
       "      <td>0.021623</td>\n",
       "      <td>0.038809</td>\n",
       "      <td>0.046284</td>\n",
       "      <td>0.007039</td>\n",
       "      <td>0.014445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.265591</td>\n",
       "      <td>-0.619277</td>\n",
       "      <td>-0.279149</td>\n",
       "      <td>-0.187955</td>\n",
       "      <td>0.023135</td>\n",
       "      <td>-0.135279</td>\n",
       "      <td>-0.067050</td>\n",
       "      <td>0.675716</td>\n",
       "      <td>-0.159730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131376</td>\n",
       "      <td>0.064030</td>\n",
       "      <td>-0.118500</td>\n",
       "      <td>0.066814</td>\n",
       "      <td>-0.149491</td>\n",
       "      <td>-0.003301</td>\n",
       "      <td>0.486243</td>\n",
       "      <td>-0.482798</td>\n",
       "      <td>-0.028555</td>\n",
       "      <td>-0.156546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.141518</td>\n",
       "      <td>-0.191579</td>\n",
       "      <td>0.005533</td>\n",
       "      <td>-0.059629</td>\n",
       "      <td>0.026487</td>\n",
       "      <td>0.013643</td>\n",
       "      <td>-0.095080</td>\n",
       "      <td>0.043151</td>\n",
       "      <td>-0.057431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008822</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>-0.005974</td>\n",
       "      <td>-0.006765</td>\n",
       "      <td>0.007707</td>\n",
       "      <td>0.013415</td>\n",
       "      <td>-0.002152</td>\n",
       "      <td>0.008844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-0.877498</td>\n",
       "      <td>0.236507</td>\n",
       "      <td>-0.127787</td>\n",
       "      <td>-0.066931</td>\n",
       "      <td>-0.135133</td>\n",
       "      <td>0.047662</td>\n",
       "      <td>0.136196</td>\n",
       "      <td>-0.051518</td>\n",
       "      <td>-0.006189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100014</td>\n",
       "      <td>-0.477439</td>\n",
       "      <td>-0.028659</td>\n",
       "      <td>0.204758</td>\n",
       "      <td>-0.015639</td>\n",
       "      <td>0.083038</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>-0.005576</td>\n",
       "      <td>-0.006840</td>\n",
       "      <td>0.081172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.162111</td>\n",
       "      <td>-0.159106</td>\n",
       "      <td>0.019576</td>\n",
       "      <td>0.078777</td>\n",
       "      <td>0.026611</td>\n",
       "      <td>-0.007421</td>\n",
       "      <td>-0.020261</td>\n",
       "      <td>0.025530</td>\n",
       "      <td>-0.021468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005455</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>0.011396</td>\n",
       "      <td>0.006313</td>\n",
       "      <td>-0.050464</td>\n",
       "      <td>-0.032066</td>\n",
       "      <td>-0.036101</td>\n",
       "      <td>-0.007855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>0.199450</td>\n",
       "      <td>1.781264</td>\n",
       "      <td>-0.540263</td>\n",
       "      <td>0.071173</td>\n",
       "      <td>0.331206</td>\n",
       "      <td>1.250888</td>\n",
       "      <td>0.351207</td>\n",
       "      <td>-0.141133</td>\n",
       "      <td>-0.117322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100453</td>\n",
       "      <td>0.028486</td>\n",
       "      <td>-0.128008</td>\n",
       "      <td>0.088439</td>\n",
       "      <td>-0.112445</td>\n",
       "      <td>-0.079405</td>\n",
       "      <td>0.109667</td>\n",
       "      <td>0.038992</td>\n",
       "      <td>-0.088774</td>\n",
       "      <td>0.054421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.635700</td>\n",
       "      <td>-0.227858</td>\n",
       "      <td>-0.237216</td>\n",
       "      <td>0.009492</td>\n",
       "      <td>-0.166560</td>\n",
       "      <td>-0.074526</td>\n",
       "      <td>0.129139</td>\n",
       "      <td>-0.274902</td>\n",
       "      <td>-0.380003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.285796</td>\n",
       "      <td>0.651523</td>\n",
       "      <td>0.113559</td>\n",
       "      <td>-0.241631</td>\n",
       "      <td>0.065565</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>-0.040256</td>\n",
       "      <td>-0.085648</td>\n",
       "      <td>-0.044685</td>\n",
       "      <td>-0.053650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.674396</td>\n",
       "      <td>-0.398007</td>\n",
       "      <td>0.607995</td>\n",
       "      <td>-0.003883</td>\n",
       "      <td>-0.024185</td>\n",
       "      <td>0.051853</td>\n",
       "      <td>0.060919</td>\n",
       "      <td>0.352332</td>\n",
       "      <td>0.488067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256775</td>\n",
       "      <td>0.148656</td>\n",
       "      <td>-0.043532</td>\n",
       "      <td>-0.055591</td>\n",
       "      <td>-0.132109</td>\n",
       "      <td>-0.018838</td>\n",
       "      <td>0.036651</td>\n",
       "      <td>0.041369</td>\n",
       "      <td>0.035976</td>\n",
       "      <td>-0.103166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  0.625380  0.034396 -0.672246  0.441779  0.159294 -0.598718   \n",
       "1           1 -1.107635 -0.008147 -0.036582 -0.227227 -0.173067  0.257546   \n",
       "2           2 -1.099033 -0.109059 -0.020829  0.007957 -0.059710  0.090729   \n",
       "3           3  0.265591 -0.619277 -0.279149 -0.187955  0.023135 -0.135279   \n",
       "4           4 -1.141518 -0.191579  0.005533 -0.059629  0.026487  0.013643   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -0.877498  0.236507 -0.127787 -0.066931 -0.135133  0.047662   \n",
       "5978     5978 -1.162111 -0.159106  0.019576  0.078777  0.026611 -0.007421   \n",
       "5979     5979  0.199450  1.781264 -0.540263  0.071173  0.331206  1.250888   \n",
       "5980     5980 -0.635700 -0.227858 -0.237216  0.009492 -0.166560 -0.074526   \n",
       "5981     5981 -0.674396 -0.398007  0.607995 -0.003883 -0.024185  0.051853   \n",
       "\n",
       "             6         7         8  ...       100       101       102  \\\n",
       "0     0.325886 -0.936302  0.305429  ... -0.143142  0.118564 -0.002334   \n",
       "1     0.036107 -0.059078 -0.073846  ...  0.030561 -0.018014  0.032755   \n",
       "2     0.179059 -0.068382  0.062943  ...  0.002262  0.035945 -0.017813   \n",
       "3    -0.067050  0.675716 -0.159730  ...  0.131376  0.064030 -0.118500   \n",
       "4    -0.095080  0.043151 -0.057431  ...  0.008822  0.004029  0.004533   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977  0.136196 -0.051518 -0.006189  ...  0.100014 -0.477439 -0.028659   \n",
       "5978 -0.020261  0.025530 -0.021468  ...  0.005455  0.005500  0.013200   \n",
       "5979  0.351207 -0.141133 -0.117322  ...  0.100453  0.028486 -0.128008   \n",
       "5980  0.129139 -0.274902 -0.380003  ... -0.285796  0.651523  0.113559   \n",
       "5981  0.060919  0.352332  0.488067  ...  0.256775  0.148656 -0.043532   \n",
       "\n",
       "           103       104       105       106       107       108       109  \n",
       "0     0.089901 -0.111054 -0.021497  0.140365  0.117912 -0.126485  0.046712  \n",
       "1     0.015114  0.011464 -0.002095 -0.073120 -0.051948  0.005610 -0.007939  \n",
       "2    -0.008323 -0.036238  0.021623  0.038809  0.046284  0.007039  0.014445  \n",
       "3     0.066814 -0.149491 -0.003301  0.486243 -0.482798 -0.028555 -0.156546  \n",
       "4    -0.000862 -0.005974 -0.006765  0.007707  0.013415 -0.002152  0.008844  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977  0.204758 -0.015639  0.083038  0.010453 -0.005576 -0.006840  0.081172  \n",
       "5978  0.006884  0.011396  0.006313 -0.050464 -0.032066 -0.036101 -0.007855  \n",
       "5979  0.088439 -0.112445 -0.079405  0.109667  0.038992 -0.088774  0.054421  \n",
       "5980 -0.241631  0.065565  0.000818 -0.040256 -0.085648 -0.044685 -0.053650  \n",
       "5981 -0.055591 -0.132109 -0.018838  0.036651  0.041369  0.035976 -0.103166  \n",
       "\n",
       "[5982 rows x 111 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                            aggfunc=lambda x: len(x), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "\n",
    "seed = 2020\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7325722824576035\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "# model architecture\n",
    "model = Sequential(name = 'dnn model')\n",
    "model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# choose the optimizer and the cost function\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# train the model -> verbose=0: silent\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=25)]\n",
    "\n",
    "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "    batch_size=64, epochs=200, callbacks=callbacks, verbose=0)\n",
    "\n",
    "# visualize training history\n",
    "# plt.plot(hist.history['loss'], label='train loss')\n",
    "# plt.plot(hist.history['val_loss'], label='validation loss')\n",
    "# plt.legend()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.title('Loss')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.plot(hist.history['acc'], label='train acc')\n",
    "# plt.plot(hist.history['val_acc'], label='validation acc')\n",
    "# plt.legend()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.title('acc')\n",
    "# plt.show()\n",
    "\n",
    "# evaluate the model performance\n",
    "\n",
    "#print(model.evaluate(X_test, y_test))\n",
    "#if roc_auc_score(y_test, model.predict(X_test)) >= 0.755:\n",
    "#    print(f'1층: {dr1},드롭1: {drop1},2층: {dr2}, 드롭2: {drop2}, 3층: {dr3}')\n",
    "#    print(roc_auc_score(y_test, model.predict(X_test)))\n",
    "print(roc_auc_score(y_valid, model.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 대분류 구매건수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.808812</td>\n",
       "      <td>0.047621</td>\n",
       "      <td>-0.382530</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>0.068270</td>\n",
       "      <td>-1.067826</td>\n",
       "      <td>0.097073</td>\n",
       "      <td>0.064659</td>\n",
       "      <td>0.123847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278258</td>\n",
       "      <td>-0.198159</td>\n",
       "      <td>-0.226099</td>\n",
       "      <td>-0.038080</td>\n",
       "      <td>0.324296</td>\n",
       "      <td>-0.434571</td>\n",
       "      <td>-0.308631</td>\n",
       "      <td>-0.016363</td>\n",
       "      <td>0.194200</td>\n",
       "      <td>-0.018975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.942326</td>\n",
       "      <td>0.181016</td>\n",
       "      <td>0.039485</td>\n",
       "      <td>0.224548</td>\n",
       "      <td>0.165447</td>\n",
       "      <td>0.185474</td>\n",
       "      <td>0.260437</td>\n",
       "      <td>0.067577</td>\n",
       "      <td>-0.233817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035076</td>\n",
       "      <td>-0.147834</td>\n",
       "      <td>0.062239</td>\n",
       "      <td>0.060819</td>\n",
       "      <td>-0.054463</td>\n",
       "      <td>-0.387346</td>\n",
       "      <td>0.112056</td>\n",
       "      <td>-0.059185</td>\n",
       "      <td>0.006323</td>\n",
       "      <td>-0.030976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.939487</td>\n",
       "      <td>0.072728</td>\n",
       "      <td>-0.171136</td>\n",
       "      <td>0.257513</td>\n",
       "      <td>-0.163384</td>\n",
       "      <td>-0.015842</td>\n",
       "      <td>-0.109646</td>\n",
       "      <td>0.098185</td>\n",
       "      <td>-0.189282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057341</td>\n",
       "      <td>0.047966</td>\n",
       "      <td>-0.038936</td>\n",
       "      <td>-0.112071</td>\n",
       "      <td>-0.006490</td>\n",
       "      <td>0.062245</td>\n",
       "      <td>-0.004990</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>0.044459</td>\n",
       "      <td>0.039179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.176414</td>\n",
       "      <td>-0.540114</td>\n",
       "      <td>-0.095850</td>\n",
       "      <td>0.039596</td>\n",
       "      <td>0.054163</td>\n",
       "      <td>0.377580</td>\n",
       "      <td>0.169790</td>\n",
       "      <td>-0.019942</td>\n",
       "      <td>0.183439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>-0.014416</td>\n",
       "      <td>-0.061704</td>\n",
       "      <td>0.120444</td>\n",
       "      <td>-0.374014</td>\n",
       "      <td>0.177117</td>\n",
       "      <td>-0.243579</td>\n",
       "      <td>0.075744</td>\n",
       "      <td>-0.142311</td>\n",
       "      <td>-0.229687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.006475</td>\n",
       "      <td>-0.015659</td>\n",
       "      <td>0.083409</td>\n",
       "      <td>-0.036429</td>\n",
       "      <td>0.082568</td>\n",
       "      <td>-0.020770</td>\n",
       "      <td>0.012661</td>\n",
       "      <td>-0.029358</td>\n",
       "      <td>0.030033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008659</td>\n",
       "      <td>0.040425</td>\n",
       "      <td>-0.007492</td>\n",
       "      <td>0.042274</td>\n",
       "      <td>0.049531</td>\n",
       "      <td>-0.034650</td>\n",
       "      <td>-0.017044</td>\n",
       "      <td>-0.018064</td>\n",
       "      <td>0.021210</td>\n",
       "      <td>-0.000362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-0.527805</td>\n",
       "      <td>0.460797</td>\n",
       "      <td>-0.064967</td>\n",
       "      <td>-0.029257</td>\n",
       "      <td>0.232430</td>\n",
       "      <td>-0.141500</td>\n",
       "      <td>0.104384</td>\n",
       "      <td>0.065651</td>\n",
       "      <td>-0.706434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.477599</td>\n",
       "      <td>0.215720</td>\n",
       "      <td>0.023805</td>\n",
       "      <td>0.367133</td>\n",
       "      <td>0.026163</td>\n",
       "      <td>0.424302</td>\n",
       "      <td>-0.089105</td>\n",
       "      <td>-0.048746</td>\n",
       "      <td>0.062714</td>\n",
       "      <td>-0.055811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.038196</td>\n",
       "      <td>0.022598</td>\n",
       "      <td>0.023371</td>\n",
       "      <td>-0.003908</td>\n",
       "      <td>0.008111</td>\n",
       "      <td>0.030509</td>\n",
       "      <td>-0.068062</td>\n",
       "      <td>-0.030968</td>\n",
       "      <td>0.011029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106910</td>\n",
       "      <td>0.017061</td>\n",
       "      <td>0.158339</td>\n",
       "      <td>-0.014808</td>\n",
       "      <td>-0.045128</td>\n",
       "      <td>0.014905</td>\n",
       "      <td>-0.012643</td>\n",
       "      <td>-0.058065</td>\n",
       "      <td>0.033216</td>\n",
       "      <td>-0.046763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>0.672151</td>\n",
       "      <td>1.288926</td>\n",
       "      <td>0.047105</td>\n",
       "      <td>0.442555</td>\n",
       "      <td>-0.225962</td>\n",
       "      <td>0.620944</td>\n",
       "      <td>-0.542938</td>\n",
       "      <td>-0.306575</td>\n",
       "      <td>-0.724191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267473</td>\n",
       "      <td>0.172990</td>\n",
       "      <td>0.246498</td>\n",
       "      <td>-0.123809</td>\n",
       "      <td>-0.203993</td>\n",
       "      <td>-0.005225</td>\n",
       "      <td>-0.029028</td>\n",
       "      <td>0.124641</td>\n",
       "      <td>0.155520</td>\n",
       "      <td>0.100988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.561052</td>\n",
       "      <td>-0.015907</td>\n",
       "      <td>0.100263</td>\n",
       "      <td>0.082922</td>\n",
       "      <td>-0.023670</td>\n",
       "      <td>0.353346</td>\n",
       "      <td>0.424770</td>\n",
       "      <td>-0.039177</td>\n",
       "      <td>0.215357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149904</td>\n",
       "      <td>-0.020741</td>\n",
       "      <td>-0.158265</td>\n",
       "      <td>0.161862</td>\n",
       "      <td>-0.201403</td>\n",
       "      <td>-0.103223</td>\n",
       "      <td>-0.567873</td>\n",
       "      <td>-0.410687</td>\n",
       "      <td>-0.080075</td>\n",
       "      <td>0.002968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.804268</td>\n",
       "      <td>-0.300905</td>\n",
       "      <td>-0.010156</td>\n",
       "      <td>0.041940</td>\n",
       "      <td>0.061396</td>\n",
       "      <td>0.123361</td>\n",
       "      <td>-0.009046</td>\n",
       "      <td>-0.017363</td>\n",
       "      <td>-0.044154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064577</td>\n",
       "      <td>0.030567</td>\n",
       "      <td>-0.091283</td>\n",
       "      <td>0.040444</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>-0.027214</td>\n",
       "      <td>-0.019029</td>\n",
       "      <td>0.156810</td>\n",
       "      <td>0.123323</td>\n",
       "      <td>0.190463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  0.808812  0.047621 -0.382530  0.136842  0.068270 -1.067826   \n",
       "1           1 -0.942326  0.181016  0.039485  0.224548  0.165447  0.185474   \n",
       "2           2 -0.939487  0.072728 -0.171136  0.257513 -0.163384 -0.015842   \n",
       "3           3  0.176414 -0.540114 -0.095850  0.039596  0.054163  0.377580   \n",
       "4           4 -1.006475 -0.015659  0.083409 -0.036429  0.082568 -0.020770   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -0.527805  0.460797 -0.064967 -0.029257  0.232430 -0.141500   \n",
       "5978     5978 -1.038196  0.022598  0.023371 -0.003908  0.008111  0.030509   \n",
       "5979     5979  0.672151  1.288926  0.047105  0.442555 -0.225962  0.620944   \n",
       "5980     5980 -0.561052 -0.015907  0.100263  0.082922 -0.023670  0.353346   \n",
       "5981     5981 -0.804268 -0.300905 -0.010156  0.041940  0.061396  0.123361   \n",
       "\n",
       "             6         7         8  ...        25        26        27  \\\n",
       "0     0.097073  0.064659  0.123847  ...  0.278258 -0.198159 -0.226099   \n",
       "1     0.260437  0.067577 -0.233817  ... -0.035076 -0.147834  0.062239   \n",
       "2    -0.109646  0.098185 -0.189282  ...  0.057341  0.047966 -0.038936   \n",
       "3     0.169790 -0.019942  0.183439  ...  0.156863 -0.014416 -0.061704   \n",
       "4     0.012661 -0.029358  0.030033  ...  0.008659  0.040425 -0.007492   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977  0.104384  0.065651 -0.706434  ... -0.477599  0.215720  0.023805   \n",
       "5978 -0.068062 -0.030968  0.011029  ...  0.106910  0.017061  0.158339   \n",
       "5979 -0.542938 -0.306575 -0.724191  ... -0.267473  0.172990  0.246498   \n",
       "5980  0.424770 -0.039177  0.215357  ...  0.149904 -0.020741 -0.158265   \n",
       "5981 -0.009046 -0.017363 -0.044154  ... -0.064577  0.030567 -0.091283   \n",
       "\n",
       "            28        29        30        31        32        33        34  \n",
       "0    -0.038080  0.324296 -0.434571 -0.308631 -0.016363  0.194200 -0.018975  \n",
       "1     0.060819 -0.054463 -0.387346  0.112056 -0.059185  0.006323 -0.030976  \n",
       "2    -0.112071 -0.006490  0.062245 -0.004990  0.035366  0.044459  0.039179  \n",
       "3     0.120444 -0.374014  0.177117 -0.243579  0.075744 -0.142311 -0.229687  \n",
       "4     0.042274  0.049531 -0.034650 -0.017044 -0.018064  0.021210 -0.000362  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977  0.367133  0.026163  0.424302 -0.089105 -0.048746  0.062714 -0.055811  \n",
       "5978 -0.014808 -0.045128  0.014905 -0.012643 -0.058065  0.033216 -0.046763  \n",
       "5979 -0.123809 -0.203993 -0.005225 -0.029028  0.124641  0.155520  0.100988  \n",
       "5980  0.161862 -0.201403 -0.103223 -0.567873 -0.410687 -0.080075  0.002968  \n",
       "5981  0.040444  0.000616 -0.027214 -0.019029  0.156810  0.123323  0.190463  \n",
       "\n",
       "[5982 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_mclas_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                            aggfunc=lambda x: len(x), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "\n",
    "seed = 2020\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7503127606338615\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "# model architecture\n",
    "model = Sequential(name = 'dnn model')\n",
    "model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# choose the optimizer and the cost function\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# train the model -> verbose=0: silent\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=25)]\n",
    "\n",
    "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "    batch_size=64, epochs=200, callbacks=callbacks, verbose=0)\n",
    "\n",
    "# visualize training history\n",
    "# plt.plot(hist.history['loss'], label='train loss')\n",
    "# plt.plot(hist.history['val_loss'], label='validation loss')\n",
    "# plt.legend()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.title('Loss')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.plot(hist.history['acc'], label='train acc')\n",
    "# plt.plot(hist.history['val_acc'], label='validation acc')\n",
    "# plt.legend()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.title('acc')\n",
    "# plt.show()\n",
    "\n",
    "# evaluate the model performance\n",
    "\n",
    "#print(model.evaluate(X_test, y_test))\n",
    "#if roc_auc_score(y_test, model.predict(X_test)) >= 0.755:\n",
    "#    print(f'1층: {dr1},드롭1: {drop1},2층: {dr2}, 드롭2: {drop2}, 3층: {dr3}')\n",
    "#    print(roc_auc_score(y_test, model.predict(X_test)))\n",
    "print(roc_auc_score(y_valid, model.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 중분류 구매여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.031835</td>\n",
       "      <td>0.212301</td>\n",
       "      <td>-0.922307</td>\n",
       "      <td>-0.869803</td>\n",
       "      <td>-0.305008</td>\n",
       "      <td>0.092058</td>\n",
       "      <td>0.831769</td>\n",
       "      <td>-0.575745</td>\n",
       "      <td>-0.703853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102421</td>\n",
       "      <td>-0.318619</td>\n",
       "      <td>-0.375416</td>\n",
       "      <td>-0.050509</td>\n",
       "      <td>0.185363</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>-0.122237</td>\n",
       "      <td>-0.086713</td>\n",
       "      <td>-0.053711</td>\n",
       "      <td>-0.077012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.545397</td>\n",
       "      <td>-0.120407</td>\n",
       "      <td>0.028675</td>\n",
       "      <td>0.202452</td>\n",
       "      <td>0.264835</td>\n",
       "      <td>0.240070</td>\n",
       "      <td>0.129344</td>\n",
       "      <td>0.058812</td>\n",
       "      <td>0.209729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028266</td>\n",
       "      <td>0.074902</td>\n",
       "      <td>0.029886</td>\n",
       "      <td>0.116798</td>\n",
       "      <td>-0.031030</td>\n",
       "      <td>-0.049614</td>\n",
       "      <td>0.024517</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.006372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.415123</td>\n",
       "      <td>-0.065534</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>-0.016653</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0.400511</td>\n",
       "      <td>-0.053878</td>\n",
       "      <td>-0.125965</td>\n",
       "      <td>-0.270389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>-0.011082</td>\n",
       "      <td>-0.020539</td>\n",
       "      <td>-0.010853</td>\n",
       "      <td>-0.012236</td>\n",
       "      <td>-0.003550</td>\n",
       "      <td>-0.000766</td>\n",
       "      <td>0.007166</td>\n",
       "      <td>0.015099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.865498</td>\n",
       "      <td>-1.222110</td>\n",
       "      <td>-0.553205</td>\n",
       "      <td>0.022980</td>\n",
       "      <td>0.309707</td>\n",
       "      <td>-0.115284</td>\n",
       "      <td>-0.423851</td>\n",
       "      <td>0.164560</td>\n",
       "      <td>0.401825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122099</td>\n",
       "      <td>0.050478</td>\n",
       "      <td>0.119724</td>\n",
       "      <td>-0.141897</td>\n",
       "      <td>-0.014390</td>\n",
       "      <td>0.095265</td>\n",
       "      <td>-0.023132</td>\n",
       "      <td>0.142257</td>\n",
       "      <td>0.325156</td>\n",
       "      <td>0.171970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.341069</td>\n",
       "      <td>-0.078229</td>\n",
       "      <td>-0.045112</td>\n",
       "      <td>0.219325</td>\n",
       "      <td>-0.030194</td>\n",
       "      <td>-0.500378</td>\n",
       "      <td>0.017921</td>\n",
       "      <td>0.145940</td>\n",
       "      <td>-0.016312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012135</td>\n",
       "      <td>0.031032</td>\n",
       "      <td>-0.017324</td>\n",
       "      <td>-0.006011</td>\n",
       "      <td>0.007325</td>\n",
       "      <td>-0.018732</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>-0.010431</td>\n",
       "      <td>-0.004584</td>\n",
       "      <td>-0.006360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-1.161452</td>\n",
       "      <td>0.308622</td>\n",
       "      <td>-0.139144</td>\n",
       "      <td>-0.117040</td>\n",
       "      <td>0.072620</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>-0.058142</td>\n",
       "      <td>-0.208458</td>\n",
       "      <td>-0.120671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004387</td>\n",
       "      <td>-0.135064</td>\n",
       "      <td>0.031170</td>\n",
       "      <td>-0.002395</td>\n",
       "      <td>-0.009385</td>\n",
       "      <td>0.033274</td>\n",
       "      <td>0.079296</td>\n",
       "      <td>0.230744</td>\n",
       "      <td>-0.209229</td>\n",
       "      <td>-0.247969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.547982</td>\n",
       "      <td>-0.166368</td>\n",
       "      <td>0.062790</td>\n",
       "      <td>-0.058637</td>\n",
       "      <td>-0.039730</td>\n",
       "      <td>0.145217</td>\n",
       "      <td>0.182716</td>\n",
       "      <td>0.213168</td>\n",
       "      <td>0.241474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017176</td>\n",
       "      <td>-0.011808</td>\n",
       "      <td>-0.000613</td>\n",
       "      <td>-0.003491</td>\n",
       "      <td>-0.025970</td>\n",
       "      <td>-0.001241</td>\n",
       "      <td>0.008654</td>\n",
       "      <td>-0.008849</td>\n",
       "      <td>-0.015761</td>\n",
       "      <td>-0.011166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>0.204833</td>\n",
       "      <td>1.663809</td>\n",
       "      <td>-0.728419</td>\n",
       "      <td>0.605944</td>\n",
       "      <td>0.811258</td>\n",
       "      <td>1.091609</td>\n",
       "      <td>-0.392112</td>\n",
       "      <td>1.131950</td>\n",
       "      <td>-0.592724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523407</td>\n",
       "      <td>0.207549</td>\n",
       "      <td>-0.355611</td>\n",
       "      <td>-0.084467</td>\n",
       "      <td>0.018351</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.144681</td>\n",
       "      <td>0.012927</td>\n",
       "      <td>0.108045</td>\n",
       "      <td>-0.044070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.925491</td>\n",
       "      <td>-0.298534</td>\n",
       "      <td>-0.188338</td>\n",
       "      <td>-0.024190</td>\n",
       "      <td>0.231828</td>\n",
       "      <td>0.287504</td>\n",
       "      <td>0.755301</td>\n",
       "      <td>-0.127350</td>\n",
       "      <td>0.329944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028420</td>\n",
       "      <td>-0.098941</td>\n",
       "      <td>-0.130566</td>\n",
       "      <td>0.075247</td>\n",
       "      <td>-0.050180</td>\n",
       "      <td>0.088172</td>\n",
       "      <td>-0.168739</td>\n",
       "      <td>-0.393781</td>\n",
       "      <td>0.228118</td>\n",
       "      <td>0.392946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.512350</td>\n",
       "      <td>-0.620099</td>\n",
       "      <td>0.938312</td>\n",
       "      <td>-0.100132</td>\n",
       "      <td>-0.191828</td>\n",
       "      <td>0.116272</td>\n",
       "      <td>-0.882415</td>\n",
       "      <td>-0.106105</td>\n",
       "      <td>-0.051885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065957</td>\n",
       "      <td>-0.151601</td>\n",
       "      <td>0.072650</td>\n",
       "      <td>-0.214053</td>\n",
       "      <td>-0.182360</td>\n",
       "      <td>0.173993</td>\n",
       "      <td>0.166463</td>\n",
       "      <td>0.261078</td>\n",
       "      <td>0.370338</td>\n",
       "      <td>0.349272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  1.031835  0.212301 -0.922307 -0.869803 -0.305008  0.092058   \n",
       "1           1 -1.545397 -0.120407  0.028675  0.202452  0.264835  0.240070   \n",
       "2           2 -1.415123 -0.065534  0.005330 -0.016653  0.142608  0.400511   \n",
       "3           3  0.865498 -1.222110 -0.553205  0.022980  0.309707 -0.115284   \n",
       "4           4 -1.341069 -0.078229 -0.045112  0.219325 -0.030194 -0.500378   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -1.161452  0.308622 -0.139144 -0.117040  0.072620  0.152300   \n",
       "5978     5978 -1.547982 -0.166368  0.062790 -0.058637 -0.039730  0.145217   \n",
       "5979     5979  0.204833  1.663809 -0.728419  0.605944  0.811258  1.091609   \n",
       "5980     5980 -0.925491 -0.298534 -0.188338 -0.024190  0.231828  0.287504   \n",
       "5981     5981 -0.512350 -0.620099  0.938312 -0.100132 -0.191828  0.116272   \n",
       "\n",
       "             6         7         8  ...       101       102       103  \\\n",
       "0     0.831769 -0.575745 -0.703853  ...  0.102421 -0.318619 -0.375416   \n",
       "1     0.129344  0.058812  0.209729  ...  0.028266  0.074902  0.029886   \n",
       "2    -0.053878 -0.125965 -0.270389  ...  0.018800  0.036842 -0.011082   \n",
       "3    -0.423851  0.164560  0.401825  ...  0.122099  0.050478  0.119724   \n",
       "4     0.017921  0.145940 -0.016312  ...  0.012135  0.031032 -0.017324   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977 -0.058142 -0.208458 -0.120671  ...  0.004387 -0.135064  0.031170   \n",
       "5978  0.182716  0.213168  0.241474  ... -0.017176 -0.011808 -0.000613   \n",
       "5979 -0.392112  1.131950 -0.592724  ... -0.523407  0.207549 -0.355611   \n",
       "5980  0.755301 -0.127350  0.329944  ... -0.028420 -0.098941 -0.130566   \n",
       "5981 -0.882415 -0.106105 -0.051885  ...  0.065957 -0.151601  0.072650   \n",
       "\n",
       "           104       105       106       107       108       109       110  \n",
       "0    -0.050509  0.185363 -0.019726 -0.122237 -0.086713 -0.053711 -0.077012  \n",
       "1     0.116798 -0.031030 -0.049614  0.024517  0.003476  0.001157  0.006372  \n",
       "2    -0.020539 -0.010853 -0.012236 -0.003550 -0.000766  0.007166  0.015099  \n",
       "3    -0.141897 -0.014390  0.095265 -0.023132  0.142257  0.325156  0.171970  \n",
       "4    -0.006011  0.007325 -0.018732  0.004144 -0.010431 -0.004584 -0.006360  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977 -0.002395 -0.009385  0.033274  0.079296  0.230744 -0.209229 -0.247969  \n",
       "5978 -0.003491 -0.025970 -0.001241  0.008654 -0.008849 -0.015761 -0.011166  \n",
       "5979 -0.084467  0.018351  0.011924  0.144681  0.012927  0.108045 -0.044070  \n",
       "5980  0.075247 -0.050180  0.088172 -0.168739 -0.393781  0.228118  0.392946  \n",
       "5981 -0.214053 -0.182360  0.173993  0.166463  0.261078  0.370338  0.349272  \n",
       "\n",
       "[5982 rows x 112 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                           aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "\n",
    "seed = 2020\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7306479357798165\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "# model architecture\n",
    "model = Sequential(name = 'dnn model')\n",
    "model.add(Dense(16, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# choose the optimizer and the cost function\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# train the model -> verbose=0: silent\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=25)]\n",
    "\n",
    "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "    batch_size=64, epochs=200, callbacks=callbacks, verbose=0)\n",
    "\n",
    "# visualize training history\n",
    "# plt.plot(hist.history['loss'], label='train loss')\n",
    "# plt.plot(hist.history['val_loss'], label='validation loss')\n",
    "# plt.legend()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.title('Loss')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.plot(hist.history['acc'], label='train acc')\n",
    "# plt.plot(hist.history['val_acc'], label='validation acc')\n",
    "# plt.legend()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.title('acc')\n",
    "# plt.show()\n",
    "\n",
    "# evaluate the model performance\n",
    "\n",
    "#print(model.evaluate(X_test, y_test))\n",
    "#if roc_auc_score(y_test, model.predict(X_test)) >= 0.755:\n",
    "#    print(f'1층: {dr1},드롭1: {drop1},2층: {dr2}, 드롭2: {drop2}, 3층: {dr3}')\n",
    "#    print(roc_auc_score(y_test, model.predict(X_test)))\n",
    "print(roc_auc_score(y_valid, model.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 대분류 구매여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.436058</td>\n",
       "      <td>-0.040728</td>\n",
       "      <td>-0.376852</td>\n",
       "      <td>0.244498</td>\n",
       "      <td>-0.337669</td>\n",
       "      <td>0.594535</td>\n",
       "      <td>1.160821</td>\n",
       "      <td>-0.507886</td>\n",
       "      <td>-0.026341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371315</td>\n",
       "      <td>-0.820841</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.359050</td>\n",
       "      <td>-0.052447</td>\n",
       "      <td>0.167750</td>\n",
       "      <td>-0.028469</td>\n",
       "      <td>0.045124</td>\n",
       "      <td>0.103598</td>\n",
       "      <td>0.014635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.495079</td>\n",
       "      <td>0.426246</td>\n",
       "      <td>0.603980</td>\n",
       "      <td>0.267780</td>\n",
       "      <td>0.167863</td>\n",
       "      <td>-0.341812</td>\n",
       "      <td>0.074077</td>\n",
       "      <td>0.082130</td>\n",
       "      <td>-0.268278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027723</td>\n",
       "      <td>-0.408299</td>\n",
       "      <td>-0.796369</td>\n",
       "      <td>-0.191770</td>\n",
       "      <td>0.136208</td>\n",
       "      <td>0.175161</td>\n",
       "      <td>-0.027328</td>\n",
       "      <td>0.023678</td>\n",
       "      <td>0.019509</td>\n",
       "      <td>-0.013586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.486196</td>\n",
       "      <td>0.137751</td>\n",
       "      <td>0.094870</td>\n",
       "      <td>0.455594</td>\n",
       "      <td>0.025952</td>\n",
       "      <td>0.268160</td>\n",
       "      <td>0.156926</td>\n",
       "      <td>-0.156983</td>\n",
       "      <td>-0.082030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102234</td>\n",
       "      <td>0.202870</td>\n",
       "      <td>0.137714</td>\n",
       "      <td>0.092741</td>\n",
       "      <td>-0.046748</td>\n",
       "      <td>-0.249884</td>\n",
       "      <td>0.133006</td>\n",
       "      <td>0.062282</td>\n",
       "      <td>-0.045456</td>\n",
       "      <td>-0.040493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.826307</td>\n",
       "      <td>-0.968225</td>\n",
       "      <td>0.253546</td>\n",
       "      <td>0.553680</td>\n",
       "      <td>-0.338130</td>\n",
       "      <td>-0.531447</td>\n",
       "      <td>-0.663852</td>\n",
       "      <td>0.274229</td>\n",
       "      <td>0.311605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.615561</td>\n",
       "      <td>0.226854</td>\n",
       "      <td>0.448593</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>-0.397996</td>\n",
       "      <td>-0.505601</td>\n",
       "      <td>-0.206288</td>\n",
       "      <td>-0.023323</td>\n",
       "      <td>-0.058132</td>\n",
       "      <td>-0.078507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.388948</td>\n",
       "      <td>0.099101</td>\n",
       "      <td>0.054519</td>\n",
       "      <td>-0.485316</td>\n",
       "      <td>-0.493745</td>\n",
       "      <td>0.040495</td>\n",
       "      <td>0.094735</td>\n",
       "      <td>-0.274916</td>\n",
       "      <td>-0.333733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094130</td>\n",
       "      <td>0.096581</td>\n",
       "      <td>-0.122793</td>\n",
       "      <td>0.078391</td>\n",
       "      <td>-0.059339</td>\n",
       "      <td>-0.210875</td>\n",
       "      <td>0.119112</td>\n",
       "      <td>0.095087</td>\n",
       "      <td>-0.033616</td>\n",
       "      <td>0.010124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>5977</td>\n",
       "      <td>-0.758481</td>\n",
       "      <td>0.812565</td>\n",
       "      <td>-0.108292</td>\n",
       "      <td>0.172876</td>\n",
       "      <td>0.196507</td>\n",
       "      <td>-0.143411</td>\n",
       "      <td>0.183231</td>\n",
       "      <td>0.159825</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.780837</td>\n",
       "      <td>-0.074323</td>\n",
       "      <td>-0.534725</td>\n",
       "      <td>-0.266745</td>\n",
       "      <td>-0.442471</td>\n",
       "      <td>-0.236793</td>\n",
       "      <td>-0.081901</td>\n",
       "      <td>0.159949</td>\n",
       "      <td>-0.030645</td>\n",
       "      <td>0.010190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>5978</td>\n",
       "      <td>-1.564872</td>\n",
       "      <td>0.227113</td>\n",
       "      <td>0.097719</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>-0.014250</td>\n",
       "      <td>0.254983</td>\n",
       "      <td>-0.491875</td>\n",
       "      <td>-0.342264</td>\n",
       "      <td>0.369359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097276</td>\n",
       "      <td>-0.100308</td>\n",
       "      <td>0.012817</td>\n",
       "      <td>0.030391</td>\n",
       "      <td>0.023927</td>\n",
       "      <td>0.100438</td>\n",
       "      <td>-0.032773</td>\n",
       "      <td>-0.034280</td>\n",
       "      <td>-0.015645</td>\n",
       "      <td>0.006128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>5979</td>\n",
       "      <td>1.010739</td>\n",
       "      <td>1.482501</td>\n",
       "      <td>1.119399</td>\n",
       "      <td>0.718032</td>\n",
       "      <td>-0.021002</td>\n",
       "      <td>0.340607</td>\n",
       "      <td>-0.337983</td>\n",
       "      <td>0.478843</td>\n",
       "      <td>0.405145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111127</td>\n",
       "      <td>0.135636</td>\n",
       "      <td>-0.055767</td>\n",
       "      <td>-0.729023</td>\n",
       "      <td>-0.262131</td>\n",
       "      <td>0.856770</td>\n",
       "      <td>0.272822</td>\n",
       "      <td>0.424178</td>\n",
       "      <td>-0.637153</td>\n",
       "      <td>-0.439690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>5980</td>\n",
       "      <td>-0.907570</td>\n",
       "      <td>0.014547</td>\n",
       "      <td>0.360717</td>\n",
       "      <td>0.103525</td>\n",
       "      <td>0.415867</td>\n",
       "      <td>-0.849732</td>\n",
       "      <td>0.382268</td>\n",
       "      <td>-0.345673</td>\n",
       "      <td>0.521682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.653179</td>\n",
       "      <td>0.014776</td>\n",
       "      <td>0.284553</td>\n",
       "      <td>-0.030792</td>\n",
       "      <td>-0.207601</td>\n",
       "      <td>-0.053542</td>\n",
       "      <td>-0.043660</td>\n",
       "      <td>-0.118073</td>\n",
       "      <td>-0.046550</td>\n",
       "      <td>-0.100628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>5981</td>\n",
       "      <td>-0.916461</td>\n",
       "      <td>-0.791022</td>\n",
       "      <td>0.136646</td>\n",
       "      <td>0.219709</td>\n",
       "      <td>-0.125223</td>\n",
       "      <td>0.073441</td>\n",
       "      <td>-0.344049</td>\n",
       "      <td>0.796474</td>\n",
       "      <td>-0.018890</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057549</td>\n",
       "      <td>-0.350444</td>\n",
       "      <td>-0.070858</td>\n",
       "      <td>0.011655</td>\n",
       "      <td>-0.310068</td>\n",
       "      <td>-0.501102</td>\n",
       "      <td>0.272890</td>\n",
       "      <td>0.247785</td>\n",
       "      <td>-0.007448</td>\n",
       "      <td>0.010920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5982 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         0         1         2         3         4         5  \\\n",
       "0           0  1.436058 -0.040728 -0.376852  0.244498 -0.337669  0.594535   \n",
       "1           1 -1.495079  0.426246  0.603980  0.267780  0.167863 -0.341812   \n",
       "2           2 -1.486196  0.137751  0.094870  0.455594  0.025952  0.268160   \n",
       "3           3  0.826307 -0.968225  0.253546  0.553680 -0.338130 -0.531447   \n",
       "4           4 -1.388948  0.099101  0.054519 -0.485316 -0.493745  0.040495   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "5977     5977 -0.758481  0.812565 -0.108292  0.172876  0.196507 -0.143411   \n",
       "5978     5978 -1.564872  0.227113  0.097719  0.005096 -0.014250  0.254983   \n",
       "5979     5979  1.010739  1.482501  1.119399  0.718032 -0.021002  0.340607   \n",
       "5980     5980 -0.907570  0.014547  0.360717  0.103525  0.415867 -0.849732   \n",
       "5981     5981 -0.916461 -0.791022  0.136646  0.219709 -0.125223  0.073441   \n",
       "\n",
       "             6         7         8  ...        26        27        28  \\\n",
       "0     1.160821 -0.507886 -0.026341  ...  0.371315 -0.820841  0.280656   \n",
       "1     0.074077  0.082130 -0.268278  ... -0.027723 -0.408299 -0.796369   \n",
       "2     0.156926 -0.156983 -0.082030  ...  0.102234  0.202870  0.137714   \n",
       "3    -0.663852  0.274229  0.311605  ... -0.615561  0.226854  0.448593   \n",
       "4     0.094735 -0.274916 -0.333733  ...  0.094130  0.096581 -0.122793   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5977  0.183231  0.159825 -0.234539  ... -0.780837 -0.074323 -0.534725   \n",
       "5978 -0.491875 -0.342264  0.369359  ... -0.097276 -0.100308  0.012817   \n",
       "5979 -0.337983  0.478843  0.405145  ...  0.111127  0.135636 -0.055767   \n",
       "5980  0.382268 -0.345673  0.521682  ... -0.653179  0.014776  0.284553   \n",
       "5981 -0.344049  0.796474 -0.018890  ... -0.057549 -0.350444 -0.070858   \n",
       "\n",
       "            29        30        31        32        33        34        35  \n",
       "0     0.359050 -0.052447  0.167750 -0.028469  0.045124  0.103598  0.014635  \n",
       "1    -0.191770  0.136208  0.175161 -0.027328  0.023678  0.019509 -0.013586  \n",
       "2     0.092741 -0.046748 -0.249884  0.133006  0.062282 -0.045456 -0.040493  \n",
       "3     0.001418 -0.397996 -0.505601 -0.206288 -0.023323 -0.058132 -0.078507  \n",
       "4     0.078391 -0.059339 -0.210875  0.119112  0.095087 -0.033616  0.010124  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5977 -0.266745 -0.442471 -0.236793 -0.081901  0.159949 -0.030645  0.010190  \n",
       "5978  0.030391  0.023927  0.100438 -0.032773 -0.034280 -0.015645  0.006128  \n",
       "5979 -0.729023 -0.262131  0.856770  0.272822  0.424178 -0.637153 -0.439690  \n",
       "5980 -0.030792 -0.207601 -0.053542 -0.043660 -0.118073 -0.046550 -0.100628  \n",
       "5981  0.011655 -0.310068 -0.501102  0.272890  0.247785 -0.007448  0.010920  \n",
       "\n",
       "[5982 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "df_test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "IDtest = df_test.cust_id.unique()\n",
    "\n",
    "\n",
    "level = 'gds_grp_mclas_nm'\n",
    "\n",
    "train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "                           aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "# 이상치(outlier)를 제거한다.\n",
    "train_test.iloc[:,1:] = train_test.iloc[:,1:].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)\n",
    "\n",
    "# 왼쪽으로 치우진 분포를 정규분포로 바꾸기 위해 로그 변환을 수행한다. -> 0.769\n",
    "train_test.iloc[:,1:] = np.log1p(train_test.iloc[:,1:])\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "train_test.iloc[:, 1:] = mmscaler.fit_transform(train_test.iloc[:,1:])\n",
    "\n",
    "# 특성 차원이 너무 많을 경우 과적합이 발생하기 때문에 차원 축소를 실행한다.\n",
    "max_d = num_d = train_test.shape[1] - 1\n",
    "pca = PCA(n_components=max_d, random_state=0).fit(train_test.iloc[:,1:])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #분산의 설명량을 누적합\n",
    "num_d = np.argmax(cumsum >= 0.99) + 1             # 분산의 설명량이 99%이상 되는 차원의 수\n",
    "if num_d == 1: num_d = max_d\n",
    "pca = PCA(n_components=num_d, random_state=0).fit_transform(train_test.iloc[:,1:])\n",
    "train_test = pd.concat([train_test.iloc[:,0], pd.DataFrame(pca)], axis=1)\n",
    "display(train_test)\n",
    "\n",
    "# 전처리 후 학습용과 제출용 데이터로 분리한다.\n",
    "X_train = train_test.query('cust_id not in @IDtest').drop('cust_id', axis=1)\n",
    "X_test = train_test.query('cust_id in @IDtest').drop('cust_id', axis=1)\n",
    "\n",
    "\n",
    "seed = 2020\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7421766402557687\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "# model architecture\n",
    "model = Sequential(name = 'dnn model')\n",
    "model.add(Dense(16, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# choose the optimizer and the cost function\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# train the model -> verbose=0: silent\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=25)]\n",
    "\n",
    "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "    batch_size=64, epochs=200, callbacks=callbacks, verbose=0)\n",
    "\n",
    "# visualize training history\n",
    "# plt.plot(hist.history['loss'], label='train loss')\n",
    "# plt.plot(hist.history['val_loss'], label='validation loss')\n",
    "# plt.legend()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.title('Loss')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.plot(hist.history['acc'], label='train acc')\n",
    "# plt.plot(hist.history['val_acc'], label='validation acc')\n",
    "# plt.legend()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.title('acc')\n",
    "# plt.show()\n",
    "\n",
    "# evaluate the model performance\n",
    "\n",
    "#print(model.evaluate(X_test, y_test))\n",
    "#if roc_auc_score(y_test, model.predict(X_test)) >= 0.755:\n",
    "#    print(f'1층: {dr1},드롭1: {drop1},2층: {dr2}, 드롭2: {drop2}, 3층: {dr3}')\n",
    "#    print(roc_auc_score(y_test, model.predict(X_test)))\n",
    "print(roc_auc_score(y_valid, model.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 중분류 구매건수 percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('train_numbersOfPurchase_nm_percentile.csv', encoding='cp949')\n",
    "X_test = pd.read_csv('train_numbersOfPurchase_nm_percentile.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "\n",
    "seed = 2020\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7577668890742285\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "# model architecture\n",
    "model = Sequential(name = 'dnn model')\n",
    "model.add(Dense(16, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# choose the optimizer and the cost function\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# train the model -> verbose=0: silent\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=25)]\n",
    "\n",
    "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "    batch_size=64, epochs=200, callbacks=callbacks, verbose=0)\n",
    "\n",
    "# visualize training history\n",
    "# plt.plot(hist.history['loss'], label='train loss')\n",
    "# plt.plot(hist.history['val_loss'], label='validation loss')\n",
    "# plt.legend()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.title('Loss')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.plot(hist.history['acc'], label='train acc')\n",
    "# plt.plot(hist.history['val_acc'], label='validation acc')\n",
    "# plt.legend()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.title('acc')\n",
    "# plt.show()\n",
    "\n",
    "# evaluate the model performance\n",
    "\n",
    "#print(model.evaluate(X_test, y_test))\n",
    "#if roc_auc_score(y_test, model.predict(X_test)) >= 0.755:\n",
    "#    print(f'1층: {dr1},드롭1: {drop1},2층: {dr2}, 드롭2: {drop2}, 3층: {dr3}')\n",
    "#    print(roc_auc_score(y_test, model.predict(X_test)))\n",
    "print(roc_auc_score(y_valid, model.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Bows - indiviual_task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Bows - group_task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test, X_dep, ID_dep = pd.read_pickle('case3_train_test.pkl')\n",
    "# X_train.shape, X_test.shape\n",
    "\n",
    "\n",
    "seed = 2020\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "i = int(round(X_train.shape[0] * 0.8, 0))\n",
    "X_valid, y_valid = X_train[i:], y_train[i:]\n",
    "X_train, y_train = X_train[:i], y_train[:i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test other features basic scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.625380</td>\n",
       "      <td>0.034396</td>\n",
       "      <td>-0.672246</td>\n",
       "      <td>0.441779</td>\n",
       "      <td>0.159294</td>\n",
       "      <td>-0.598718</td>\n",
       "      <td>0.325886</td>\n",
       "      <td>-0.936302</td>\n",
       "      <td>0.305429</td>\n",
       "      <td>-0.529067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143142</td>\n",
       "      <td>0.118564</td>\n",
       "      <td>-0.002334</td>\n",
       "      <td>0.089901</td>\n",
       "      <td>-0.111054</td>\n",
       "      <td>-0.021497</td>\n",
       "      <td>0.140365</td>\n",
       "      <td>0.117912</td>\n",
       "      <td>-0.126485</td>\n",
       "      <td>0.046712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.107635</td>\n",
       "      <td>-0.008147</td>\n",
       "      <td>-0.036582</td>\n",
       "      <td>-0.227227</td>\n",
       "      <td>-0.173067</td>\n",
       "      <td>0.257546</td>\n",
       "      <td>0.036107</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>-0.073846</td>\n",
       "      <td>-0.007021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030561</td>\n",
       "      <td>-0.018014</td>\n",
       "      <td>0.032755</td>\n",
       "      <td>0.015114</td>\n",
       "      <td>0.011464</td>\n",
       "      <td>-0.002095</td>\n",
       "      <td>-0.073120</td>\n",
       "      <td>-0.051948</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>-0.007939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.099033</td>\n",
       "      <td>-0.109059</td>\n",
       "      <td>-0.020829</td>\n",
       "      <td>0.007957</td>\n",
       "      <td>-0.059710</td>\n",
       "      <td>0.090729</td>\n",
       "      <td>0.179059</td>\n",
       "      <td>-0.068382</td>\n",
       "      <td>0.062943</td>\n",
       "      <td>-0.019888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.035945</td>\n",
       "      <td>-0.017813</td>\n",
       "      <td>-0.008323</td>\n",
       "      <td>-0.036238</td>\n",
       "      <td>0.021623</td>\n",
       "      <td>0.038809</td>\n",
       "      <td>0.046284</td>\n",
       "      <td>0.007039</td>\n",
       "      <td>0.014445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.265591</td>\n",
       "      <td>-0.619277</td>\n",
       "      <td>-0.279149</td>\n",
       "      <td>-0.187955</td>\n",
       "      <td>0.023135</td>\n",
       "      <td>-0.135279</td>\n",
       "      <td>-0.067050</td>\n",
       "      <td>0.675716</td>\n",
       "      <td>-0.159730</td>\n",
       "      <td>-0.068754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131376</td>\n",
       "      <td>0.064030</td>\n",
       "      <td>-0.118500</td>\n",
       "      <td>0.066814</td>\n",
       "      <td>-0.149491</td>\n",
       "      <td>-0.003301</td>\n",
       "      <td>0.486243</td>\n",
       "      <td>-0.482798</td>\n",
       "      <td>-0.028555</td>\n",
       "      <td>-0.156546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.141518</td>\n",
       "      <td>-0.191579</td>\n",
       "      <td>0.005533</td>\n",
       "      <td>-0.059629</td>\n",
       "      <td>0.026487</td>\n",
       "      <td>0.013643</td>\n",
       "      <td>-0.095080</td>\n",
       "      <td>0.043151</td>\n",
       "      <td>-0.057431</td>\n",
       "      <td>-0.002027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008822</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>-0.005974</td>\n",
       "      <td>-0.006765</td>\n",
       "      <td>0.007707</td>\n",
       "      <td>0.013415</td>\n",
       "      <td>-0.002152</td>\n",
       "      <td>0.008844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2795</th>\n",
       "      <td>-1.046915</td>\n",
       "      <td>-0.140033</td>\n",
       "      <td>0.103378</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>0.117287</td>\n",
       "      <td>-0.135593</td>\n",
       "      <td>0.194944</td>\n",
       "      <td>0.064718</td>\n",
       "      <td>-0.104468</td>\n",
       "      <td>0.140249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018055</td>\n",
       "      <td>0.042107</td>\n",
       "      <td>0.073418</td>\n",
       "      <td>0.032542</td>\n",
       "      <td>-0.048577</td>\n",
       "      <td>0.024386</td>\n",
       "      <td>-0.007156</td>\n",
       "      <td>-0.040956</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>-0.006824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796</th>\n",
       "      <td>1.527293</td>\n",
       "      <td>1.673800</td>\n",
       "      <td>0.590022</td>\n",
       "      <td>-0.410030</td>\n",
       "      <td>0.012423</td>\n",
       "      <td>-0.644904</td>\n",
       "      <td>-0.296354</td>\n",
       "      <td>-0.389029</td>\n",
       "      <td>0.611323</td>\n",
       "      <td>-0.646424</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118156</td>\n",
       "      <td>-0.154492</td>\n",
       "      <td>0.027863</td>\n",
       "      <td>0.238299</td>\n",
       "      <td>0.782080</td>\n",
       "      <td>-0.166157</td>\n",
       "      <td>0.164577</td>\n",
       "      <td>-0.059302</td>\n",
       "      <td>-0.306662</td>\n",
       "      <td>0.037948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2797</th>\n",
       "      <td>-1.000736</td>\n",
       "      <td>0.061578</td>\n",
       "      <td>-0.065981</td>\n",
       "      <td>-0.128722</td>\n",
       "      <td>-0.158024</td>\n",
       "      <td>0.071688</td>\n",
       "      <td>-0.036240</td>\n",
       "      <td>0.063314</td>\n",
       "      <td>-0.170748</td>\n",
       "      <td>0.247493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063057</td>\n",
       "      <td>0.029071</td>\n",
       "      <td>-0.046326</td>\n",
       "      <td>-0.022041</td>\n",
       "      <td>-0.008329</td>\n",
       "      <td>-0.034907</td>\n",
       "      <td>-0.037619</td>\n",
       "      <td>-0.025569</td>\n",
       "      <td>-0.019030</td>\n",
       "      <td>-0.012227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2798</th>\n",
       "      <td>-0.940100</td>\n",
       "      <td>-0.410004</td>\n",
       "      <td>-0.085304</td>\n",
       "      <td>0.097139</td>\n",
       "      <td>-0.015143</td>\n",
       "      <td>-0.038927</td>\n",
       "      <td>0.177830</td>\n",
       "      <td>-0.246140</td>\n",
       "      <td>-0.151301</td>\n",
       "      <td>-0.015606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006842</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.072775</td>\n",
       "      <td>0.052616</td>\n",
       "      <td>0.017694</td>\n",
       "      <td>0.026172</td>\n",
       "      <td>-0.053506</td>\n",
       "      <td>-0.042600</td>\n",
       "      <td>-0.012527</td>\n",
       "      <td>-0.031088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>-1.187763</td>\n",
       "      <td>-0.173355</td>\n",
       "      <td>0.014675</td>\n",
       "      <td>0.140338</td>\n",
       "      <td>-0.019429</td>\n",
       "      <td>-0.069380</td>\n",
       "      <td>0.019775</td>\n",
       "      <td>-0.010007</td>\n",
       "      <td>-0.038559</td>\n",
       "      <td>0.043015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>-0.004080</td>\n",
       "      <td>0.024599</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.019154</td>\n",
       "      <td>0.026391</td>\n",
       "      <td>-0.083488</td>\n",
       "      <td>-0.052727</td>\n",
       "      <td>-0.026782</td>\n",
       "      <td>-0.011026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2800 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.625380  0.034396 -0.672246  0.441779  0.159294 -0.598718  0.325886   \n",
       "1    -1.107635 -0.008147 -0.036582 -0.227227 -0.173067  0.257546  0.036107   \n",
       "2    -1.099033 -0.109059 -0.020829  0.007957 -0.059710  0.090729  0.179059   \n",
       "3     0.265591 -0.619277 -0.279149 -0.187955  0.023135 -0.135279 -0.067050   \n",
       "4    -1.141518 -0.191579  0.005533 -0.059629  0.026487  0.013643 -0.095080   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2795 -1.046915 -0.140033  0.103378  0.049180  0.117287 -0.135593  0.194944   \n",
       "2796  1.527293  1.673800  0.590022 -0.410030  0.012423 -0.644904 -0.296354   \n",
       "2797 -1.000736  0.061578 -0.065981 -0.128722 -0.158024  0.071688 -0.036240   \n",
       "2798 -0.940100 -0.410004 -0.085304  0.097139 -0.015143 -0.038927  0.177830   \n",
       "2799 -1.187763 -0.173355  0.014675  0.140338 -0.019429 -0.069380  0.019775   \n",
       "\n",
       "           7         8         9    ...       100       101       102  \\\n",
       "0    -0.936302  0.305429 -0.529067  ... -0.143142  0.118564 -0.002334   \n",
       "1    -0.059078 -0.073846 -0.007021  ...  0.030561 -0.018014  0.032755   \n",
       "2    -0.068382  0.062943 -0.019888  ...  0.002262  0.035945 -0.017813   \n",
       "3     0.675716 -0.159730 -0.068754  ...  0.131376  0.064030 -0.118500   \n",
       "4     0.043151 -0.057431 -0.002027  ...  0.008822  0.004029  0.004533   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2795  0.064718 -0.104468  0.140249  ... -0.018055  0.042107  0.073418   \n",
       "2796 -0.389029  0.611323 -0.646424  ... -0.118156 -0.154492  0.027863   \n",
       "2797  0.063314 -0.170748  0.247493  ...  0.063057  0.029071 -0.046326   \n",
       "2798 -0.246140 -0.151301 -0.015606  ...  0.006842  0.033337  0.072775   \n",
       "2799 -0.010007 -0.038559  0.043015  ...  0.002315 -0.004080  0.024599   \n",
       "\n",
       "           103       104       105       106       107       108       109  \n",
       "0     0.089901 -0.111054 -0.021497  0.140365  0.117912 -0.126485  0.046712  \n",
       "1     0.015114  0.011464 -0.002095 -0.073120 -0.051948  0.005610 -0.007939  \n",
       "2    -0.008323 -0.036238  0.021623  0.038809  0.046284  0.007039  0.014445  \n",
       "3     0.066814 -0.149491 -0.003301  0.486243 -0.482798 -0.028555 -0.156546  \n",
       "4    -0.000862 -0.005974 -0.006765  0.007707  0.013415 -0.002152  0.008844  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2795  0.032542 -0.048577  0.024386 -0.007156 -0.040956  0.002725 -0.006824  \n",
       "2796  0.238299  0.782080 -0.166157  0.164577 -0.059302 -0.306662  0.037948  \n",
       "2797 -0.022041 -0.008329 -0.034907 -0.037619 -0.025569 -0.019030 -0.012227  \n",
       "2798  0.052616  0.017694  0.026172 -0.053506 -0.042600 -0.012527 -0.031088  \n",
       "2799  0.000280  0.019154  0.026391 -0.083488 -0.052727 -0.026782 -0.011026  \n",
       "\n",
       "[2800 rows x 110 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7280372532666111\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "# model architecture\n",
    "model = Sequential(name = 'dnn model')\n",
    "model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# choose the optimizer and the cost function\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# train the model -> verbose=0: silent\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=25)]\n",
    "\n",
    "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "    batch_size=64, epochs=200, callbacks=callbacks, verbose=0)\n",
    "\n",
    "# visualize training history\n",
    "# plt.plot(hist.history['loss'], label='train loss')\n",
    "# plt.plot(hist.history['val_loss'], label='validation loss')\n",
    "# plt.legend()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.title('Loss')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.plot(hist.history['acc'], label='train acc')\n",
    "# plt.plot(hist.history['val_acc'], label='validation acc')\n",
    "# plt.legend()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.title('acc')\n",
    "# plt.show()\n",
    "\n",
    "# evaluate the model performance\n",
    "\n",
    "#print(model.evaluate(X_test, y_test))\n",
    "#if roc_auc_score(y_test, model.predict(X_test)) >= 0.755:\n",
    "#    print(f'1층: {dr1},드롭1: {drop1},2층: {dr2}, 드롭2: {drop2}, 3층: {dr3}')\n",
    "#    print(roc_auc_score(y_test, model.predict(X_test)))\n",
    "print(roc_auc_score(y_valid, model.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1층: 4,드롭1: 0,2층: 4, 드롭2: 0, 3층: 4\n",
      "0.754451755845315\n",
      "1층: 4,드롭1: 0,2층: 4, 드롭2: 0, 3층: 16\n",
      "0.7520051984451639\n",
      "1층: 4,드롭1: 0,2층: 8, 드롭2: 0.2, 3층: 8\n",
      "0.7528840018632981\n",
      "1층: 4,드롭1: 0,2층: 8, 드롭2: 0.3, 3층: 4\n",
      "0.7529681634378633\n",
      "1층: 4,드롭1: 0,2층: 8, 드롭2: 0.4, 3층: 8\n",
      "0.7552542266725645\n",
      "1층: 4,드롭1: 0,2층: 16, 드롭2: 0, 3층: 4\n",
      "0.7533791850810887\n",
      "1층: 4,드롭1: 0,2층: 16, 드롭2: 0.2, 3층: 8\n",
      "0.7549488963090256\n",
      "1층: 4,드롭1: 0,2층: 16, 드롭2: 0.3, 3층: 16\n",
      "0.751797730377631\n",
      "1층: 4,드롭1: 0,2층: 16, 드롭2: 0.4, 3층: 8\n",
      "0.7521226332003712\n",
      "1층: 4,드롭1: 0,2층: 32, 드롭2: 0, 3층: 4\n",
      "0.7580433021087367\n",
      "1층: 4,드롭1: 0,2층: 32, 드롭2: 0, 3층: 32\n",
      "0.7507114588919639\n",
      "1층: 4,드롭1: 0,2층: 32, 드롭2: 0.2, 3층: 4\n",
      "0.7601042820626239\n",
      "1층: 4,드롭1: 0.2,2층: 4, 드롭2: 0, 3층: 4\n",
      "0.7557631106117959\n",
      "1층: 4,드롭1: 0.2,2층: 4, 드롭2: 0, 3층: 8\n",
      "0.7504276582335464\n",
      "1층: 4,드롭1: 0.2,2층: 4, 드롭2: 0, 3층: 16\n",
      "0.7534555176719734\n",
      "1층: 4,드롭1: 0.2,2층: 4, 드롭2: 0, 3층: 32\n",
      "0.7549919557192684\n",
      "1층: 4,드롭1: 0.2,2층: 4, 드롭2: 0.3, 3층: 8\n",
      "0.7572075581008451\n",
      "1층: 4,드롭1: 0.2,2층: 4, 드롭2: 0.3, 3층: 16\n",
      "0.7511988131260741\n",
      "1층: 4,드롭1: 0.2,2층: 4, 드롭2: 0.4, 3층: 4\n",
      "0.7516235354907402\n",
      "1층: 4,드롭1: 0.2,2층: 4, 드롭2: 0.4, 3층: 32\n",
      "0.7516098347692994\n",
      "1층: 4,드롭1: 0.2,2층: 4, 드롭2: 0.5, 3층: 4\n",
      "0.7592039489393685\n",
      "1층: 4,드롭1: 0.2,2층: 4, 드롭2: 0.5, 3층: 32\n",
      "0.7507682190236474\n",
      "1층: 4,드롭1: 0.2,2층: 8, 드롭2: 0, 3층: 8\n",
      "0.7526276026477623\n",
      "1층: 4,드롭1: 0.2,2층: 8, 드롭2: 0.2, 3층: 4\n",
      "0.7549802122437476\n",
      "1층: 4,드롭1: 0.2,2층: 8, 드롭2: 0.2, 3층: 16\n",
      "0.7516137492611397\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2382\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2384\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Operation 'dropout_504/cond' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m       \u001b[0mxla_compile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_XlaCompile\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2386\u001b[0m       \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2387\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2388\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Operation 'dropout_504/cond' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-c5aa306f5487>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                     hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n\u001b[1;32m---> 37\u001b[1;33m                         batch_size=64, epochs=200, callbacks=callbacks, verbose=0)\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# visualize training history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1211\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m             \u001b[0mfit_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m         \u001b[0mfit_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    314\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[0;32m    315\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                         loss=self.total_loss)\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtraining_updates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymbolic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             raise ValueError('An operation has `None` for gradient. '\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[1;34m(loss, variables)\u001b[0m\n\u001b[0;32m   3023\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_tf_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3024\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3025\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36mgradients_v2\u001b[1;34m(ys, xs, grad_ys, name, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[0mys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgate_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m         unconnected_gradients)\n\u001b[0m\u001b[0;32m    275\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[0;32m    677\u001b[0m                 \u001b[1;31m# functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m--> 679\u001b[1;33m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[0;32m    680\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m                 \u001b[1;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    348\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_XlaScope\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Exit early\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    677\u001b[0m                 \u001b[1;31m# functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m--> 679\u001b[1;33m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[0;32m    680\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m                 \u001b[1;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\cond_v2.py\u001b[0m in \u001b[0;36m_IfGrad\u001b[1;34m(op, *grads)\u001b[0m\n\u001b[0;32m    108\u001b[0m   \u001b[1;31m# Get the if operator (this logic handles the case where op is a MockOp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m   \u001b[0mif_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m   \u001b[0mtrue_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfalse_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_func_graphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mif_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m   \u001b[1;31m# Note: op.graph != ops.get_default_graph() when we are computing the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m   \u001b[1;31m# of a nested cond.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\cond_v2.py\u001b[0m in \u001b[0;36mget_func_graphs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    330\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"If\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"StatelessIf\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     return (_get_func_graph_for_branch(op.get_attr(\"then_branch\")),\n\u001b[1;32m--> 332\u001b[1;33m             _get_func_graph_for_branch(op.get_attr(\"else_branch\")))\n\u001b[0m\u001b[0;32m    333\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Case\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     return [_get_func_graph_for_branch(branch_fn)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\cond_v2.py\u001b[0m in \u001b[0;36m_get_func_graph_for_branch\u001b[1;34m(name_attr_list)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m       func_graph = function_def_to_graph.function_def_to_graph(\n\u001b[1;32m--> 322\u001b[1;33m           fdef, input_shapes)\n\u001b[0m\u001b[0;32m    323\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mexternal_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternal_t\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m       \u001b[0mcustom_gradient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_handle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexternal_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternal_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\function_def_to_graph.py\u001b[0m in \u001b[0;36mfunction_def_to_graph\u001b[1;34m(fdef, input_shapes, copy_functions)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# Add all function nodes to the graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mimporter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_graph_def_for_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m# Initialize fields specific to FuncGraph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\importer.py\u001b[0m in \u001b[0;36mimport_graph_def_for_function\u001b[1;34m(graph_def, name)\u001b[0m\n\u001b[0;32m    410\u001b[0m   \u001b[1;34m\"\"\"Like import_graph_def but does not validate colocation constraints.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m   return _import_graph_def_internal(\n\u001b[1;32m--> 412\u001b[1;33m       graph_def, validate_colocation_constraints=False, name=name)\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\importer.py\u001b[0m in \u001b[0;36m_import_graph_def_internal\u001b[1;34m(graph_def, input_map, return_elements, validate_colocation_constraints, name, op_dict, producer_op_list)\u001b[0m\n\u001b[0;32m    496\u001b[0m   \u001b[1;31m# _ProcessNewOps.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mserialized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         results = c_api.TF_GraphImportGraphDefWithResults(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36mhelper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;33m<\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \"\"\"\n\u001b[1;32m--> 237\u001b[1;33m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_GeneratorContextManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dr1st = dense_range_1st_layer = [4, 8, 16, 32]\n",
    "dr2nd = dense_range_2nd_layer = [4, 8, 16, 32]\n",
    "dr3rd = dense_range_3rd_layer = [4, 8, 16, 32]\n",
    "drop1st = dropout_range_1st_layer = [0, 0.2, 0.3, 0.4, 0.5]\n",
    "drop2nd = dropout_range_2nd_layer = [0, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "for dr1 in dr1st:\n",
    "    \n",
    "    for drop1 in drop1st:\n",
    "    \n",
    "        for dr2 in dr2nd:\n",
    "        \n",
    "            for drop2 in drop2nd:\n",
    "                \n",
    "                for dr3 in dr3rd:\n",
    "\n",
    "        \n",
    "# model architecture\n",
    "                    model = Sequential(name = 'dnn model')\n",
    "                    model.add(Dense(dr1, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "                    model.add(Dropout(drop1))\n",
    "                    model.add(Dense(dr2, activation='relu'))\n",
    "                    model.add(Dropout(drop2))\n",
    "                    model.add(Dense(dr3, activation='relu'))\n",
    "                    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# choose the optimizer and the cost function\n",
    "                    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# train the model -> verbose=0: silent\n",
    "                    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=25)]\n",
    "\n",
    "                    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "                        batch_size=64, epochs=200, callbacks=callbacks, verbose=0)\n",
    "\n",
    "# visualize training history\n",
    "# plt.plot(hist.history['loss'], label='train loss')\n",
    "# plt.plot(hist.history['val_loss'], label='validation loss')\n",
    "# plt.legend()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.title('Loss')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.plot(hist.history['acc'], label='train acc')\n",
    "# plt.plot(hist.history['val_acc'], label='validation acc')\n",
    "# plt.legend()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.title('acc')\n",
    "# plt.show()\n",
    "\n",
    "# evaluate the model performance\n",
    "\n",
    "#                    print(model.evaluate(X_test, y_test))\n",
    "                    if roc_auc_score(y_test, model.predict(X_test)) >= 0.755:\n",
    "                        print(f'1층: {dr1},드롭1: {drop1},2층: {dr2}, 드롭2: {drop2}, 3층: {dr3}')\n",
    "                        print(roc_auc_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2482,)\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_dep).flatten()\n",
    "print(pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3500</td>\n",
       "      <td>0.778295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3501</td>\n",
       "      <td>0.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3502</td>\n",
       "      <td>0.139749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3503</td>\n",
       "      <td>0.169812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3504</td>\n",
       "      <td>0.267438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>5977</td>\n",
       "      <td>0.516476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>5978</td>\n",
       "      <td>0.382308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>5979</td>\n",
       "      <td>0.816023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>5980</td>\n",
       "      <td>0.276791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>5981</td>\n",
       "      <td>0.376510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2482 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id    gender\n",
       "0        3500  0.778295\n",
       "1        3501  0.210400\n",
       "2        3502  0.139749\n",
       "3        3503  0.169812\n",
       "4        3504  0.267438\n",
       "...       ...       ...\n",
       "2477     5977  0.516476\n",
       "2478     5978  0.382308\n",
       "2479     5979  0.816023\n",
       "2480     5980  0.276791\n",
       "2481     5981  0.376510\n",
       "\n",
       "[2482 rows x 2 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make submissions\n",
    "\n",
    "pd.DataFrame({'cust_id': ID_dep, 'gender': pred}).to_csv('submission_dnn_nm.csv', index=False, encoding='cp949')\n",
    "pd.read_csv('submission_dnn_nm.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 형식으로 저장\n",
    "with open('nm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추후 저장한 모형 불러올 때\n",
    "model = pd.read_pickle('nm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5 형식으로 저장\n",
    "model.save('nm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추후 저장한 모형 불러올 때\n",
    "model = load_model('nm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. libraries, read data     \n",
    "2. data engineering   \n",
    "3. model architecture, choose the optimizer and the cost function, train the model   \n",
    "4. visualize training history, evaluate the model performance   \n",
    "5. predict unseen data, save the model for future use   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
